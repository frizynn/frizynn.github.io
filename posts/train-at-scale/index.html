<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large-Scale Training: FSDP, QLoRA, and More. | Juan Lebrero</title><meta name=keywords content="LoRA,QLoRA,LLMs,finetuning,quantization,4-bit,deepspeed,fsdp,zero,precision,JAX,bfloat16,fp16"><meta name=description content="To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.
Numeric Precision
The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs."><meta name=author content="Juan Francisco Lebrero"><link rel=canonical href=https://juanlebrero.com/posts/train-at-scale/><link crossorigin=anonymous href=/assets/css/stylesheet.a1fde4156680e69bfffaba99ce74309162dd968c8d923159061a96f66f6a4b51.css integrity="sha256-of3kFWaA5pv/+rqZznQwkWLdloyNkjFZBhqW9m9qS1E=" rel="preload stylesheet" as=style><link rel=icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://juanlebrero.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://juanlebrero.com/favicon-32x32.png><link rel=apple-touch-icon href=https://juanlebrero.com/apple-touch-icon.png><link rel=mask-icon href=https://juanlebrero.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://juanlebrero.com/posts/train-at-scale/><link rel=alternate hreflang=es href=https://juanlebrero.com/es/posts/train-at-scale/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><meta property="og:url" content="https://juanlebrero.com/posts/train-at-scale/"><meta property="og:site_name" content="Juan Lebrero"><meta property="og:title" content="Large-Scale Training: FSDP, QLoRA, and More."><meta property="og:description" content="To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.
Numeric Precision The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-13T00:00:00+00:00"><meta property="article:tag" content="LoRA"><meta property="article:tag" content="QLoRA"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Finetuning"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="4-Bit"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large-Scale Training: FSDP, QLoRA, and More."><meta name=twitter:description content="To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.
Numeric Precision
The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://juanlebrero.com/posts/"},{"@type":"ListItem","position":2,"name":"Large-Scale Training: FSDP, QLoRA, and More.","item":"https://juanlebrero.com/posts/train-at-scale/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large-Scale Training: FSDP, QLoRA, and More.","name":"Large-Scale Training: FSDP, QLoRA, and More.","description":"To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.\nNumeric Precision The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs.\n","keywords":["LoRA","QLoRA","LLMs","finetuning","quantization","4-bit","deepspeed","fsdp","zero","precision","JAX","bfloat16","fp16"],"articleBody":"To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.\nNumeric Precision The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs.\nWhy precision matters A computer is finite. Real numbers are not. If you try to represent something like $\\pi$, which has infinitely many decimals, you need an approximation scheme. Floating point is that scheme.\nFloating point approximates a real value using three parts: a sign, a mantissa that controls fine resolution within a fixed interval, and an exponent that sets the dynamic range so you can represent very large and very small magnitudes. In IEEE 754 the base is 2, so the usual model is\n$$ \\text{value} \\approx \\text{sign}\\times \\text{mantissa}\\times \\text{base}^{\\text{exponent}}. $$\nMore bits in the mantissa buy you finer resolution. More bits in the exponent buy you wider range.\nFigure 1. FP32 layout under IEEE 754. This matters for three simple reasons. Low-bit formats run faster on modern accelerators and use less memory. If the range or resolution is too small, you get overflows, underflows, or loss of significance that can wreck training. And if you want to train big LLMs without burning money, you lean on mixed precision to keep compute and memory in check.\nFigure 2. The formats you actually see in deep learning: BF16, FP32, FP16. FP32 (single precision) Think of this as the baseline. It uses 1 bit for sign, 8 for exponent, and 23 for mantissa. The range spans roughly $1.18\\times10^{-38}$ to $3.4\\times10^{38}$ and machine epsilon is about $1.19\\times10^{-7}$. Even if you store tensors in lower precision to save memory, critical accumulations are often kept in FP32 to keep training steady.\nFP16 (half) FP16 cuts memory and usually speeds things up on the right hardware. The tradeoff is less detail and smaller range. Tiny numbers can vanish and large ones can saturate. That is why people apply loss scaling: you temporarily scale up losses and gradients so they sit in a safe numerical range, then scale updates back down.\nBF16 (brain floating point) BF16 is the current workhorse on TPUs and recent GPUs like H100. It keeps the FP32-sized exponent so the dynamic range matches FP32, but trims mantissa bits. In practice this avoids most overflow and underflow issues without resorting to tricks like loss scaling. You give up some decimal detail, which is usually fine for large-scale training.\nComparing precisions I will use JAX for quick experiments. It makes it easy to run on GPUs and TPUs and later helps show parallelism and memory behavior more transparently than other stacks.\nFirst, query versions, backend, and devices:\nimport jax # JAX setup print(f\"JAX version: {jax.__version__}\") print(f\"JAX backend: {jax.default_backend()}\") print(f\"Available devices: {jax.devices()}\") I will time code and inspect memory with psutil, tracemalloc, and time.\nimport jax.numpy as jnp import psutil import tracemalloc import time def get_memory_usage(): \"\"\"Return current process RSS in MB.\"\"\" process = psutil.Process() return process.memory_info().rss / 1024 / 1024 def measure_memory_and_time(func): def wrapper(*args, **kwargs): tracemalloc.start() start_memory = get_memory_usage() start_time = time.time() result = func(*args, **kwargs) jax.block_until_ready(result) end_time = time.time() end_memory = get_memory_usage() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() return { 'result': result, 'execution_time': end_time - start_time, 'memory_delta': end_memory - start_memory, 'peak_memory': peak / 1024 / 1024, 'current_memory': current / 1024 / 1024 } return wrapper Now a matrix multiply micro-bench and a tiny feedforward pass, both wrapped with the helper above.\n@measure_memory_and_time def matrix_multiplication_test(dtype, shape): \"\"\"Matrix multiply at a given dtype.\"\"\" key = jax.random.PRNGKey(42) key1, key2 = jax.random.split(key) def matmul_operation(): a = jax.random.normal(key1, shape, dtype=dtype) b = jax.random.normal(key2, shape, dtype=dtype) return jnp.dot(a, b) return matmul_operation() @measure_memory_and_time def neural_network_forward_pass_test(dtype, input_size, hidden_size, output_size): \"\"\"One forward pass of a tiny MLP.\"\"\" key = jax.random.PRNGKey(123) keys = jax.random.split(key, 3) def nn_forward(): # Weight init W1 = jax.random.normal(keys[0], (input_size, hidden_size), dtype=dtype) b1 = jax.random.normal(keys[1], (hidden_size,), dtype=dtype) W2 = jax.random.normal(keys[2], (hidden_size, output_size), dtype=dtype) b2 = jax.random.normal(keys[2], (output_size,), dtype=dtype) # Input x = jax.random.normal(keys[0], (input_size,), dtype=dtype) # Forward h = jnp.tanh(jnp.dot(x, W1) + b1) y = jnp.dot(h, W2) + b2 return y return nn_forward() Run like this:\nmatrix_multiplication_test(jnp.float16, (5000, 5000)) neural_network_forward_pass_test(jnp.float16, 784, 256, 10) I will also include FP64 to show a result that looks counterintuitive at first glance. Results depend on hardware. On an M1 the numbers looked like this, time in seconds and memory in MB.\nMatrix multiplication\nPrecision Time Peak Memory FP16 1.024 0.013 BF16 0.978 0.008 FP32 0.943 0.008 FP64 0.928 0.009 Neural network\nPrecision Time Peak Memory FP16 0.007 0.020 BF16 0.004 0.015 FP32 0.003 0.015 FP64 0.002 0.016 If you stop here you might think the whole discussion is useless because FP64 came out faster. That reading is wrong. CPUs like the M1 are tuned for FP32 and FP64 through vendor libraries such as Accelerate. FP16 and BF16 often get converted behind the scenes when you run on CPU, which adds overhead that dominates at this tiny scale. These are small problems, so a lot of the runtime is setup and conversion, not the math. And on GPUs the story flips because Tensor Cores are built to chew through FP16 and BF16 while moving less data.\nIf you repeat these tests on a GPU with much larger matrices, you will see the expected speedups. A simple TFLOP comparison from vendor docs makes the point.\nFigure 3. Throughput grows as precision drops on matrix ops. To wrap this section, here is a minimal mixed-precision example. Do the numerically sensitive work in FP32, then cast results to a cheaper format for storage.\ndef mixed_precision_forward_pass(x, W, b): # Promote to FP32 for compute x_fp32 = x.astype(jnp.float32) W_fp32 = W.astype(jnp.float32) b_fp32 = b.astype(jnp.float32) # Forward y = jnp.dot(x_fp32, W_fp32) + b_fp32 # Store in FP16 to save memory return y.astype(jnp.float16) Quantization Training often keeps critical math in FP32 and uses FP16 for the rest. Deployment is a different game. You want lower latency and smaller memory footprints without trashing quality. Quantization does exactly that by representing weights and activations with fewer bits and accepting a controlled numeric error. You usually try Post-Training Quantization first. If that is not enough, you switch to Quantization-Aware Training.\nFigure 4. QAT on the left, PTQ on the right. Post-Training Quantization (PTQ) PTQ takes a trained model and converts weights and activations to a low-bit format without retraining. Most of the time this is enough.\nStarting from a real value $x\\in\\mathbb{R}$, the standard affine uniform quantizer maps $x$ to an integer $q$ with $b$ bits using a scale $s$ and a zero point $z$:\n$$ q = \\operatorname{clip}\\Big(\\operatorname{round}\\big(\\tfrac{x}{s}\\big) + z,\\ q_{\\min}, q_{\\max}\\Big), \\qquad \\hat{x} = s,(q - z). $$\nFor signed $b$-bit integers, $q_{\\min}=-2^{b-1}$ and $q_{\\max}=2^{b-1}-1$. Given a target real interval $[\\alpha,\\beta]$ you get a practical choice\n$$ s=\\frac{\\beta-\\alpha}{q_{\\max}-q_{\\min}}, \\qquad z=\\operatorname{round}\\Big(\\frac{-\\alpha}{s}\\Big)+q_{\\min}. $$\nIf $z=0$ you have symmetric quantization, which centers the integer range around zero. If your distribution is skewed, an asymmetric choice with $z\\neq 0$ shifts the range and uses the available levels better.\nHere is a simple reference in Python:\nimport jax.numpy as jnp def quantize_tensor(x, num_bits=8, signed=True, eps=1e-8): if signed: qmin = - (2 ** (num_bits - 1)) qmax = 2 ** (num_bits - 1) - 1 else: qmin = 0 qmax = 2 ** num_bits - 1 x_min = jnp.min(x) x_max = jnp.max(x) scale = (x_max - x_min) / (qmax - qmin + eps) scale = jnp.where(scale == 0, 1.0, scale) zero_point = jnp.round(qmin - x_min / (scale + eps)) zero_point = jnp.clip(zero_point, qmin, qmax) q = jnp.clip(jnp.round(x / (scale + eps) + zero_point), qmin, qmax).astype(jnp.int32) x_hat = scale * (q.astype(jnp.float32) - zero_point) return q, x_hat, float(scale), int(zero_point) Weights are often fine with per-tensor or per-channel min-max because their distributions are roughly centered. Activations are different. ReLU produces only nonnegative values, so a symmetric scheme wastes half the codes on unused negatives. You solve that with asymmetric or unsigned quantization and by calibrating with a representative dataset so you do not pick ranges that get dominated by outliers. Percentile-based clipping is common because it trades a tiny bias for fewer saturation errors.\nFigure 5. Symmetric compared to asymmetric quantization. If accuracy drops too much with PTQ, move to QAT.\nQuantization-Aware Training (QAT) QAT trains a model while simulating the quantizer that will be used at inference time. You optimize\n$$ \\min_{w}\\ \\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\Big[L\\big(f_{q(w)}(x),\\,y\\big)\\Big]. $$ Here $w$ are the real-valued parameters before quantization. $f_{q(w)}$ is the same network but with weights passed through a quantizer $q(\\cdot)$. $L$ is your loss. If you define $q$ with literal rounding and clipping you break differentiability. The usual uniform affine quantizer is\n$$ q=\\operatorname{clip}\\!\\big(\\operatorname{round}(u),\\,q_{\\min},\\,q_{\\max}\\big),\\quad u=\\frac{w}{s}+z,\\quad \\tilde w=s\\,(q-z), $$ and you compute the forward with $\\tilde w$, not $w$. To backpropagate you use the straight-through estimator. Treat rounding as the identity in the backward pass and give clipping a derivative of one inside the valid range and zero outside:\n$$ \\frac{\\partial \\tilde w}{\\partial w}\\ \\approx\\ \\begin{cases} 1 \u0026 \\text{if } q_{\\min} \u003c u \u003c q_{\\max} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ That way gradients flow as long as values stay within range. If you also learn the scale $s$, you keep using STE and get\n$$ \\frac{\\partial \\tilde w}{\\partial s}\\ \\approx\\ \\begin{cases} -\\,z\\;-\\;\\frac{w}{s} \u0026 \\text{if } q_{\\min} \u003c u \u003c q_{\\max} \\\\ \\operatorname{clip}(u,\\,q_{\\min},\\,q_{\\max})\\;-\\;z\\;-\\;\\frac{w}{s} \u0026 \\text{otherwise} \\end{cases} $$ In practice you normalize this gradient by the number of elements that share the same scale. That is where per-tensor and per-channel schemes show up.\nConsider a linear layer with weights $W\\in\\mathbb{R}^{C_o\\times C_i}$ and activations $X\\in\\mathbb{R}^{T\\times C_i}$. Per-tensor uses a single scale for the whole $W$ and another for the whole $X$. It is simple and cheap but wastes resolution if ranges differ a lot internally. Per-channel assigns a different scale to each output channel of $W$, which you can think of as a vector $\\Delta_W\\in\\mathbb{R}^{1\\times C_o}$. If you also apply per-token on activations you get $\\Delta_X\\in\\mathbb{R}^{T\\times 1}$ so each row of $X$ has its own scale. This aligns with the real heterogeneity of values and reduces error at 8 bits and especially at 4 bits, at the cost of extra scale storage.\n\\(X\\in\\mathbb{R}^{T\\times C_i}\\) are activations and \\(W\\in\\mathbb{R}^{C_o\\times C_i}\\) are weights. The top diagram shows per-tensor with single scales \\(\\Delta X[1]\\) and \\(\\Delta W[1]\\). The bottom shows per-token plus per-channel with \\(\\Delta X[T\\times 1]\\) and \\(\\Delta W[1\\times C_o]\\). Dashed boxes indicate the region each scale covers. Training reduces to standard empirical risk with mini-batches while applying the same quantization scheme in the forward. For each step you compute outputs with $\\tilde w=s,(q-z)$ using the chosen granularity for weights and, if applicable, activations. You evaluate the loss, backpropagate with STE, normalize scale gradients by the number of elements they cover, and update $w$ plus scales if they are learnable:\n$$ \\min_{w}\\ \\frac{1}{B}\\sum_{i=1}^{B} L\\big(f_{\\tilde w}(x_i),\\,y_i\\big). $$ ","wordCount":"1822","inLanguage":"en","datePublished":"2025-09-13T00:00:00Z","dateModified":"2025-09-13T00:00:00Z","author":{"@type":"Person","name":"Juan Francisco Lebrero"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://juanlebrero.com/posts/train-at-scale/"},"publisher":{"@type":"Organization","name":"Juan Lebrero","logo":{"@type":"ImageObject","url":"https://juanlebrero.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://juanlebrero.com/ accesskey=h title="Juan Lebrero (Alt + H)">Juan Lebrero</a><div class=social-icons align=left><a href=https://x.com/lebrious target=_blank rel="noopener noreferrer me" title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a href=https://www.linkedin.com/in/lebrero-juan-francisco/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li class=sep aria-hidden=true>|</li><li><a href=https://juanlebrero.com/es/posts/train-at-scale/ title=Español aria-label=Español>ES</a></li></ul></div></div><ul id=menu><li><a href=https://juanlebrero.com/ title=Home><span>Home</span></a></li><li><a href=https://juanlebrero.com/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Large-Scale Training: FSDP, QLoRA, and More.</h1><div class=post-meta><span title='2025-09-13 00:00:00 +0000 UTC'>September 13, 2025</span>&nbsp;·&nbsp;Juan Francisco Lebrero&nbsp;|&nbsp;Translations:
<a href=https://juanlebrero.com/es/posts/train-at-scale/>Es</a></div></header><div class=post-content><p>To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.</p><h2 id=numeric-precision>Numeric Precision<a hidden class=anchor aria-hidden=true href=#numeric-precision>#</a></h2><p>The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs.</p><h3 id=why-precision-matters>Why precision matters<a hidden class=anchor aria-hidden=true href=#why-precision-matters>#</a></h3><p>A computer is finite. Real numbers are not. If you try to represent something like $\pi$, which has infinitely many decimals, you need an approximation scheme. Floating point is that scheme.</p><p>Floating point approximates a real value using three parts: a sign, a mantissa that controls fine resolution within a fixed interval, and an exponent that sets the dynamic range so you can represent very large and very small magnitudes. In IEEE 754 the base is 2, so the usual model is</p><p>$$
\text{value} \approx \text{sign}\times \text{mantissa}\times \text{base}^{\text{exponent}}.
$$</p><p>More bits in the mantissa buy you finer resolution. More bits in the exponent buy you wider range.</p><figure><img src=images/layout.png alt="32-bit floating-point layout (FP32)" style=width:100%;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figure 1. FP32 layout under IEEE 754.</figcaption></figure><p>This matters for three simple reasons. Low-bit formats run faster on modern accelerators and use less memory. If the range or resolution is too small, you get overflows, underflows, or loss of significance that can wreck training. And if you want to train big LLMs without burning money, you lean on mixed precision to keep compute and memory in check.</p><figure><img src=images/floating_point.png alt="BF16 vs FP32 vs FP16"><figcaption style=text-align:center;font-size:.95em;color:#666>Figure 2. The formats you actually see in deep learning: <b>BF16</b>, <b>FP32</b>, <b>FP16</b>.</figcaption></figure><h3 id=fp32-single-precision>FP32 (single precision)<a hidden class=anchor aria-hidden=true href=#fp32-single-precision>#</a></h3><p>Think of this as the baseline. It uses 1 bit for sign, 8 for exponent, and 23 for mantissa. The range spans roughly $1.18\times10^{-38}$ to $3.4\times10^{38}$ and machine epsilon is about $1.19\times10^{-7}$. Even if you store tensors in lower precision to save memory, critical accumulations are often kept in FP32 to keep training steady.</p><h3 id=fp16-half>FP16 (half)<a hidden class=anchor aria-hidden=true href=#fp16-half>#</a></h3><p>FP16 cuts memory and usually speeds things up on the right hardware. The tradeoff is less detail and smaller range. Tiny numbers can vanish and large ones can saturate. That is why people apply loss scaling: you temporarily scale up losses and gradients so they sit in a safe numerical range, then scale updates back down.</p><h3 id=bf16-brain-floating-point>BF16 (brain floating point)<a hidden class=anchor aria-hidden=true href=#bf16-brain-floating-point>#</a></h3><p>BF16 is the current workhorse on TPUs and recent GPUs like H100. It keeps the FP32-sized exponent so the dynamic range matches FP32, but trims mantissa bits. In practice this avoids most overflow and underflow issues without resorting to tricks like loss scaling. You give up some decimal detail, which is usually fine for large-scale training.</p><h2 id=comparing-precisions>Comparing precisions<a hidden class=anchor aria-hidden=true href=#comparing-precisions>#</a></h2><p>I will use JAX for quick experiments. It makes it easy to run on GPUs and TPUs and later helps show parallelism and memory behavior more transparently than other stacks.</p><p>First, query versions, backend, and devices:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax</span>
</span></span><span class=line><span class=cl><span class=c1># JAX setup</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX version: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX backend: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>default_backend</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Available devices: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>devices</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>I will time code and inspect memory with <code>psutil</code>, <code>tracemalloc</code>, and <code>time</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>psutil</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tracemalloc</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_memory_usage</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Return current process RSS in MB.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>process</span> <span class=o>=</span> <span class=n>psutil</span><span class=o>.</span><span class=n>Process</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>process</span><span class=o>.</span><span class=n>memory_info</span><span class=p>()</span><span class=o>.</span><span class=n>rss</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>measure_memory_and_time</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>wrapper</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>start_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>result</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>jax</span><span class=o>.</span><span class=n>block_until_ready</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>end_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>current</span><span class=p>,</span> <span class=n>peak</span> <span class=o>=</span> <span class=n>tracemalloc</span><span class=o>.</span><span class=n>get_traced_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>result</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;execution_time&#39;</span><span class=p>:</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;memory_delta&#39;</span><span class=p>:</span> <span class=n>end_memory</span> <span class=o>-</span> <span class=n>start_memory</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;peak_memory&#39;</span><span class=p>:</span> <span class=n>peak</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;current_memory&#39;</span><span class=p>:</span> <span class=n>current</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>       <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>wrapper</span>
</span></span></code></pre></div><p>Now a matrix multiply micro-bench and a tiny feedforward pass, both wrapped with the helper above.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>matrix_multiplication_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Matrix multiply at a given dtype.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>key1</span><span class=p>,</span> <span class=n>key2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>matmul_operation</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=n>a</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key1</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key2</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>matmul_operation</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;One forward pass of a tiny MLP.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>keys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>nn_forward</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=c1># Weight init</span>
</span></span><span class=line><span class=cl>       <span class=n>W1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>W2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>output_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=c1># Input</span>
</span></span><span class=line><span class=cl>       <span class=n>x</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=c1># Forward</span>
</span></span><span class=line><span class=cl>       <span class=n>h</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>nn_forward</span><span class=p>()</span>
</span></span></code></pre></div><p>Run like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>matrix_multiplication_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=p>(</span><span class=mi>5000</span><span class=p>,</span> <span class=mi>5000</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><p>I will also include FP64 to show a result that looks counterintuitive at first glance. Results depend on hardware. On an M1 the numbers looked like this, time in seconds and memory in MB.</p><div style=display:flex;gap:2px;flex-wrap:wrap><div style=flex:1;min-width:300px><p><strong>Matrix multiplication</strong></p><table><thead><tr><th>Precision</th><th>Time</th><th>Peak Memory</th></tr></thead><tbody><tr><td>FP16</td><td>1.024</td><td>0.013</td></tr><tr><td>BF16</td><td>0.978</td><td>0.008</td></tr><tr><td>FP32</td><td>0.943</td><td>0.008</td></tr><tr><td>FP64</td><td><strong>0.928</strong></td><td>0.009</td></tr></tbody></table></div><div style=flex:1;min-width:300px><p><strong>Neural network</strong></p><table><thead><tr><th>Precision</th><th>Time</th><th>Peak Memory</th></tr></thead><tbody><tr><td>FP16</td><td>0.007</td><td>0.020</td></tr><tr><td>BF16</td><td>0.004</td><td>0.015</td></tr><tr><td>FP32</td><td>0.003</td><td>0.015</td></tr><tr><td>FP64</td><td><strong>0.002</strong></td><td>0.016</td></tr></tbody></table></div></div><p>If you stop here you might think the whole discussion is useless because FP64 came out faster. That reading is wrong. CPUs like the M1 are tuned for FP32 and FP64 through vendor libraries such as Accelerate. FP16 and BF16 often get converted behind the scenes when you run on CPU, which adds overhead that dominates at this tiny scale. These are small problems, so a lot of the runtime is setup and conversion, not the math. And on GPUs the story flips because Tensor Cores are built to chew through FP16 and BF16 while moving less data.</p><p>If you repeat these tests on a GPU with much larger matrices, you will see the expected speedups. A simple TFLOP comparison from vendor docs makes the point.</p><div style=display:flex;justify-content:center><figure><img src=images/nvidia-a100-matmul-tflops.png alt="Throughput vs precision"><figcaption style=text-align:center;font-size:.95em;color:#666>Figure 3. Throughput grows as precision drops on matrix ops.</figcaption></figure></div><p>To wrap this section, here is a minimal mixed-precision example. Do the numerically sensitive work in FP32, then cast results to a cheaper format for storage.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>mixed_precision_forward_pass</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=c1># Promote to FP32 for compute</span>
</span></span><span class=line><span class=cl>   <span class=n>x_fp32</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>W_fp32</span> <span class=o>=</span> <span class=n>W</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>b_fp32</span> <span class=o>=</span> <span class=n>b</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=c1># Forward</span>
</span></span><span class=line><span class=cl>   <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_fp32</span><span class=p>,</span> <span class=n>W_fp32</span><span class=p>)</span> <span class=o>+</span> <span class=n>b_fp32</span>
</span></span><span class=line><span class=cl>   <span class=c1># Store in FP16 to save memory</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>y</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=quantization>Quantization<a hidden class=anchor aria-hidden=true href=#quantization>#</a></h2><p>Training often keeps critical math in FP32 and uses FP16 for the rest. Deployment is a different game. You want lower latency and smaller memory footprints without trashing quality. Quantization does exactly that by representing weights and activations with fewer bits and accepting a controlled numeric error. You usually try Post-Training Quantization first. If that is not enough, you switch to Quantization-Aware Training.</p><div style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><img src=images/Quantization-Aware-TrainingQAT-and-Post-Training-Quantization-PTQ.png alt="QAT versus PTQ" style="max-width:100%;height:auto;border:1px solid #ccc;border-radius:8px"><p style=text-align:center;margin-top:.5em;color:#555>Figure 4. QAT on the left, PTQ on the right.</p></div><h3 id=post-training-quantization-ptq>Post-Training Quantization (PTQ)<a hidden class=anchor aria-hidden=true href=#post-training-quantization-ptq>#</a></h3><p>PTQ takes a trained model and converts weights and activations to a low-bit format without retraining. Most of the time this is enough.</p><p>Starting from a real value $x\in\mathbb{R}$, the standard affine uniform quantizer maps $x$ to an integer $q$ with $b$ bits using a scale $s$ and a zero point $z$:</p><p>$$
q = \operatorname{clip}\Big(\operatorname{round}\big(\tfrac{x}{s}\big) + z,\ q_{\min}, q_{\max}\Big),
\qquad
\hat{x} = s,(q - z).
$$</p><p>For signed $b$-bit integers, $q_{\min}=-2^{b-1}$ and $q_{\max}=2^{b-1}-1$. Given a target real interval $[\alpha,\beta]$ you get a practical choice</p><p>$$
s=\frac{\beta-\alpha}{q_{\max}-q_{\min}},
\qquad
z=\operatorname{round}\Big(\frac{-\alpha}{s}\Big)+q_{\min}.
$$</p><p>If $z=0$ you have symmetric quantization, which centers the integer range around zero. If your distribution is skewed, an asymmetric choice with $z\neq 0$ shifts the range and uses the available levels better.</p><p>Here is a simple reference in Python:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>quantize_tensor</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>num_bits</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>signed</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>signed</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>qmin</span> <span class=o>=</span> <span class=o>-</span> <span class=p>(</span><span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>qmax</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>qmin</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>qmax</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>**</span> <span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x_min</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_max</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=p>(</span><span class=n>x_max</span> <span class=o>-</span> <span class=n>x_min</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>qmax</span> <span class=o>-</span> <span class=n>qmin</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>scale</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=n>scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>zero_point</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>qmin</span> <span class=o>-</span> <span class=n>x_min</span> <span class=o>/</span> <span class=p>(</span><span class=n>scale</span> <span class=o>+</span> <span class=n>eps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>zero_point</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>zero_point</span><span class=p>,</span> <span class=n>qmin</span><span class=p>,</span> <span class=n>qmax</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>x</span> <span class=o>/</span> <span class=p>(</span><span class=n>scale</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span> <span class=o>+</span> <span class=n>zero_point</span><span class=p>),</span> <span class=n>qmin</span><span class=p>,</span> <span class=n>qmax</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_hat</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=p>(</span><span class=n>q</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span> <span class=o>-</span> <span class=n>zero_point</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>q</span><span class=p>,</span> <span class=n>x_hat</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=n>scale</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>zero_point</span><span class=p>)</span>
</span></span></code></pre></div><p>Weights are often fine with per-tensor or per-channel min-max because their distributions are roughly centered. Activations are different. ReLU produces only nonnegative values, so a symmetric scheme wastes half the codes on unused negatives. You solve that with asymmetric or unsigned quantization and by calibrating with a representative dataset so you do not pick ranges that get dominated by outliers. Percentile-based clipping is common because it trades a tiny bias for fewer saturation errors.</p><div style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><img src=images/quantization_cropped.png alt="Symmetric vs asymmetric quantization" style="max-width:100%;height:auto;border:1px solid #ccc;border-radius:8px"><p style=text-align:center;margin-top:.5em;color:#555>Figure 5. Symmetric compared to asymmetric quantization.</p></div><p>If accuracy drops too much with PTQ, move to QAT.</p><h3 id=quantization-aware-training-qat>Quantization-Aware Training (QAT)<a hidden class=anchor aria-hidden=true href=#quantization-aware-training-qat>#</a></h3><p>QAT trains a model while simulating the quantizer that will be used at inference time. You optimize</p><div class=math>$$
\min_{w}\ \mathbb{E}_{(x,y)\sim\mathcal{D}}\Big[L\big(f_{q(w)}(x),\,y\big)\Big].
$$</div><p>Here $w$ are the real-valued parameters before quantization. $f_{q(w)}$ is the same network but with weights passed through a quantizer $q(\cdot)$. $L$ is your loss. If you define $q$ with literal rounding and clipping you break differentiability. The usual uniform affine quantizer is</p><div class=math>$$
q=\operatorname{clip}\!\big(\operatorname{round}(u),\,q_{\min},\,q_{\max}\big),\quad
u=\frac{w}{s}+z,\quad
\tilde w=s\,(q-z),
$$</div><p>and you compute the forward with $\tilde w$, not $w$. To backpropagate you use the straight-through estimator. Treat rounding as the identity in the backward pass and give clipping a derivative of one inside the valid range and zero outside:</p><div class=math>$$
\frac{\partial \tilde w}{\partial w}\ \approx\
\begin{cases}
1 & \text{if } q_{\min} < u < q_{\max} \\
0 & \text{otherwise}
\end{cases}
$$</div><p>That way gradients flow as long as values stay within range. If you also learn the scale $s$, you keep using STE and get</p><div class=math>$$
\frac{\partial \tilde w}{\partial s}\ \approx\
\begin{cases}
-\,z\;-\;\frac{w}{s} & \text{if } q_{\min} < u < q_{\max} \\
\operatorname{clip}(u,\,q_{\min},\,q_{\max})\;-\;z\;-\;\frac{w}{s} & \text{otherwise}
\end{cases}
$$</div><p>In practice you normalize this gradient by the number of elements that share the same scale. That is where per-tensor and per-channel schemes show up.</p><p>Consider a linear layer with weights $W\in\mathbb{R}^{C_o\times C_i}$ and activations $X\in\mathbb{R}^{T\times C_i}$. Per-tensor uses a single scale for the whole $W$ and another for the whole $X$. It is simple and cheap but wastes resolution if ranges differ a lot internally. Per-channel assigns a different scale to each output channel of $W$, which you can think of as a vector $\Delta_W\in\mathbb{R}^{1\times C_o}$. If you also apply per-token on activations you get $\Delta_X\in\mathbb{R}^{T\times 1}$ so each row of $X$ has its own scale. This aligns with the real heterogeneity of values and reduces error at 8 bits and especially at 4 bits, at the cost of extra scale storage.</p><p align=center><img src=images/per-channel_per-tensor.png alt="Per-tensor and per-channel quantization" style=max-width:100%;height:auto></p><p align=center style=color:#555;margin-top:.5em>\(X\in\mathbb{R}^{T\times C_i}\) are activations and \(W\in\mathbb{R}^{C_o\times C_i}\) are weights. The top diagram shows per-tensor with single scales \(\Delta X[1]\) and \(\Delta W[1]\). The bottom shows per-token plus per-channel with \(\Delta X[T\times 1]\) and \(\Delta W[1\times C_o]\). Dashed boxes indicate the region each scale covers.</p><p>Training reduces to standard empirical risk with mini-batches while applying the same quantization scheme in the forward. For each step you compute outputs with $\tilde w=s,(q-z)$ using the chosen granularity for weights and, if applicable, activations. You evaluate the loss, backpropagate with STE, normalize scale gradients by the number of elements they cover, and update $w$ plus scales if they are learnable:</p><div class=math>$$
\min_{w}\ \frac{1}{B}\sum_{i=1}^{B} L\big(f_{\tilde w}(x_i),\,y_i\big).
$$</div></div><footer class=post-footer><ul class=post-tags><li><a href=https://juanlebrero.com/tags/lora/>LoRA</a></li><li><a href=https://juanlebrero.com/tags/qlora/>QLoRA</a></li><li><a href=https://juanlebrero.com/tags/llms/>LLMs</a></li><li><a href=https://juanlebrero.com/tags/finetuning/>Finetuning</a></li><li><a href=https://juanlebrero.com/tags/quantization/>Quantization</a></li><li><a href=https://juanlebrero.com/tags/4-bit/>4-Bit</a></li><li><a href=https://juanlebrero.com/tags/deepspeed/>Deepspeed</a></li><li><a href=https://juanlebrero.com/tags/fsdp/>Fsdp</a></li><li><a href=https://juanlebrero.com/tags/zero/>Zero</a></li><li><a href=https://juanlebrero.com/tags/precision/>Precision</a></li><li><a href=https://juanlebrero.com/tags/jax/>JAX</a></li><li><a href=https://juanlebrero.com/tags/bfloat16/>Bfloat16</a></li><li><a href=https://juanlebrero.com/tags/fp16/>Fp16</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://juanlebrero.com/>Juan Lebrero</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>