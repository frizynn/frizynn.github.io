<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large-Scale Training: FSDP, QLoRA, and More | Juan Lebrero</title><meta name=keywords content="LoRA,QLoRA,LLMs,finetuning,quantization,4-bit,deepspeed,fsdp,zero,precision,JAX,bfloat16,fp16"><meta name=description content="To train models at large scale, we need to understand various concepts that will help us optimize performance and training stability. That&rsquo;s why in this guide, we&rsquo;ll look at concepts like numerical precision, data parallelism, quantization, LoRA, and more.
Numerical Precision
The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) constitutes one of the most determining factors for performance, memory usage, and stability of large-scale model training. That&rsquo;s why it&rsquo;s important to understand how numerical precision works and how it affects model performance, which will be explained in this section."><meta name=author content="Juan Francisco Lebrero"><link rel=canonical href=https://juanlebrero.com/posts/train-at-scale/><link crossorigin=anonymous href=/assets/css/stylesheet.f5054132853e5d2ab19e5b43341972ba4d1da729ccd501e6e3e826786720b251.css integrity="sha256-9QVBMoU+XSqxnltDNBlyuk0dpynM1QHm4+gmeGcgslE=" rel="preload stylesheet" as=style><link rel=icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://juanlebrero.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://juanlebrero.com/favicon-32x32.png><link rel=apple-touch-icon href=https://juanlebrero.com/apple-touch-icon.png><link rel=mask-icon href=https://juanlebrero.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://juanlebrero.com/posts/train-at-scale/><link rel=alternate hreflang=es href=https://juanlebrero.com/es/posts/train-at-scale/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://juanlebrero.com/posts/train-at-scale/"><meta property="og:site_name" content="Juan Lebrero"><meta property="og:title" content="Large-Scale Training: FSDP, QLoRA, and More"><meta property="og:description" content="To train models at large scale, we need to understand various concepts that will help us optimize performance and training stability. That’s why in this guide, we’ll look at concepts like numerical precision, data parallelism, quantization, LoRA, and more.
Numerical Precision The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) constitutes one of the most determining factors for performance, memory usage, and stability of large-scale model training. That’s why it’s important to understand how numerical precision works and how it affects model performance, which will be explained in this section."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-27T00:00:00+00:00"><meta property="article:tag" content="LoRA"><meta property="article:tag" content="QLoRA"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Finetuning"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="4-Bit"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large-Scale Training: FSDP, QLoRA, and More"><meta name=twitter:description content="To train models at large scale, we need to understand various concepts that will help us optimize performance and training stability. That&rsquo;s why in this guide, we&rsquo;ll look at concepts like numerical precision, data parallelism, quantization, LoRA, and more.
Numerical Precision
The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) constitutes one of the most determining factors for performance, memory usage, and stability of large-scale model training. That&rsquo;s why it&rsquo;s important to understand how numerical precision works and how it affects model performance, which will be explained in this section."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://juanlebrero.com/posts/"},{"@type":"ListItem","position":2,"name":"Large-Scale Training: FSDP, QLoRA, and More","item":"https://juanlebrero.com/posts/train-at-scale/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large-Scale Training: FSDP, QLoRA, and More","name":"Large-Scale Training: FSDP, QLoRA, and More","description":"To train models at large scale, we need to understand various concepts that will help us optimize performance and training stability. That\u0026rsquo;s why in this guide, we\u0026rsquo;ll look at concepts like numerical precision, data parallelism, quantization, LoRA, and more.\nNumerical Precision The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) constitutes one of the most determining factors for performance, memory usage, and stability of large-scale model training. That\u0026rsquo;s why it\u0026rsquo;s important to understand how numerical precision works and how it affects model performance, which will be explained in this section.\n","keywords":["LoRA","QLoRA","LLMs","finetuning","quantization","4-bit","deepspeed","fsdp","zero","precision","JAX","bfloat16","fp16"],"articleBody":"To train models at large scale, we need to understand various concepts that will help us optimize performance and training stability. That’s why in this guide, we’ll look at concepts like numerical precision, data parallelism, quantization, LoRA, and more.\nNumerical Precision The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) constitutes one of the most determining factors for performance, memory usage, and stability of large-scale model training. That’s why it’s important to understand how numerical precision works and how it affects model performance, which will be explained in this section.\n¿Por qué Importa la Precisión? Primero, necesitamos entender que intentan hacer los formatos de punto flotante. Principalmente, intentan representar un valor real de manera aproximada, y lo hacen con dos componentes clave: la mantisa y el exponente.\nUn número en punto flotante representa aproximadamente un valor real mediante la fórmula:\n$$\\text{valor} \\approx \\text{signo} \\times \\text{mantisa} \\times \\text{base}^{\\text{exponente}}$$\ndonde:\nMantisa: Controla la resolución fina (cuántos pasos discretos entra en el intervalo 1.0 - 2.0) Exponente: Determina el rango dinámico (qué tan grandes o pequeños pueden ser los números representables) Base: En IEEE 754 es 2. Más bits para la mantisa $\\rightarrow$ mayor precisión; más bits para el exponente $\\rightarrow$ mayor rango\nPero, ¿por qué es importante?\nBueno, hay tres razones principales por las que la precisión numérica es crucial en el entrenamiento de modelos de gran escala.\nEficiencia computacional: Los formatos de menor ancho de bits aceleran el cómputo en Tensor Cores/TPUs y reducen significativamente el uso de memoria.\nEstabilidad numérica: El rango dinámico y la granularidad de representación afectan directamente el underflow/overflow y el ruido numérico durante el entrenamiento.\nEscabilidad: Entrenar LLMs a gran escala requiere aprovechar la precisión mixta para que el costo computacional sea viable económicamente.\nFormatos de precisión numérica Los formatos de punto flotante son muy variados, pero los más comunes son:\nFP32 (IEEE 754, precisión simple) Este es el punto de referencia para casi todo. Usa 1 bit de signo, 8 de exponente y 23 de mantisa. Su épsilon es $\\varepsilon \\approx 2^{-23} \\approx 1.19\\times10^{-7}$ y cubre un rango amplio, desde $1.18\\times10^{-38}$ hasta $3.4\\times10^{38}$.\nEn práctica de ML lo tomamos como “precisión plena”. Incluso cuando trabajamos con precisión mixta, los acumuladores de gradientes se mantienen en FP32 para que el entrenamiento no se desestabilice.\nFP16 (IEEE 754, half) Acá buscamos velocidad y ahorro de memoria. FP16 tiene 1 bit de signo, 5 de exponente y 10 de mantisa, con $\\varepsilon \\approx 2^{-10} \\approx 9.77\\times10^{-4}$ y rango aproximado de $6.1\\times10^{-5}$ a $6.55\\times10^{4}$. Suele acelerar tanto el entrenamiento como la inferencia, aunque conviene usar loss scaling para que los gradientes chicos no desaparezcan.\nAdemás, ofrece mejor resolución fraccional que BF16, pero el rango dinámico es más corto, así que se puede llegar a saturar con activaciones o gradientes grandes y “apagar” señales muy chiquititas.\nBF16 (Brain Floating Point) El favorito actual para entrenar modelos grandes en TPUs y GPUs, como una H100. Tiene 1 bit de signo, 8 de exponente y 7 de mantisa, con $\\varepsilon \\approx 2^{-7} \\approx 7.81\\times10^{-3}$. Lo clave es que comparte el mismo rango que FP32, de $1.18\\times10^{-38}$ a $3.4\\times10^{38}$. En la práctica suele funcionar sin loss scaling. Mantiene el rango amplio que evita overflows y underflows molestos, y aunque la resolución fraccional sea menor que en FP16, para LLMs entrenando en serio suele alcanzar sin dramas.\nComparación de precisiones Para comparar las precisiones, voy a usar JAX, que es un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La razón de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitirá más adelante ver en “crudo” la paralelización de las operaciones y la optimización de la memoria.\nPrimero, importamos JAX y vemos la versión y el backend, así como los dispositivos disponibles:\nimport jax # Configuración de JAX print(f\"JAX version: {jax.__version__}\") print(f\"JAX backend: {jax.default_backend()}\") print(f\"Available devices: {jax.devices()}\") Para medir el rendimiento y la memoria, necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecución. Para esto, vamos a usar psutil, tracemalloc y time.\nimport jax.numpy as jnp import psutil import tracemalloc import time def get_memory_usage(): \"\"\"Obtiene el uso actual de memoria en MB\"\"\" process = psutil.Process() return process.memory_info().rss / 1024 / 1024 def measure_memory_and_time(func): def wrapper(*args, **kwargs): tracemalloc.start() start_memory = get_memory_usage() start_time = time.time() result = func(*args, **kwargs) jax.block_until_ready(result) end_time = time.time() end_memory = get_memory_usage() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() return { 'result': result, 'execution_time': end_time - start_time, 'memory_delta': end_memory - start_memory, 'peak_memory': peak / 1024 / 1024, 'current_memory': current / 1024 / 1024 } return wrapper Ahora, vamos a medir el rendimiento y la memoria de la multiplicación de matrices y la red neuronal. Para esto, vamos a usar la función measure_memory_and_time que definimos anteriormente.\n@measure_memory_and_time def matrix_multiplication_test(dtype, shape): \"\"\"Prueba de multiplicación de matrices con precisión dada\"\"\" key = jax.random.PRNGKey(42) key1, key2 = jax.random.split(key) def matmul_operation(): a = jax.random.normal(key1, shape, dtype=dtype) b = jax.random.normal(key2, shape, dtype=dtype) return jnp.dot(a, b) return matmul_operation() @measure_memory_and_time def neural_network_forward_pass_test(dtype, input_size, hidden_size, output_size): \"\"\"Prueba de pase forward de red neuronal simple\"\"\" key = jax.random.PRNGKey(123) keys = jax.random.split(key, 3) def nn_forward(): # Inicialización de pesos W1 = jax.random.normal(keys[0], (input_size, hidden_size), dtype=dtype) b1 = jax.random.normal(keys[1], (hidden_size,), dtype=dtype) W2 = jax.random.normal(keys[2], (hidden_size, output_size), dtype=dtype) b2 = jax.random.normal(keys[2], (output_size,), dtype=dtype) # Datos de entrada x = jax.random.normal(keys[0], (input_size,), dtype=dtype) # Pase forward h = jnp.tanh(jnp.dot(x, W1) + b1) y = jnp.dot(h, W2) + b2 return y return nn_forward() Para correr las pruebas, simplemente llamamos a las funciones matrix_multiplication_test y neural_network_forward_pass_test con los tipos de precisión y las dimensiones de las matrices y la red neuronal, por ejemplo:\nmatrix_multiplication_test(jnp.float16, (5000, 5000)) neural_network_forward_pass_test(jnp.float16, 784, 256, 10) Corriendo las pruebas en un M1, obtenemos los siguientes resultados para la multiplicación de matrices:\nPrecisión Tiempo (s) Memoria Delta (MB) Memoria Pico (MB) FP16 1.024 448.61 0.013 BF16 0.978 49.33 0.008 FP32 0.943 -49.25 0.008 FP64 0.928 7.84 0.009 Y para la red neuronal:\nPrecisión Tiempo (s) Memoria Delta (MB) Memoria Pico (MB) FP16 0.007 11.59 0.020 BF16 0.004 4.81 0.015 FP32 0.003 7.16 0.015 FP64 0.002 0.14 0.016 NOTA: NOTAR QUE FP64 ES EL MÁS RÁPIDO, EXTRAÑAMENTE. ESTOS RESULTADOS SON PARA UN CPU. CABE DESTACAR QUE JAX ESTÁ OPTIMIZADO PARA GPUs, ADEMÁS DE QUE:\nEn CPUs modernos (como el M1), las operaciones FP64 pueden ser más eficientes debido a optimizaciones específicas del hardware y la arquitectura ARM. En GPUs, el rendimiento se invierte dramáticamente: FP16/BF16 son significativamente más rápidos que FP32/FP64 debido a: Unidades de procesamiento especializadas (Tensor Cores en NVIDIA) Mayor paralelización de operaciones de menor precisión Menor uso de memoria y ancho de banda JAX utiliza XLA (Accelerated Linear Algebra) que optimiza automáticamente el código para el hardware disponible, lo que puede explicar estas diferencias de rendimiento. Para entrenamiento real, se recomienda usar FP16/BF16 en GPUs para obtener el mejor rendimiento y eficiencia de memoria. Precisión Mixta y Optimizaciones Implementación de Precisión Mixta # Ejemplo de entrenamiento con precisión mixta def mixed_precision_forward_pass(x, W, b): # Convertir a FP32 para cómputo x_fp32 = x.astype(jnp.float32) W_fp32 = W.astype(jnp.float32) b_fp32 = b.astype(jnp.float32) # Pase forward y = jnp.dot(x_fp32, W_fp32) + b_fp32 # Convertir de vuelta a FP16 para eficiencia de memoria return y.astype(jnp.float16) ","wordCount":"1193","inLanguage":"en","datePublished":"2025-01-27T00:00:00Z","dateModified":"2025-01-27T00:00:00Z","author":{"@type":"Person","name":"Juan Francisco Lebrero"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://juanlebrero.com/posts/train-at-scale/"},"publisher":{"@type":"Organization","name":"Juan Lebrero","logo":{"@type":"ImageObject","url":"https://juanlebrero.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://juanlebrero.com/ accesskey=h title="Juan Lebrero (Alt + H)">Juan Lebrero</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li class=sep aria-hidden=true>|</li><li><a href=https://juanlebrero.com/es/posts/train-at-scale/ title=Español aria-label=Español>ES</a></li></ul></div></div><ul id=menu><li><a href=https://juanlebrero.com/ title=Home><span>Home</span></a></li><li><a href=https://juanlebrero.com/posts/ title=Posts><span>Posts</span></a></li><li class=social-inline><div class=social-icons align=right><a href=https://x.com/lebrious target=_blank rel="noopener noreferrer me" title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a href=https://www.linkedin.com/in/lebrero-juan-francisco/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Large-Scale Training: FSDP, QLoRA, and More</h1><div class=post-meta><span title='2025-01-27 00:00:00 +0000 UTC'>January 27, 2025</span>&nbsp;·&nbsp;Juan Francisco Lebrero&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://juanlebrero.com/es/posts/train-at-scale/>Es</a></li></ul></div></header><div class=post-content><p>To train models at large scale, we need to understand various concepts that will help us optimize performance and training stability. That&rsquo;s why in this guide, we&rsquo;ll look at concepts like numerical precision, data parallelism, quantization, LoRA, and more.</p><h2 id=numerical-precision>Numerical Precision<a hidden class=anchor aria-hidden=true href=#numerical-precision>#</a></h2><p>The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) constitutes one of the most determining factors for performance, memory usage, and stability of large-scale model training. That&rsquo;s why it&rsquo;s important to understand how numerical precision works and how it affects model performance, which will be explained in this section.</p><h3 id=por-qué-importa-la-precisión>¿Por qué Importa la Precisión?<a hidden class=anchor aria-hidden=true href=#por-qué-importa-la-precisión>#</a></h3><p>Primero, necesitamos entender que intentan hacer los formatos de punto flotante. Principalmente, intentan representar un valor real de manera aproximada, y lo hacen con dos componentes clave: la <em>mantisa</em> y el <em>exponente</em>.</p><p>Un número en punto flotante representa aproximadamente un valor real mediante la fórmula:</p><p>$$\text{valor} \approx \text{signo} \times \text{mantisa} \times \text{base}^{\text{exponente}}$$</p><p>donde:</p><ul><li><strong>Mantisa</strong>: Controla la resolución fina (cuántos pasos discretos entra en el intervalo 1.0 - 2.0)</li><li><strong>Exponente</strong>: Determina el rango dinámico (qué tan grandes o pequeños pueden ser los números representables)</li><li><strong>Base</strong>: En IEEE 754 es 2.</li></ul><p>Más bits para la mantisa $\rightarrow$ mayor precisión; más bits para el exponente $\rightarrow$ mayor rango</p><p>Pero, ¿por qué es importante?</p><p>Bueno, hay tres razones principales por las que la precisión numérica es crucial en el entrenamiento de modelos de gran escala.</p><ul><li><p><strong>Eficiencia computacional</strong>: Los formatos de menor ancho de bits aceleran el cómputo en Tensor Cores/TPUs y reducen significativamente el uso de memoria.</p></li><li><p><strong>Estabilidad numérica</strong>: El rango dinámico y la granularidad de representación afectan directamente el underflow/overflow y el ruido numérico durante el entrenamiento.</p></li><li><p><strong>Escabilidad</strong>: Entrenar LLMs a gran escala requiere aprovechar la precisión mixta para que el costo computacional sea viable económicamente.</p></li></ul><h2 id=formatos-de-precisión-numérica>Formatos de precisión numérica<a hidden class=anchor aria-hidden=true href=#formatos-de-precisión-numérica>#</a></h2><p>Los formatos de punto flotante son muy variados, pero los más comunes son:</p><h3 id=fp32-ieee-754-precisión-simple>FP32 (IEEE 754, precisión simple)<a hidden class=anchor aria-hidden=true href=#fp32-ieee-754-precisión-simple>#</a></h3><p>Este es el punto de referencia para casi todo. Usa 1 bit de signo, 8 de exponente y 23 de mantisa. Su épsilon es $\varepsilon \approx 2^{-23} \approx 1.19\times10^{-7}$ y cubre un rango amplio, desde $1.18\times10^{-38}$ hasta $3.4\times10^{38}$.</p><p>En práctica de ML lo tomamos como “precisión plena”. Incluso cuando trabajamos con precisión mixta, los acumuladores de gradientes se mantienen en FP32 para que el entrenamiento no se desestabilice.</p><h3 id=fp16-ieee-754-half>FP16 (IEEE 754, half)<a hidden class=anchor aria-hidden=true href=#fp16-ieee-754-half>#</a></h3><p>Acá buscamos velocidad y ahorro de memoria. FP16 tiene 1 bit de signo, 5 de exponente y 10 de mantisa, con $\varepsilon \approx 2^{-10} \approx 9.77\times10^{-4}$ y rango aproximado de $6.1\times10^{-5}$ a $6.55\times10^{4}$. Suele acelerar tanto el entrenamiento como la inferencia, aunque conviene usar loss scaling para que los gradientes chicos no desaparezcan.</p><p>Además, ofrece mejor resolución fraccional que BF16, pero el rango dinámico es más corto, así que se puede llegar a saturar con activaciones o gradientes grandes y “apagar” señales muy chiquititas.</p><h3 id=bf16-brain-floating-point>BF16 (Brain Floating Point)<a hidden class=anchor aria-hidden=true href=#bf16-brain-floating-point>#</a></h3><p>El favorito actual para entrenar modelos grandes en TPUs y GPUs, como una H100. Tiene 1 bit de signo, 8 de exponente y 7 de mantisa, con $\varepsilon \approx 2^{-7} \approx 7.81\times10^{-3}$. Lo clave es que comparte el mismo rango que FP32, de $1.18\times10^{-38}$ a $3.4\times10^{38}$. En la práctica suele funcionar sin <em>loss scaling</em>. Mantiene el rango amplio que evita overflows y underflows molestos, y aunque la resolución fraccional sea menor que en FP16, para LLMs entrenando en serio suele alcanzar sin dramas.</p><h2 id=comparación-de-precisiones>Comparación de precisiones<a hidden class=anchor aria-hidden=true href=#comparación-de-precisiones>#</a></h2><p>Para comparar las precisiones, voy a usar JAX, que es un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La razón de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitirá más adelante ver en &ldquo;crudo&rdquo; la paralelización de las operaciones y la optimización de la memoria.</p><p>Primero, importamos JAX y vemos la versión y el backend, así como los dispositivos disponibles:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Configuración de JAX</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;JAX version: </span><span style=color:#e6db74>{</span>jax<span style=color:#f92672>.</span>__version__<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;JAX backend: </span><span style=color:#e6db74>{</span>jax<span style=color:#f92672>.</span>default_backend()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Available devices: </span><span style=color:#e6db74>{</span>jax<span style=color:#f92672>.</span>devices()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>Para medir el rendimiento y la memoria, necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecución. Para esto, vamos a usar <code>psutil</code>, <code>tracemalloc</code> y <code>time</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> jax.numpy <span style=color:#66d9ef>as</span> jnp
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> psutil
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tracemalloc
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_memory_usage</span>():
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#34;&#34;&#34;Obtiene el uso actual de memoria en MB&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>   process <span style=color:#f92672>=</span> psutil<span style=color:#f92672>.</span>Process()
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> process<span style=color:#f92672>.</span>memory_info()<span style=color:#f92672>.</span>rss <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>measure_memory_and_time</span>(func):
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wrapper</span>(<span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>       tracemalloc<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>       start_memory <span style=color:#f92672>=</span> get_memory_usage()
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>       start_time <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>       result <span style=color:#f92672>=</span> func(<span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs)
</span></span><span style=display:flex><span>       jax<span style=color:#f92672>.</span>block_until_ready(result)
</span></span><span style=display:flex><span>       end_time <span style=color:#f92672>=</span> time<span style=color:#f92672>.</span>time()
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>       end_memory <span style=color:#f92672>=</span> get_memory_usage()
</span></span><span style=display:flex><span>       current, peak <span style=color:#f92672>=</span> tracemalloc<span style=color:#f92672>.</span>get_traced_memory()
</span></span><span style=display:flex><span>       tracemalloc<span style=color:#f92672>.</span>stop()
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>return</span> {
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;result&#39;</span>: result,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;execution_time&#39;</span>: end_time <span style=color:#f92672>-</span> start_time,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;memory_delta&#39;</span>: end_memory <span style=color:#f92672>-</span> start_memory,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;peak_memory&#39;</span>: peak <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span>,
</span></span><span style=display:flex><span>           <span style=color:#e6db74>&#39;current_memory&#39;</span>: current <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>       }
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> wrapper
</span></span></code></pre></div><p>Ahora, vamos a medir el rendimiento y la memoria de la multiplicación de matrices y la red neuronal. Para esto, vamos a usar la función <code>measure_memory_and_time</code> que definimos anteriormente.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@measure_memory_and_time</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>matrix_multiplication_test</span>(dtype, shape):
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#34;&#34;&#34;Prueba de multiplicación de matrices con precisión dada&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>   key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>   key1, key2 <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>matmul_operation</span>():
</span></span><span style=display:flex><span>       a <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(key1, shape, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>       b <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(key2, shape, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>return</span> jnp<span style=color:#f92672>.</span>dot(a, b)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> matmul_operation()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@measure_memory_and_time</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>neural_network_forward_pass_test</span>(dtype, input_size, hidden_size, output_size):
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#34;&#34;&#34;Prueba de pase forward de red neuronal simple&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>   key <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>PRNGKey(<span style=color:#ae81ff>123</span>)
</span></span><span style=display:flex><span>   keys <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>split(key, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>nn_forward</span>():
</span></span><span style=display:flex><span>       <span style=color:#75715e># Inicialización de pesos</span>
</span></span><span style=display:flex><span>       W1 <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(keys[<span style=color:#ae81ff>0</span>], (input_size, hidden_size), dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>       b1 <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(keys[<span style=color:#ae81ff>1</span>], (hidden_size,), dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>       W2 <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(keys[<span style=color:#ae81ff>2</span>], (hidden_size, output_size), dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>       b2 <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(keys[<span style=color:#ae81ff>2</span>], (output_size,), dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>       <span style=color:#75715e># Datos de entrada</span>
</span></span><span style=display:flex><span>       x <span style=color:#f92672>=</span> jax<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(keys[<span style=color:#ae81ff>0</span>], (input_size,), dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>       <span style=color:#75715e># Pase forward</span>
</span></span><span style=display:flex><span>       h <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>tanh(jnp<span style=color:#f92672>.</span>dot(x, W1) <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>       y <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>dot(h, W2) <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>      
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>return</span> y
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> nn_forward()
</span></span></code></pre></div><p>Para correr las pruebas, simplemente llamamos a las funciones <code>matrix_multiplication_test</code> y <code>neural_network_forward_pass_test</code> con los tipos de precisión y las dimensiones de las matrices y la red neuronal, por ejemplo:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>matrix_multiplication_test(jnp<span style=color:#f92672>.</span>float16, (<span style=color:#ae81ff>5000</span>, <span style=color:#ae81ff>5000</span>))
</span></span><span style=display:flex><span>neural_network_forward_pass_test(jnp<span style=color:#f92672>.</span>float16, <span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><p>Corriendo las pruebas en un M1, obtenemos los siguientes resultados para la multiplicación de matrices:</p><table><thead><tr><th>Precisión</th><th>Tiempo (s)</th><th>Memoria Delta (MB)</th><th>Memoria Pico (MB)</th></tr></thead><tbody><tr><td>FP16</td><td>1.024</td><td>448.61</td><td>0.013</td></tr><tr><td>BF16</td><td>0.978</td><td>49.33</td><td>0.008</td></tr><tr><td>FP32</td><td>0.943</td><td>-49.25</td><td>0.008</td></tr><tr><td>FP64</td><td>0.928</td><td>7.84</td><td>0.009</td></tr></tbody></table><p>Y para la red neuronal:</p><table><thead><tr><th>Precisión</th><th>Tiempo (s)</th><th>Memoria Delta (MB)</th><th>Memoria Pico (MB)</th></tr></thead><tbody><tr><td>FP16</td><td>0.007</td><td>11.59</td><td>0.020</td></tr><tr><td>BF16</td><td>0.004</td><td>4.81</td><td>0.015</td></tr><tr><td>FP32</td><td>0.003</td><td>7.16</td><td>0.015</td></tr><tr><td>FP64</td><td>0.002</td><td>0.14</td><td>0.016</td></tr></tbody></table><p><strong>NOTA</strong>: NOTAR QUE FP64 ES EL MÁS RÁPIDO, EXTRAÑAMENTE. ESTOS RESULTADOS SON PARA UN CPU. CABE DESTACAR QUE JAX ESTÁ OPTIMIZADO PARA GPUs, ADEMÁS DE QUE:</p><ol><li><strong>En CPUs modernos</strong> (como el M1), las operaciones FP64 pueden ser más eficientes debido a optimizaciones específicas del hardware y la arquitectura ARM.</li><li><strong>En GPUs</strong>, el rendimiento se invierte dramáticamente: FP16/BF16 son significativamente más rápidos que FP32/FP64 debido a:</li></ol><ul><li>Unidades de procesamiento especializadas (Tensor Cores en NVIDIA)</li><li>Mayor paralelización de operaciones de menor precisión</li><li>Menor uso de memoria y ancho de banda</li></ul><ol start=3><li><strong>JAX utiliza XLA</strong> (Accelerated Linear Algebra) que optimiza automáticamente el código para el hardware disponible, lo que puede explicar estas diferencias de rendimiento.</li><li><strong>Para entrenamiento real</strong>, se recomienda usar FP16/BF16 en GPUs para obtener el mejor rendimiento y eficiencia de memoria.</li></ol><h2 id=precisión-mixta-y-optimizaciones>Precisión Mixta y Optimizaciones<a hidden class=anchor aria-hidden=true href=#precisión-mixta-y-optimizaciones>#</a></h2><h3 id=implementación-de-precisión-mixta>Implementación de Precisión Mixta<a hidden class=anchor aria-hidden=true href=#implementación-de-precisión-mixta>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Ejemplo de entrenamiento con precisión mixta</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mixed_precision_forward_pass</span>(x, W, b):
</span></span><span style=display:flex><span>   <span style=color:#75715e># Convertir a FP32 para cómputo</span>
</span></span><span style=display:flex><span>   x_fp32 <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>astype(jnp<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>   W_fp32 <span style=color:#f92672>=</span> W<span style=color:#f92672>.</span>astype(jnp<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>   b_fp32 <span style=color:#f92672>=</span> b<span style=color:#f92672>.</span>astype(jnp<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>   <span style=color:#75715e># Pase forward</span>
</span></span><span style=display:flex><span>   y <span style=color:#f92672>=</span> jnp<span style=color:#f92672>.</span>dot(x_fp32, W_fp32) <span style=color:#f92672>+</span> b_fp32
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>   <span style=color:#75715e># Convertir de vuelta a FP16 para eficiencia de memoria</span>
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> y<span style=color:#f92672>.</span>astype(jnp<span style=color:#f92672>.</span>float16)
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://juanlebrero.com/tags/lora/>LoRA</a></li><li><a href=https://juanlebrero.com/tags/qlora/>QLoRA</a></li><li><a href=https://juanlebrero.com/tags/llms/>LLMs</a></li><li><a href=https://juanlebrero.com/tags/finetuning/>Finetuning</a></li><li><a href=https://juanlebrero.com/tags/quantization/>Quantization</a></li><li><a href=https://juanlebrero.com/tags/4-bit/>4-Bit</a></li><li><a href=https://juanlebrero.com/tags/deepspeed/>Deepspeed</a></li><li><a href=https://juanlebrero.com/tags/fsdp/>Fsdp</a></li><li><a href=https://juanlebrero.com/tags/zero/>Zero</a></li><li><a href=https://juanlebrero.com/tags/precision/>Precision</a></li><li><a href=https://juanlebrero.com/tags/jax/>JAX</a></li><li><a href=https://juanlebrero.com/tags/bfloat16/>Bfloat16</a></li><li><a href=https://juanlebrero.com/tags/fp16/>Fp16</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://juanlebrero.com/>Juan Lebrero</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>