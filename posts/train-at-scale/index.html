<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large-Scale Training: FSDP, QLoRA, and More. | Juan Lebrero</title><meta name=keywords content="LoRA,QLoRA,LLMs,finetuning,quantization,4-bit,deepspeed,fsdp,zero,precision,JAX,bfloat16,fp16"><meta name=description content="To train models at scale, we need to understand several concepts that help optimize training performance and stability. That’s why we will cover numerical precision, data parallelism, quantization, LoRA, and more.
Numerical Precision
The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) is one of the most critical factors for performance, memory usage, and training stability of large-scale models. It is therefore essential to understand how numerical precision works and how it affects model performance, which will be explained in this section."><meta name=author content="Juan Francisco Lebrero"><link rel=canonical href=https://juanlebrero.com/posts/train-at-scale/><link crossorigin=anonymous href=/assets/css/stylesheet.baccaaa2085a898c7eec3389fdd189261eed2042be4931a1bc8dffaae7ea290a.css integrity="sha256-usyqoghaiYx+7DOJ/dGJJh7tIEK+STGhvI3/qufqKQo=" rel="preload stylesheet" as=style><link rel=icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://juanlebrero.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://juanlebrero.com/favicon-32x32.png><link rel=apple-touch-icon href=https://juanlebrero.com/apple-touch-icon.png><link rel=mask-icon href=https://juanlebrero.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://juanlebrero.com/posts/train-at-scale/><link rel=alternate hreflang=es href=https://juanlebrero.com/es/posts/train-at-scale/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><meta property="og:url" content="https://juanlebrero.com/posts/train-at-scale/"><meta property="og:site_name" content="Juan Lebrero"><meta property="og:title" content="Large-Scale Training: FSDP, QLoRA, and More."><meta property="og:description" content="To train models at scale, we need to understand several concepts that help optimize training performance and stability. That’s why we will cover numerical precision, data parallelism, quantization, LoRA, and more.
Numerical Precision The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) is one of the most critical factors for performance, memory usage, and training stability of large-scale models. It is therefore essential to understand how numerical precision works and how it affects model performance, which will be explained in this section."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-13T00:00:00+00:00"><meta property="article:tag" content="LoRA"><meta property="article:tag" content="QLoRA"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Finetuning"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="4-Bit"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large-Scale Training: FSDP, QLoRA, and More."><meta name=twitter:description content="To train models at scale, we need to understand several concepts that help optimize training performance and stability. That’s why we will cover numerical precision, data parallelism, quantization, LoRA, and more.
Numerical Precision
The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) is one of the most critical factors for performance, memory usage, and training stability of large-scale models. It is therefore essential to understand how numerical precision works and how it affects model performance, which will be explained in this section."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://juanlebrero.com/posts/"},{"@type":"ListItem","position":2,"name":"Large-Scale Training: FSDP, QLoRA, and More.","item":"https://juanlebrero.com/posts/train-at-scale/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large-Scale Training: FSDP, QLoRA, and More.","name":"Large-Scale Training: FSDP, QLoRA, and More.","description":"To train models at scale, we need to understand several concepts that help optimize training performance and stability. That’s why we will cover numerical precision, data parallelism, quantization, LoRA, and more.\nNumerical Precision The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) is one of the most critical factors for performance, memory usage, and training stability of large-scale models. It is therefore essential to understand how numerical precision works and how it affects model performance, which will be explained in this section.\n","keywords":["LoRA","QLoRA","LLMs","finetuning","quantization","4-bit","deepspeed","fsdp","zero","precision","JAX","bfloat16","fp16"],"articleBody":"To train models at scale, we need to understand several concepts that help optimize training performance and stability. That’s why we will cover numerical precision, data parallelism, quantization, LoRA, and more.\nNumerical Precision The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) is one of the most critical factors for performance, memory usage, and training stability of large-scale models. It is therefore essential to understand how numerical precision works and how it affects model performance, which will be explained in this section.\nWhy Does Precision Matter? How would you represent the number $\\pi$, which has INFINITE decimals, in something FINITE like a computer? The answer lies in floating-point numbers.\nFloating-point numbers approximate real values using two key components: the mantissa and the exponent.\nA floating-point number approximately represents a real value with the formula:\n$$\\text{value} \\approx \\text{sign} \\times \\text{mantissa} \\times \\text{base}^{\\text{exponent}}$$\nWhere:\nMantissa: Controls fine resolution (how many discrete steps fit between 1.0 and 2.0). Exponent: Determines the dynamic range (how large or small numbers can be represented). Base: In IEEE 754 it is 2. More bits for the mantissa $\\rightarrow$ higher precision.\nMore bits for the exponent $\\rightarrow$ wider range.\nFigure 1. Schematic of 32-bit floating-point representation (FP32) according to IEEE 754. But why does this matter for us?\nThere are three main reasons why numerical precision is critical in large-scale model training:\nComputational efficiency: Lower bit-width formats accelerate computation on Tensor Cores/TPUs and significantly reduce memory usage. Numerical stability: If the number format lacks sufficient range or detail, values may become too large, too small, or lose accuracy, leading to errors or strange results during training. Scalability: When training large-scale LLMs, mixed precision is crucial to prevent computational costs from skyrocketing. Figure 2. Visual comparison of the most widely used floating-point formats in deep learning: BF16, FP32, and FP16. FP32 (IEEE 754, Single Precision) This is the “standard” format most commonly used. It stores numbers with 1 bit for the sign, 8 for the exponent, and 23 for the mantissa. It can represent very small and very large values, from $1.18\\times10^{-38}$ to $3.4\\times10^{38}$, with high precision ($\\varepsilon \\approx 1.19\\times10^{-7}$).\nIn machine learning, FP32 is considered “full precision.” Even when other formats are used to save memory, critical calculations (such as gradient accumulation) are often done in FP32 to maintain stability.\nFP16 (IEEE 754, Half Precision) FP16 uses fewer bits than FP32, which reduces memory usage and speeds up computations.\nPros: Faster and cheaper training/inference, lower memory footprint.\nCons: Reduced detail and range may cause very small numbers to vanish (requiring techniques like loss scaling) or very large numbers to saturate.\nBF16 (Brain Floating Point) BF16 is widely used today for training large models on TPUs and modern GPUs (like the H100).\nIt stores numbers similarly to FP32 but with fewer mantissa bits. Importantly, it can represent the same large and small values as FP32, so it avoids breakdowns with extreme values. Unlike FP16, BF16 typically does not require loss scaling and is stable enough for training large models (such as LLMs).\nPrecision Comparison To compare precisions, we’ll use JAX, a framework developed by Google that allows efficient execution on GPUs and TPUs. Unlike PyTorch, JAX lets us explore “raw” operation parallelization and memory optimization later on.\n…\n(Runs same code examples in English)\n…\nAt first glance, one might think: “So FP64 is the best.” But in reality, that’s not true. These results are explained by several factors:\nCPUs are optimized for certain types: Processors like the M1 run better with FP32 and FP64 because the libraries they rely on (like Accelerate on Mac) are optimized for those formats. FP16 and BF16 are often not as well supported on CPU, so the system internally converts them to FP32/FP64, making them slower for small problems.\nSize matters: In the table examples, matrices and networks are small. When data sizes are small, most time goes into setup (initialization, conversions, synchronization) rather than the math itself. This makes FP64 appear “faster” because its execution path is more direct and optimized.\nOn GPU it’s the opposite: GPUs run FP16 and BF16 much faster, thanks to specialized hardware (like NVIDIA’s Tensor Cores) that process these formats in parallel, with lower memory and bandwidth usage.\nTo confirm this yourself, try running the tests on a GPU with much larger matrices—you’ll clearly see the difference.\nFigure 3. Precision comparison. Finally, as a conclusion to this section, here’s an example of mixed precision training, which is the standard practice for training large-scale models.\nThe idea is simple: the parts of the model that require numerical stability are computed in FP32, while intermediate results, gradients, and parameters are stored in FP16 or BF16, taking advantage of memory savings and higher hardware throughput.\n# Example of mixed precision training def mixed_precision_forward_pass(x, W, b): # Convert to FP32 for computation x_fp32 = x.astype(jnp.float32) W_fp32 = W.astype(jnp.float32) b_fp32 = b.astype(jnp.float32) # Forward pass y = jnp.dot(x_fp32, W_fp32) + b_fp32 # Convert back to FP16 for memory efficiency return y.astype(jnp.float16) ","wordCount":"831","inLanguage":"en","datePublished":"2025-09-13T00:00:00Z","dateModified":"2025-09-13T00:00:00Z","author":{"@type":"Person","name":"Juan Francisco Lebrero"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://juanlebrero.com/posts/train-at-scale/"},"publisher":{"@type":"Organization","name":"Juan Lebrero","logo":{"@type":"ImageObject","url":"https://juanlebrero.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://juanlebrero.com/ accesskey=h title="Juan Lebrero (Alt + H)">Juan Lebrero</a><div class=social-icons align=left><a href=https://x.com/lebrious target=_blank rel="noopener noreferrer me" title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a href=https://www.linkedin.com/in/lebrero-juan-francisco/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li class=sep aria-hidden=true>|</li><li><a href=https://juanlebrero.com/es/posts/train-at-scale/ title=Español aria-label=Español>ES</a></li></ul></div></div><ul id=menu><li><a href=https://juanlebrero.com/ title=Home><span>Home</span></a></li><li><a href=https://juanlebrero.com/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Large-Scale Training: FSDP, QLoRA, and More.</h1><div class=post-meta><span title='2025-09-13 00:00:00 +0000 UTC'>September 13, 2025</span>&nbsp;·&nbsp;Juan Francisco Lebrero&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://juanlebrero.com/es/posts/train-at-scale/>Es</a></li></ul></div></header><div class=post-content><p>To train models at scale, we need to understand several concepts that help optimize training performance and stability. That’s why we will cover numerical precision, data parallelism, quantization, LoRA, and more.</p><h2 id=numerical-precision>Numerical Precision<a hidden class=anchor aria-hidden=true href=#numerical-precision>#</a></h2><p>The choice of numerical format (FP32, FP16, BF16, FP8, INT8, etc.) is one of the most critical factors for performance, memory usage, and training stability of large-scale models. It is therefore essential to understand how numerical precision works and how it affects model performance, which will be explained in this section.</p><h3 id=why-does-precision-matter>Why Does Precision Matter?<a hidden class=anchor aria-hidden=true href=#why-does-precision-matter>#</a></h3><p>How would you represent the number $\pi$, which has INFINITE decimals, in something FINITE like a computer? The answer lies in floating-point numbers.</p><p>Floating-point numbers approximate real values using two key components: the <em>mantissa</em> and the <em>exponent</em>.</p><p>A floating-point number approximately represents a real value with the formula:</p><p>$$\text{value} \approx \text{sign} \times \text{mantissa} \times \text{base}^{\text{exponent}}$$</p><p>Where:</p><ul><li><strong>Mantissa</strong>: Controls fine resolution (how many discrete steps fit between 1.0 and 2.0).</li><li><strong>Exponent</strong>: Determines the dynamic range (how large or small numbers can be represented).</li><li><strong>Base</strong>: In IEEE 754 it is 2.</li></ul><p>More bits for the mantissa $\rightarrow$ higher precision.<br>More bits for the exponent $\rightarrow$ wider range.</p><figure><img src=images/layout.png alt="32-bit Floating Point Representation (FP32)" style=width:100%;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figure 1. Schematic of 32-bit floating-point representation (FP32) according to IEEE 754.<br></figcaption></figure><p>But why does this matter for us?</p><p>There are three main reasons why numerical precision is critical in large-scale model training:</p><ul><li><strong>Computational efficiency</strong>: Lower bit-width formats accelerate computation on Tensor Cores/TPUs and significantly reduce memory usage.</li><li><strong>Numerical stability</strong>: If the number format lacks sufficient range or detail, values may become too large, too small, or lose accuracy, leading to errors or strange results during training.</li><li><strong>Scalability</strong>: When training large-scale LLMs, mixed precision is crucial to prevent computational costs from skyrocketing.</li></ul><figure><img src=images/floating_point.png alt="Floating Point Format Comparison: BF16, FP32, FP16"><figcaption style=text-align:center;font-size:.95em;color:#666>Figure 2. Visual comparison of the most widely used floating-point formats in deep learning: <b>BF16</b>, <b>FP32</b>, and <b>FP16</b>.<br></figcaption></figure><h3 id=fp32-ieee-754-single-precision>FP32 (IEEE 754, Single Precision)<a hidden class=anchor aria-hidden=true href=#fp32-ieee-754-single-precision>#</a></h3><p>This is the “standard” format most commonly used. It stores numbers with 1 bit for the sign, 8 for the exponent, and 23 for the mantissa. It can represent very small and very large values, from $1.18\times10^{-38}$ to $3.4\times10^{38}$, with high precision ($\varepsilon \approx 1.19\times10^{-7}$).</p><p>In machine learning, FP32 is considered “full precision.” Even when other formats are used to save memory, critical calculations (such as gradient accumulation) are often done in FP32 to maintain stability.</p><h3 id=fp16-ieee-754-half-precision>FP16 (IEEE 754, Half Precision)<a hidden class=anchor aria-hidden=true href=#fp16-ieee-754-half-precision>#</a></h3><p>FP16 uses fewer bits than FP32, which reduces memory usage and speeds up computations.</p><p>Pros: Faster and cheaper training/inference, lower memory footprint.<br>Cons: Reduced detail and range may cause very small numbers to vanish (requiring techniques like <a href=https://picdictionary.com/ml-dictionary/loss-scaling-in-ai-and-deep-learning>loss scaling</a>) or very large numbers to saturate.</p><h3 id=bf16-brain-floating-point>BF16 (Brain Floating Point)<a hidden class=anchor aria-hidden=true href=#bf16-brain-floating-point>#</a></h3><p>BF16 is widely used today for training large models on TPUs and modern GPUs (like the H100).</p><p>It stores numbers similarly to FP32 but with fewer mantissa bits. Importantly, it can represent the same large and small values as FP32, so it avoids breakdowns with extreme values. Unlike FP16, BF16 typically does not require loss scaling and is stable enough for training large models (such as LLMs).</p><h2 id=precision-comparison>Precision Comparison<a hidden class=anchor aria-hidden=true href=#precision-comparison>#</a></h2><p>To compare precisions, we’ll use JAX, a framework developed by Google that allows efficient execution on GPUs and TPUs. Unlike PyTorch, JAX lets us explore “raw” operation parallelization and memory optimization later on.</p><p>&mldr;</p><p>(Runs same code examples in English)</p><p>&mldr;</p><p>At first glance, one might think: “So FP64 is the best.” But in reality, that’s not true. These results are explained by several factors:</p><ol><li><p><strong>CPUs are optimized for certain types</strong>: Processors like the M1 run better with FP32 and FP64 because the libraries they rely on (like Accelerate on Mac) are optimized for those formats. FP16 and BF16 are often not as well supported on CPU, so the system internally converts them to FP32/FP64, making them slower for small problems.</p></li><li><p><strong>Size matters</strong>: In the table examples, matrices and networks are small. When data sizes are small, most time goes into setup (initialization, conversions, synchronization) rather than the math itself. This makes FP64 appear “faster” because its execution path is more direct and optimized.</p></li><li><p><strong>On GPU it’s the opposite</strong>: GPUs run FP16 and BF16 much faster, thanks to specialized hardware (like NVIDIA’s Tensor Cores) that process these formats in parallel, with lower memory and bandwidth usage.</p></li></ol><p>To confirm this yourself, try running the tests on a GPU with much larger matrices—you’ll clearly see the difference.</p><div style=display:flex;justify-content:center><figure><img src=images/nvidia-a100-matmul-tflops.png alt="Precision Comparison" style=max-width:350px;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figure 3. Precision comparison.<br></figcaption></figure></div><p>Finally, as a conclusion to this section, here’s an example of mixed precision training, which is the standard practice for training large-scale models.</p><p>The idea is simple: the parts of the model that require numerical stability are computed in FP32, while intermediate results, gradients, and parameters are stored in FP16 or BF16, taking advantage of memory savings and higher hardware throughput.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Example of mixed precision training</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mixed_precision_forward_pass</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=c1># Convert to FP32 for computation</span>
</span></span><span class=line><span class=cl>   <span class=n>x_fp32</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>W_fp32</span> <span class=o>=</span> <span class=n>W</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>b_fp32</span> <span class=o>=</span> <span class=n>b</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Forward pass</span>
</span></span><span class=line><span class=cl>   <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_fp32</span><span class=p>,</span> <span class=n>W_fp32</span><span class=p>)</span> <span class=o>+</span> <span class=n>b_fp32</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Convert back to FP16 for memory efficiency</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>y</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://juanlebrero.com/tags/lora/>LoRA</a></li><li><a href=https://juanlebrero.com/tags/qlora/>QLoRA</a></li><li><a href=https://juanlebrero.com/tags/llms/>LLMs</a></li><li><a href=https://juanlebrero.com/tags/finetuning/>Finetuning</a></li><li><a href=https://juanlebrero.com/tags/quantization/>Quantization</a></li><li><a href=https://juanlebrero.com/tags/4-bit/>4-Bit</a></li><li><a href=https://juanlebrero.com/tags/deepspeed/>Deepspeed</a></li><li><a href=https://juanlebrero.com/tags/fsdp/>Fsdp</a></li><li><a href=https://juanlebrero.com/tags/zero/>Zero</a></li><li><a href=https://juanlebrero.com/tags/precision/>Precision</a></li><li><a href=https://juanlebrero.com/tags/jax/>JAX</a></li><li><a href=https://juanlebrero.com/tags/bfloat16/>Bfloat16</a></li><li><a href=https://juanlebrero.com/tags/fp16/>Fp16</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://juanlebrero.com/>Juan Lebrero</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>