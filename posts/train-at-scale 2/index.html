<!DOCTYPE html>
<html lang="en"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=53744&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.
Numeric Precision
The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs.">  

  <title>
    
      Large-Scale Training: FSDP, QLoRA, and More.
    
  </title>

  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  
  
  <link rel="stylesheet" href="/css/main.28a7439b9550b83d3a5c7595d7a696931ebc2a2ad3d4ca19111b9e621784331c7904e46b7aff95891dc7f536272d85359b62fe0fe14c1c1841e57d9799b774a6.css" integrity="sha512-KKdDm5VQuD06XHWV16aWkx68KirT1MoZERueYheEMxx5BORrev&#43;ViR3H9TYnLYU1m2L&#43;D&#43;FMHBhB5X2Xmbd0pg==" />
  
  
  <link rel="stylesheet" href="/css/custom.css" />
  
   <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
</head>
<body a="auto">
<button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
  <span class="theme-icon">üåô</span>
</button>

<style>
.theme-toggle {
  background: none;
  border: none;
  cursor: pointer;
  font-size: 1.2em;
  padding: 0.5rem;
  border-radius: 50%;
  transition: all 0.3s ease;
  position: fixed;
  top: 1rem;
  right: 1rem;
  z-index: 1000;
  width: 2.5rem;
  height: 2.5rem;
  display: flex;
  align-items: center;
  justify-content: center;
}

.theme-toggle:hover {
  background-color: var(--primary-text-color, #000);
  color: var(--bg-color, #fff);
  transform: scale(1.1);
}

.theme-toggle:focus {
  outline: 2px solid var(--link-color, #3548cf);
  outline-offset: 2px;
}

.theme-icon {
  transition: transform 0.3s ease;
}

.theme-toggle:hover .theme-icon {
  transform: rotate(180deg);
}

 
@media (max-width: 768px) {
  .theme-toggle {
    top: 0.5rem;
    right: 0.5rem;
    width: 2rem;
    height: 2rem;
    font-size: 1em;
  }
}
</style>

<script>
(function() {
  'use strict';
  
  
  const THEME_KEY = 'theme-preference';
  const THEMES = {
    LIGHT: 'light', 
    DARK: 'dark'
  };
  
  
  const ICONS = {
    [THEMES.LIGHT]: '‚òÄÔ∏è',
    [THEMES.DARK]: 'üåô'
  };
  
  
  function getCurrentTheme() {
    return localStorage.getItem(THEME_KEY) || THEMES.LIGHT;
  }
  
  
  function saveTheme(theme) {
    localStorage.setItem(THEME_KEY, theme);
  }
  
  
  function applyTheme(theme) {
    document.body.setAttribute('a', theme);
  }
  
  
  function updateButtonIcon(theme) {
    const button = document.getElementById('theme-toggle');
    const icon = button.querySelector('.theme-icon');
    icon.textContent = ICONS[theme];
  }
  
  
  function cycleTheme() {
    const current = getCurrentTheme();
    let next;
    
    switch(current) {
      case THEMES.LIGHT:
        next = THEMES.DARK;
        break;
      case THEMES.DARK:
        next = THEMES.LIGHT;
        break;
      default:
        next = THEMES.LIGHT;
    }
    
    saveTheme(next);
    applyTheme(next);
    updateButtonIcon(next);
  }
  
  
  function initTheme() {
    const theme = getCurrentTheme();
    applyTheme(theme);
    updateButtonIcon(theme);
  }
  
  
  function addToggleListener() {
    const button = document.getElementById('theme-toggle');
    if (button) {
      button.addEventListener('click', cycleTheme);
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
      initTheme();
      addToggleListener();
    });
  } else {
    initTheme();
    addToggleListener();
  }
})();
</script>

<div class="lang-switch-container">
  <ul class="lang-switch">
    <li><a href="/es/posts/train-at-scale/" title="Espa√±ol" aria-label="Espa√±ol">ES</a>
    </li>
  </ul>
</div>
<main class="page-content" aria-label="Content">
            <div class="w"><nav class="breadcrumbs" aria-label="Breadcrumb">
  <ol><li>
      <a href="http://localhost:53744/">Juan Lebrero</a>
    </li><li>
      <a href="http://localhost:53744/posts/">Posts</a>
    </li><li aria-current="page">Large-Scale Training: FSDP, QLoRA, and More.</li>
  </ol>
</nav>
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2025-09-13 00:00:00 &#43;0000 UTC">
            2025-09-13
        </time>
    </p>

    <h1>Large-Scale Training: FSDP, QLoRA, and More.</h1>

    

    <p>To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.</p>
<h2 id="numeric-precision">Numeric Precision</h2>
<p>The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs.</p>
<h3 id="why-precision-matters">Why precision matters</h3>
<p>A computer is finite. Real numbers are not. If you try to represent something like $\pi$, which has infinitely many decimals, you need an approximation scheme. Floating point is that scheme.</p>
<p>Floating point approximates a real value using three parts: a sign, a mantissa that controls fine resolution within a fixed interval, and an exponent that sets the dynamic range so you can represent very large and very small magnitudes. In IEEE 754 the base is 2, so the usual model is</p>
<p>$$
\text{value} \approx \text{sign}\times \text{mantissa}\times \text{base}^{\text{exponent}}.
$$</p>
<p>More bits in the mantissa buy you finer resolution. More bits in the exponent buy you wider range.</p>
<figure>
 <img src="images/layout.png" alt="32-bit floating-point layout (FP32)" style="width: 100%; height: auto;">
 <figcaption style="text-align: center; font-size: 0.95em; color: #666;">
   Figure 1. FP32 layout under IEEE 754.
 </figcaption>
</figure>
<p>This matters for three simple reasons. Low-bit formats run faster on modern accelerators and use less memory. If the range or resolution is too small, you get overflows, underflows, or loss of significance that can wreck training. And if you want to train big LLMs without burning money, you lean on mixed precision to keep compute and memory in check.</p>
<figure>
 <img src="images/floating_point.png" alt="BF16 vs FP32 vs FP16" />
 <figcaption style="text-align: center; font-size: 0.95em; color: #666;">
   Figure 2. The formats you actually see in deep learning: <b>BF16</b>, <b>FP32</b>, <b>FP16</b>.
 </figcaption>
</figure>
<h3 id="fp32-single-precision">FP32 (single precision)</h3>
<p>Think of this as the baseline. It uses 1 bit for sign, 8 for exponent, and 23 for mantissa. The range spans roughly $1.18\times10^{-38}$ to $3.4\times10^{38}$ and machine epsilon is about $1.19\times10^{-7}$. Even if you store tensors in lower precision to save memory, critical accumulations are often kept in FP32 to keep training steady.</p>
<h3 id="fp16-half">FP16 (half)</h3>
<p>FP16 cuts memory and usually speeds things up on the right hardware. The tradeoff is less detail and smaller range. Tiny numbers can vanish and large ones can saturate. That is why people apply loss scaling: you temporarily scale up losses and gradients so they sit in a safe numerical range, then scale updates back down.</p>
<h3 id="bf16-brain-floating-point">BF16 (brain floating point)</h3>
<p>BF16 is the current workhorse on TPUs and recent GPUs like H100. It keeps the FP32-sized exponent so the dynamic range matches FP32, but trims mantissa bits. In practice this avoids most overflow and underflow issues without resorting to tricks like loss scaling. You give up some decimal detail, which is usually fine for large-scale training.</p>
<h2 id="comparing-precisions">Comparing precisions</h2>
<p>I will use JAX for quick experiments. It makes it easy to run on GPUs and TPUs and later helps show parallelism and memory behavior more transparently than other stacks.</p>
<p>First, query versions, backend, and devices:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax</span>
</span></span><span class="line"><span class="cl"><span class="c1"># JAX setup</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;JAX version: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;JAX backend: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">default_backend</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Available devices: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>I will time code and inspect memory with <code>psutil</code>, <code>tracemalloc</code>, and <code>time</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">psutil</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tracemalloc</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_memory_usage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">   <span class="s2">&#34;&#34;&#34;Return current process RSS in MB.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">measure_memory_and_time</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">       <span class="n">tracemalloc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">start_memory</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">jax</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">end_memory</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">current</span><span class="p">,</span> <span class="n">peak</span> <span class="o">=</span> <span class="n">tracemalloc</span><span class="o">.</span><span class="n">get_traced_memory</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">tracemalloc</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;result&#39;</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;execution_time&#39;</span><span class="p">:</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;memory_delta&#39;</span><span class="p">:</span> <span class="n">end_memory</span> <span class="o">-</span> <span class="n">start_memory</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;peak_memory&#39;</span><span class="p">:</span> <span class="n">peak</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;current_memory&#39;</span><span class="p">:</span> <span class="n">current</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">       <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">wrapper</span>
</span></span></code></pre></div><p>Now a matrix multiply micro-bench and a tiny feedforward pass, both wrapped with the helper above.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@measure_memory_and_time</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">matrix_multiplication_test</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="s2">&#34;&#34;&#34;Matrix multiply at a given dtype.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="k">def</span> <span class="nf">matmul_operation</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">       <span class="n">a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key1</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">matmul_operation</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@measure_memory_and_time</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">neural_network_forward_pass_test</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="s2">&#34;&#34;&#34;One forward pass of a tiny MLP.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="k">def</span> <span class="nf">nn_forward</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">       <span class="c1"># Weight init</span>
</span></span><span class="line"><span class="cl">       <span class="n">W1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">b1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">W2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">b2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="c1"># Input</span>
</span></span><span class="line"><span class="cl">       <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="c1"># Forward</span>
</span></span><span class="line"><span class="cl">       <span class="n">h</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">nn_forward</span><span class="p">()</span>
</span></span></code></pre></div><p>Run like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">matrix_multiplication_test</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">neural_network_forward_pass_test</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span></code></pre></div><p>I will also include FP64 to show a result that looks counterintuitive at first glance. Results depend on hardware. On an M1 the numbers looked like this, time in seconds and memory in MB.</p>
<div style="display: flex; gap: 2px; flex-wrap: wrap;">
<div style="flex: 1; min-width: 300px;">
<p><strong>Matrix multiplication</strong></p>
<table>
  <thead>
      <tr>
          <th>Precision</th>
          <th>Time</th>
          <th>Peak Memory</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FP16</td>
          <td>1.024</td>
          <td>0.013</td>
      </tr>
      <tr>
          <td>BF16</td>
          <td>0.978</td>
          <td>0.008</td>
      </tr>
      <tr>
          <td>FP32</td>
          <td>0.943</td>
          <td>0.008</td>
      </tr>
      <tr>
          <td>FP64</td>
          <td><strong>0.928</strong></td>
          <td>0.009</td>
      </tr>
  </tbody>
</table>
</div>
<div style="flex: 1; min-width: 300px;">
<p><strong>Neural network</strong></p>
<table>
  <thead>
      <tr>
          <th>Precision</th>
          <th>Time</th>
          <th>Peak Memory</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FP16</td>
          <td>0.007</td>
          <td>0.020</td>
      </tr>
      <tr>
          <td>BF16</td>
          <td>0.004</td>
          <td>0.015</td>
      </tr>
      <tr>
          <td>FP32</td>
          <td>0.003</td>
          <td>0.015</td>
      </tr>
      <tr>
          <td>FP64</td>
          <td><strong>0.002</strong></td>
          <td>0.016</td>
      </tr>
  </tbody>
</table>
</div>
</div>
<p>If you stop here you might think the whole discussion is useless because FP64 came out faster. That reading is wrong. CPUs like the M1 are tuned for FP32 and FP64 through vendor libraries such as Accelerate. FP16 and BF16 often get converted behind the scenes when you run on CPU, which adds overhead that dominates at this tiny scale. These are small problems, so a lot of the runtime is setup and conversion, not the math. And on GPUs the story flips because Tensor Cores are built to chew through FP16 and BF16 while moving less data.</p>
<p>If you repeat these tests on a GPU with much larger matrices, you will see the expected speedups. A simple TFLOP comparison from vendor docs makes the point.</p>
<div style="display: flex; justify-content: center;">
  <figure>
    <img src="images/nvidia-a100-matmul-tflops.png" alt="Throughput vs precision">
    <figcaption style="text-align: center; font-size: 0.95em; color: #666;">
      Figure 3. Throughput grows as precision drops on matrix ops.
    </figcaption>
  </figure>
</div>
<p>To wrap this section, here is a minimal mixed-precision example. Do the numerically sensitive work in FP32, then cast results to a cheaper format for storage.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mixed_precision_forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="c1"># Promote to FP32 for compute</span>
</span></span><span class="line"><span class="cl">   <span class="n">x_fp32</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">W_fp32</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">b_fp32</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="c1"># Forward</span>
</span></span><span class="line"><span class="cl">   <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_fp32</span><span class="p">,</span> <span class="n">W_fp32</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fp32</span>
</span></span><span class="line"><span class="cl">   <span class="c1"># Store in FP16 to save memory</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="quantization">Quantization</h2>
<p>Training often keeps critical math in FP32 and uses FP16 for the rest. Deployment is a different game. You want lower latency and smaller memory footprints without trashing quality. Quantization does exactly that by representing weights and activations with fewer bits and accepting a controlled numeric error. You usually try Post-Training Quantization first. If that is not enough, you switch to Quantization-Aware Training.</p>
<div style="display: flex; flex-direction: column; align-items: center; margin: 2em 0;">
  <img src="images/Quantization-Aware-TrainingQAT-and-Post-Training-Quantization-PTQ.png" alt="QAT versus PTQ" style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
  <p style="text-align: center; margin-top: 0.5em; color: #555;">
    Figure 4. QAT on the left, PTQ on the right.
  </p>
</div>
<h3 id="post-training-quantization-ptq">Post-Training Quantization (PTQ)</h3>
<p>PTQ takes a trained model and converts weights and activations to a low-bit format without retraining. Most of the time this is enough.</p>
<p>Starting from a real value $x\in\mathbb{R}$, the standard affine uniform quantizer maps $x$ to an integer $q$ with $b$ bits using a scale $s$ and a zero point $z$:</p>
<p>$$
q = \operatorname{clip}\Big(\operatorname{round}\big(\tfrac{x}{s}\big) + z,\ q_{\min}, q_{\max}\Big),
\qquad
\hat{x} = s,(q - z).
$$</p>
<p>For signed $b$-bit integers, $q_{\min}=-2^{b-1}$ and $q_{\max}=2^{b-1}-1$. Given a target real interval $[\alpha,\beta]$ you get a practical choice</p>
<p>$$
s=\frac{\beta-\alpha}{q_{\max}-q_{\min}},
\qquad
z=\operatorname{round}\Big(\frac{-\alpha}{s}\Big)+q_{\min}.
$$</p>
<p>If $z=0$ you have symmetric quantization, which centers the integer range around zero. If your distribution is skewed, an asymmetric choice with $z\neq 0$ shifts the range and uses the available levels better.</p>
<p>Here is a simple reference in Python:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">quantize_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">signed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">signed</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmin</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmax</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmin</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmax</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x_min</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_max</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">qmax</span> <span class="o">-</span> <span class="n">qmin</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">qmin</span> <span class="o">-</span> <span class="n">x_min</span> <span class="o">/</span> <span class="p">(</span><span class="n">scale</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="n">scale</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">zero_point</span><span class="p">),</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="n">zero_point</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)</span>
</span></span></code></pre></div><p>Weights are often fine with per-tensor or per-channel min-max because their distributions are roughly centered. Activations are different. ReLU produces only nonnegative values, so a symmetric scheme wastes half the codes on unused negatives. You solve that with asymmetric or unsigned quantization and by calibrating with a representative dataset so you do not pick ranges that get dominated by outliers. Percentile-based clipping is common because it trades a tiny bias for fewer saturation errors.</p>
<div style="display: flex; flex-direction: column; align-items: center; margin: 2em 0;">
  <img src="images/quantization_cropped.png" alt="Symmetric vs asymmetric quantization" style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
  <p style="text-align: center; margin-top: 0.5em; color: #555;">
    Figure 5. Symmetric compared to asymmetric quantization.
  </p>
</div>
<p>If accuracy drops too much with PTQ, move to QAT.</p>
<h3 id="quantization-aware-training-qat">Quantization-Aware Training (QAT)</h3>
<p>QAT trains a model while simulating the quantizer that will be used at inference time. You optimize</p>
<div class="math">$$
\min_{w}\ \mathbb{E}_{(x,y)\sim\mathcal{D}}\Big[L\big(f_{q(w)}(x),\,y\big)\Big].
$$</div>
<p>Here $w$ are the real-valued parameters before quantization. $f_{q(w)}$ is the same network but with weights passed through a quantizer $q(\cdot)$. $L$ is your loss. If you define $q$ with literal rounding and clipping you break differentiability. The usual uniform affine quantizer is</p>
<div class="math">$$
q=\operatorname{clip}\!\big(\operatorname{round}(u),\,q_{\min},\,q_{\max}\big),\quad
u=\frac{w}{s}+z,\quad
\tilde w=s\,(q-z),
$$</div>
<p>and you compute the forward with $\tilde w$, not $w$. To backpropagate you use the straight-through estimator. Treat rounding as the identity in the backward pass and give clipping a derivative of one inside the valid range and zero outside:</p>
<div class="math">$$
\frac{\partial \tilde w}{\partial w}\ \approx\
\begin{cases}
1 & \text{if } q_{\min} < u < q_{\max} \\
0 & \text{otherwise}
\end{cases}
$$</div>
<p>That way gradients flow as long as values stay within range. If you also learn the scale $s$, you keep using STE and get</p>
<div class="math">$$
\frac{\partial \tilde w}{\partial s}\ \approx\
\begin{cases}
-\,z\;-\;\frac{w}{s} & \text{if } q_{\min} < u < q_{\max} \\
\operatorname{clip}(u,\,q_{\min},\,q_{\max})\;-\;z\;-\;\frac{w}{s} & \text{otherwise}
\end{cases}
$$</div>
<p>In practice you normalize this gradient by the number of elements that share the same scale. That is where per-tensor and per-channel schemes show up.</p>
<p>Consider a linear layer with weights $W\in\mathbb{R}^{C_o\times C_i}$ and activations $X\in\mathbb{R}^{T\times C_i}$. Per-tensor uses a single scale for the whole $W$ and another for the whole $X$. It is simple and cheap but wastes resolution if ranges differ a lot internally. Per-channel assigns a different scale to each output channel of $W$, which you can think of as a vector $\Delta_W\in\mathbb{R}^{1\times C_o}$. If you also apply per-token on activations you get $\Delta_X\in\mathbb{R}^{T\times 1}$ so each row of $X$ has its own scale. This aligns with the real heterogeneity of values and reduces error at 8 bits and especially at 4 bits, at the cost of extra scale storage.</p>
<p align="center">
  <img src="images/per-channel_per-tensor.png" alt="Per-tensor and per-channel quantization" style="max-width: 100%; height: auto;">
</p>
<p align="center" style="color:#555; margin-top:0.5em;">
  \(X\in\mathbb{R}^{T\times C_i}\) are activations and \(W\in\mathbb{R}^{C_o\times C_i}\) are weights. The top diagram shows per-tensor with single scales \(\Delta X[1]\) and \(\Delta W[1]\). The bottom shows per-token plus per-channel with \(\Delta X[T\times 1]\) and \(\Delta W[1\times C_o]\). Dashed boxes indicate the region each scale covers.
</p>
<p>Training reduces to standard empirical risk with mini-batches while applying the same quantization scheme in the forward. For each step you compute outputs with $\tilde w=s,(q-z)$ using the chosen granularity for weights and, if applicable, activations. You evaluate the loss, backpropagate with STE, normalize scale gradients by the number of elements they cover, and update $w$ plus scales if they are learnable:</p>
<div class="math">$$
\min_{w}\ \frac{1}{B}\sum_{i=1}^{B} L\big(f_{\tilde w}(x_i),\,y_i\big).
$$</div>

</article>

                
<footer class="footer">
  <div class="footer-content">
    <p>&copy; 2025 Juan Lebrero. All rights reserved.</p>
    <div class="footer-links">
      <a href="/about/">About</a>
      <span class="separator">‚Ä¢</span>
      <a href="/contact/">Contact</a>
      <span class="separator">‚Ä¢</span>
      <a href="/index.xml">RSS</a>
    </div>
  </div>
</footer>

            </div>
        </main>
    </body>
</html>
