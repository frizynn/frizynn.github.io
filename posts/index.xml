<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Juan Lebrero</title><link>https://juanlebrero.com/posts/</link><description>Recent content in Posts on Juan Lebrero</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 30 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://juanlebrero.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>From Local to Global: A Deep Dive into GraphRAG</title><link>https://juanlebrero.com/posts/graph-rag/</link><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/posts/graph-rag/</guid><description>&lt;p&gt;RAG (&lt;em&gt;Retrieval-Augmented Generation&lt;/em&gt;) has established itself as the industry standard for mitigating hallucinations in Large Language Models (LLMs) by injecting reliable data during response generation. The mechanism is well-known: given a query, the system retrieves relevant text fragments (&amp;ldquo;chunks&amp;rdquo;) from a vector database and passes them as context to the model to formulate a grounded answer. This approach involves retrieving specific data points for targeted questions. However, its performance degrades significantly when the task requires a transversal understanding of an entire corpus, such as answering &lt;em&gt;&amp;ldquo;What are the patterns of technological evolution in these 10,000 reports?&amp;rdquo;&lt;/em&gt;. Vector similarity retrieval, by delivering isolated pieces, lacks the architecture necessary to synthesize a global overview.&lt;/p&gt;</description></item><item><title>Masked Autoencoders: Because Suffering Builds Character (Even for AI)</title><link>https://juanlebrero.com/posts/vit-mae/</link><pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/posts/vit-mae/</guid><description>&lt;p&gt;Imagine trying to learn a language by only seeing 25% of each sentence. That sounds like a terrible idea, right? Yet this is exactly what Vision Transformer Masked Autoencoders (ViT-MAE) do with images. The paper &amp;ldquo;Masked Autoencoders Are Scalable Vision Learners&amp;rdquo; by He et al. showed that hiding 75% of an image (by splitting it into patches and randomly masking 75% of them) and asking a model to reconstruct it produces remarkably powerful visual representations. It turns out that what you don&amp;rsquo;t see can teach you a lot about what you do.&lt;/p&gt;</description></item></channel></rss>