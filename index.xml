<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Juan Lebrero</title><link>https://juanlebrero.com/</link><description>Recent content on Juan Lebrero</description><generator>Hugo</generator><language>en</language><lastBuildDate>Fri, 02 Jan 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://juanlebrero.com/index.xml" rel="self" type="application/rss+xml"/><item><title>LightRAG: The Evolution of Graph-Based RAG Systems</title><link>https://juanlebrero.com/posts/light-rag/</link><pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/posts/light-rag/</guid><description>&lt;p&gt;A few weeks ago I wrote about &lt;a href="https://juanlebrero.com/en/posts/graph-rag/"&gt;GraphRAG&lt;/a&gt; and how it changed the way we think about information retrieval using knowledge graphs. It was a huge advancement because it stopped relying exclusively on finding similar text and started understanding implicit relationships between entities.&lt;/p&gt;
&lt;p&gt;But after implementing it and using it in projects, I encountered some friction points. The most obvious one is having to choose between &amp;ldquo;Local&amp;rdquo; mode and &amp;ldquo;Global&amp;rdquo; mode before making a query. That works well in theory, but in practice it forces you to think: &amp;ldquo;Does this question need specific detail or broad synthesis?&amp;rdquo; And the answer is often &amp;ldquo;both,&amp;rdquo; which leaves you with a design problem.&lt;/p&gt;</description></item><item><title>From Local to Global: A Deep Dive into GraphRAG</title><link>https://juanlebrero.com/posts/graph-rag/</link><pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/posts/graph-rag/</guid><description>&lt;p&gt;RAG (&lt;em&gt;Retrieval-Augmented Generation&lt;/em&gt;) has established itself as the industry standard for mitigating hallucinations in Large Language Models (LLMs) by injecting reliable data during response generation. The mechanism is well-known: given a query, the system retrieves relevant text fragments (&amp;ldquo;chunks&amp;rdquo;) from a vector database and passes them as context to the model to formulate a grounded answer. This approach involves retrieving specific data points for targeted questions. However, its performance degrades significantly when the task requires a transversal understanding of an entire corpus, such as answering &lt;em&gt;&amp;ldquo;What are the patterns of technological evolution in these 10,000 reports?&amp;rdquo;&lt;/em&gt;. Vector similarity retrieval, by delivering isolated pieces, lacks the architecture necessary to synthesize a global overview.&lt;/p&gt;</description></item><item><title>Masked Autoencoders: Because Suffering Builds Character (Even for AI)</title><link>https://juanlebrero.com/posts/vit-mae/</link><pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/posts/vit-mae/</guid><description>&lt;p&gt;Imagine trying to learn a language by only seeing 25% of each sentence. That sounds like a terrible idea, right? Yet this is exactly what Vision Transformer Masked Autoencoders (ViT-MAE) do with images. The paper &amp;ldquo;Masked Autoencoders Are Scalable Vision Learners&amp;rdquo; by He et al. showed that hiding 75% of an image (by splitting it into patches and randomly masking 75% of them) and asking a model to reconstruct it produces remarkably powerful visual representations. It turns out that what you don&amp;rsquo;t see can teach you a lot about what you do.&lt;/p&gt;</description></item><item><title>About</title><link>https://juanlebrero.com/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/about/</guid><description>&lt;p&gt;I am Juan Francisco Lebrero, an AI enthusiast dedicated to exploring how artificial intelligence can transform our reality. My journey is defined by a search for solutions that are not only technically rigorous but also hold tangible impact in the world around us. Currently, I am pursuing a B.Sc. in Artificial Intelligence Engineering at the University of San Andr√©s (UdeSA), where I delve into the frontiers of this discipline.&lt;/p&gt;
&lt;h2 id="a-journey-through-innovation"&gt;A Journey through Innovation&lt;/h2&gt;
&lt;p&gt;In my current role as a Research Assistant at LiNAR, I focus on one of the most compelling applications of computer vision: the non-invasive assessment of embryos in IVF processes. My work involves developing models to predict viability without the need for biopsies, utilizing advanced time-lapse imaging techniques.&lt;/p&gt;</description></item><item><title>Contact</title><link>https://juanlebrero.com/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juanlebrero.com/contact/</guid><description>&lt;p&gt;I&amp;rsquo;m always interested in connecting with fellow ML engineers, researchers, and anyone passionate about AI!&lt;/p&gt;
&lt;h2 id="get-in-touch"&gt;Get in Touch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt;: &lt;a href="https://www.linkedin.com/in/lebrero-juan-francisco/"&gt;Juan Francisco Lebrero&lt;/a&gt; - Professional networking&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: &lt;a href="mailto:lebrerojuanfrancisco@gmail.com"&gt;lebrerojuanfrancisco@gmail.com&lt;/a&gt; - Direct communication&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-im-interested-in"&gt;What I&amp;rsquo;m Interested In&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;: Open to discussing ML projects and research&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speaking&lt;/strong&gt;: Available for talks on large-scale training and ML engineering&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consulting&lt;/strong&gt;: Interested in helping with ML infrastructure and training challenges&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mentoring&lt;/strong&gt;: Happy to share knowledge and help others in the field&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="response-time"&gt;Response Time&lt;/h2&gt;
&lt;p&gt;I typically respond to messages within 24-48 hours. For urgent matters, Mail is usually the fastest way to reach me.&lt;/p&gt;</description></item></channel></rss>