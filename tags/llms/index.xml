<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLMs on Juan Lebrero</title>
    <link>http://localhost:1313/tags/llms/</link>
    <description>Recent content in LLMs on Juan Lebrero</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 13 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Large-Scale Training: FSDP, QLoRA, and More.</title>
      <link>http://localhost:1313/posts/train-at-scale/</link>
      <pubDate>Sat, 13 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/train-at-scale/</guid>
      <description>&lt;p&gt;To train models at scale you need a solid grip on a handful of ideas that decide both speed and stability. This walkthrough focuses on numeric precision, data parallelism, quantization, LoRA, and a few other pieces you actually use in practice.&lt;/p&gt;&#xA;&lt;h2 id=&#34;numeric-precision&#34;&gt;Numeric Precision&lt;/h2&gt;&#xA;&lt;p&gt;The numeric format you choose (FP32, FP16, BF16, FP8, INT8, and so on) drives throughput, memory footprint, and training stability. You cannot ignore it if you care about scaling. This section explains how precision works and how it shows up in real training runs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
