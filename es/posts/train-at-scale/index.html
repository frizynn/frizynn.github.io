<!doctype html><html lang=es><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisi√≥n num√©rica, paralelizaci√≥n de datos, cuantizaci√≥n, LoRA, y m√°s.
Precisi√≥n num√©rica
La elecci√≥n del formato num√©rico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores m√°s determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisi√≥n num√©rica y como afecta al rendimiento de los modelos, lo que se explicar√° en esta secci√≥n."><title>Entrenamiento a Gran Escala: FSDP, QLoRA, y m√°s.</title><link rel="shortcut icon" type=image/x-icon href=/><link rel=stylesheet href=/css/main.5434bd314af44f6353b8b35a80419909bf984c0348babb07e3b86a5efd2fc42db5940d0d9b717d017ffd0d10fbd3ecc0940019697fd843005791d04967f8f4a4.css integrity="sha512-VDS9MUr0T2NTuLNagEGZCb+YTANIursH47hqXv0vxC21lA0Nm3F9AX/9DRD70+zAlAAZaX/YQwBXkdBJZ/j0pA=="><link rel=stylesheet href=/css/custom.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body a=auto><button id=theme-toggle class=theme-toggle aria-label="Toggle theme">
<span class=theme-icon>üåô</span></button><style>.theme-toggle{background:0 0;border:none;cursor:pointer;font-size:1.2em;padding:.5rem;border-radius:50%;transition:all .3s ease;position:fixed;top:1rem;right:1rem;z-index:1000;width:2.5rem;height:2.5rem;display:flex;align-items:center;justify-content:center}.theme-toggle:hover{background-color:var(--primary-text-color,#000);color:var(--bg-color,#fff);transform:scale(1.1)}.theme-toggle:focus{outline:2px solid var(--link-color,#3548cf);outline-offset:2px}.theme-icon{transition:transform .3s ease}.theme-toggle:hover .theme-icon{transform:rotate(180deg)}@media(max-width:768px){.theme-toggle{top:.5rem;right:.5rem;width:2rem;height:2rem;font-size:1em}}</style><script>(function(){"use strict";const t="theme-preference",e={LIGHT:"light",DARK:"dark"},r={[e.LIGHT]:"‚òÄÔ∏è",[e.DARK]:"üåô"};function n(){return localStorage.getItem(t)||e.LIGHT}function c(e){localStorage.setItem(t,e)}function s(e){document.body.setAttribute("a",e)}function o(e){const t=document.getElementById("theme-toggle"),n=t.querySelector(".theme-icon");n.textContent=r[e]}function l(){const i=n();let t;switch(i){case e.LIGHT:t=e.DARK;break;case e.DARK:t=e.LIGHT;break;default:t=e.LIGHT}c(t),s(t),o(t)}function i(){const e=n();s(e),o(e)}function a(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",l)}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",function(){i(),a()}):(i(),a())})()</script><div class=lang-switch-container><ul class=lang-switch><li><a href=/posts/train-at-scale/ title=English aria-label=English>EN</a></li></ul></div><main class=page-content aria-label=Content><div class=w><nav class=breadcrumbs aria-label=Breadcrumb><ol><li><a href=https://juanlebrero.com/es/>Juan Lebrero</a></li><li><a href=https://juanlebrero.com/es/posts/>Posts</a></li><li aria-current=page>Entrenamiento a Gran Escala: FSDP, QLoRA, y m√°s.</li></ol></nav><a href=/>..</a><article><p class=post-meta><time datetime="2025-09-13 00:00:00 +0000 UTC">2025-09-13</time></p><h1>Entrenamiento a Gran Escala: FSDP, QLoRA, y m√°s.</h1><p>Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisi√≥n num√©rica, paralelizaci√≥n de datos, cuantizaci√≥n, LoRA, y m√°s.</p><h2 id=precisi√≥n-num√©rica>Precisi√≥n num√©rica</h2><p>La elecci√≥n del formato num√©rico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores m√°s determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisi√≥n num√©rica y como afecta al rendimiento de los modelos, lo que se explicar√° en esta secci√≥n.</p><h3 id=por-qu√©-importa-la-precisi√≥n>¬øPor qu√© Importa la Precisi√≥n?</h3><p>¬øComo representar√≠as el n√∫mero $\pi$, un n√∫mero con decimales INFINITOS, en algo FINITO como lo es una computadora? De est√° pregunta, surge como respuesta el punto flotante.</p><p>Los n√∫meros representados con punto flotante representan, de manera aproximada, a los n√∫meros reales, y lo hacen con dos componentes clave: la <em>mantisa</em> y el <em>exponente</em>.</p><p>Un n√∫mero en punto flotante representa aproximadamente un valor real mediante la f√≥rmula:</p><p>$$\text{valor} \approx \text{signo} \times \text{mantisa} \times \text{base}^{\text{exponente}}$$</p><p>donde:</p><ul><li><strong>Mantisa</strong>: Controla la resoluci√≥n fina (cu√°ntos pasos discretos entra en el intervalo 1.0 - 2.0)</li><li><strong>Exponente</strong>: Determina el rango din√°mico (qu√© tan grandes o peque√±os pueden ser los n√∫meros representables)</li><li><strong>Base</strong>: En IEEE 754 es 2.</li></ul><p>M√°s bits para la mantisa $\rightarrow$ mayor precisi√≥n</p><p>M√°s bits para el exponente $\rightarrow$ mayor rango</p><figure><img src=images/layout.png alt="Representaci√≥n de Punto Flotante de 32 bits (FP32)" style=width:100%;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 1. Esquema de la representaci√≥n de un n√∫mero en punto flotante de 32 bits (FP32) seg√∫n el est√°ndar IEEE 754.<br></figcaption></figure><p>Pero, ¬øpor qu√© nos interesa a nosotros?</p><p>Bueno, hay tres razones principales por las que la precisi√≥n num√©rica es crucial en el entrenamiento de modelos de gran escala:</p><ul><li><p><strong>Eficiencia computacional</strong>: Los formatos de menor ancho de bits aceleran el c√≥mputo en Tensor Cores/TPUs y reducen bastante el uso de memoria.</p></li><li><p><strong>Estabilidad num√©rica</strong>: B√°sicamente, si el formato de n√∫mero no tiene suficiente rango o detalle, los n√∫meros pueden volverse demasiado grandes, demasiado chiquitos o perder precisi√≥n, lo que puede causar errores o resultados raros durante el entrenamiento.</p></li><li><p><strong>Escabilidad</strong>: Cuando entrenamos LLMs a gran escala, aprovechar la precisi√≥n mixta es crucial para que el costo computacional no se nos vaya a la luna.</p></li></ul><figure><img src=images/floating_point.png alt="Comparativa de formatos de punto flotante: BF16, FP32 y FP16"><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 2. Comparaci√≥n visual de los formatos de punto flotante m√°s utilizados en deep learning: <b>BF16</b>, <b>FP32</b> y <b>FP16</b>.<br></figcaption></figure><h3 id=fp32-ieee-754-precisi√≥n-simple>FP32 (IEEE 754, precisi√≥n simple)</h3><p>Este es el formato ‚Äúnormal‚Äù que se usa casi siempre. Guarda los n√∫meros usando 1 bit para el signo, 8 para el exponente y 23 para la parte decimal (mantisa). Puede representar n√∫meros muy chicos y muy grandes, desde $1.18\times10^{-38}$ hasta $3.4\times10^{38}$, y su precisi√≥n es muy alta ($\varepsilon \approx 1.19\times10^{-7}$).</p><p>En machine learning, FP32 es lo que se considera ‚Äúprecisi√≥n completa‚Äù. Incluso cuando usamos otros formatos para ahorrar memoria, los c√°lculos importantes (como acumular los gradientes) se hacen en FP32 para que el entrenamiento no se vuelva inestable.</p><h3 id=fp16-ieee-754-half>FP16 (IEEE 754, half)</h3><p>FP16 es un formato de n√∫mero que usa menos memoria y permite que todo vaya m√°s r√°pido. B√°sicamente, guarda los n√∫meros usando menos bits que el formato normal (FP32), as√≠ que ocupa menos espacio y acelera los c√°lculos.</p><p>Lo bueno: hace que entrenar y usar modelos sea m√°s r√°pido y barato. Lo malo: como tiene menos detalle y menos rango, a veces los n√∫meros muy chicos pueden desaparecer (por eso se suele usar <a href=https://picdictionary.com/ml-dictionary/loss-scaling-in-ai-and-deep-learning target=_blank rel=noopener>loss scaling</a> para evitarlo), y si los n√∫meros son muy grandes, se pueden &ldquo;saturar&rdquo; y perder informaci√≥n.</p><h3 id=bf16-brain-floating-point>BF16 (Brain Floating Point)</h3><p>BF16 es el formato que m√°s se usa hoy para entrenar modelos grandes en TPUs y GPUs modernas (como la H100).</p><p>Guarda los n√∫meros de una forma parecida a FP32 (el formato ‚Äúnormal‚Äù), pero con menos detalle en los decimales. Lo importante es que puede representar n√∫meros igual de grandes o chicos que FP32, as√≠ que no se ‚Äúrompe‚Äù con n√∫meros extremos. Adem√°s, casi siempre funciona bien sin tener que hacer trucos raros como el <em>loss scaling</em>. Aunque no tiene tanta precisi√≥n en los decimales como FP16, para entrenar modelos grandes (como los LLMs) suele ser suficiente y no da problemas.</p><h2 id=comparaci√≥n-de-precisiones>Comparaci√≥n de precisiones</h2><p>Para comparar las precisiones, voy a usar JAX, un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La raz√≥n de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitir√° m√°s adelante ver en &ldquo;crudo&rdquo; la paralelizaci√≥n de las operaciones y la optimizaci√≥n de la memoria.</p><p>Primero, importamos JAX y vemos la versi√≥n y el backend, as√≠ como los dispositivos disponibles:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax</span>
</span></span><span class=line><span class=cl><span class=c1># Configuraci√≥n de JAX</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX version: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX backend: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>default_backend</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Available devices: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>devices</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecuci√≥n. Para esto, vamos a usar <code>psutil</code>, <code>tracemalloc</code> y <code>time</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>psutil</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tracemalloc</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_memory_usage</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Obtiene el uso actual de memoria en MB&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>process</span> <span class=o>=</span> <span class=n>psutil</span><span class=o>.</span><span class=n>Process</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>process</span><span class=o>.</span><span class=n>memory_info</span><span class=p>()</span><span class=o>.</span><span class=n>rss</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>measure_memory_and_time</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>wrapper</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>start_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>result</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>jax</span><span class=o>.</span><span class=n>block_until_ready</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=n>end_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>current</span><span class=p>,</span> <span class=n>peak</span> <span class=o>=</span> <span class=n>tracemalloc</span><span class=o>.</span><span class=n>get_traced_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>result</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;execution_time&#39;</span><span class=p>:</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;memory_delta&#39;</span><span class=p>:</span> <span class=n>end_memory</span> <span class=o>-</span> <span class=n>start_memory</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;peak_memory&#39;</span><span class=p>:</span> <span class=n>peak</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;current_memory&#39;</span><span class=p>:</span> <span class=n>current</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>       <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>wrapper</span>
</span></span></code></pre></div><p>Ahora, vamos a medir el rendimiento y la memoria de la multiplicaci√≥n de matrices y la &ldquo;red neuronal&rdquo;. Para esto, vamos a usar el decorador <code>measure_memory_and_time</code> que definimos anteriormente.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>matrix_multiplication_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Prueba de multiplicaci√≥n de matrices con precisi√≥n dada&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>key1</span><span class=p>,</span> <span class=n>key2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>matmul_operation</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=n>a</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key1</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key2</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>matmul_operation</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Prueba de pase forward de red neuronal simple&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>keys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>nn_forward</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=c1># Inicializaci√≥n de pesos</span>
</span></span><span class=line><span class=cl>       <span class=n>W1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>W2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>output_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=c1># Datos de entrada</span>
</span></span><span class=line><span class=cl>       <span class=n>x</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=c1># Pase forward</span>
</span></span><span class=line><span class=cl>       <span class=n>h</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>nn_forward</span><span class=p>()</span>
</span></span></code></pre></div><p>Para correr las pruebas, simplemente llamamos a las funciones <code>matrix_multiplication_test</code> y <code>neural_network_forward_pass_test</code> con los tipos de precisi√≥n y las dimensiones de las matrices y la red neuronal, por ejemplo:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>matrix_multiplication_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=p>(</span><span class=mi>5000</span><span class=p>,</span> <span class=mi>5000</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><p>Para correr las pruebas, voy a incluir tambi√©n FP64, para mostrar algo que puede ser contraintuitivo.</p><p>Dependiendo en qu√© hardware las estemos corriendo, los resultados pueden variar bastante.
Corriendo las pruebas en un M1, obtenemos los siguientes resultados. Tiempo en segundos y memoria en MB:</p><div style=display:flex;gap:2px;flex-wrap:wrap><div style=flex:1;min-width:300px><p><strong>Multiplicaci√≥n de matrices:</strong></p><table><thead><tr><th>Precisi√≥n</th><th>Tiempo</th><th>Memoria Pico</th></tr></thead><tbody><tr><td>FP16</td><td>1.024</td><td>0.013</td></tr><tr><td>BF16</td><td>0.978</td><td>0.008</td></tr><tr><td>FP32</td><td>0.943</td><td>0.008</td></tr><tr><td>FP64</td><td><strong>0.928</strong></td><td>0.009</td></tr></tbody></table></div><div style=flex:1;min-width:300px><p><strong>Red neuronal:</strong></p><table><thead><tr><th>Precisi√≥n</th><th>Tiempo</th><th>Memoria Pico</th></tr></thead><tbody><tr><td>FP16</td><td>0.007</td><td>0.020</td></tr><tr><td>BF16</td><td>0.004</td><td>0.015</td></tr><tr><td>FP32</td><td>0.003</td><td>0.015</td></tr><tr><td>FP64</td><td><strong>0.002</strong></td><td>0.016</td></tr></tbody></table></div></div><p>A simple vista, uno pensar√≠a que este blog no sirve de nada y que todo lo que dije es una farsa porque &ldquo;FP64 dio mejor&rdquo;. Pero en realidad no es as√≠. Estos resultados se explican por varios factores:</p><ol><li><p><strong>CPUs est√°n optimizadas para ciertos tipos</strong>: Los procesadores como el M1 funcionan mejor con FP32 y FP64 porque las librer√≠as que usan (como Accelerate en Mac) est√°n hechas para esos formatos. En cambio, FP16 y BF16 no est√°n tan bien soportados en CPU, as√≠ que muchas veces el sistema tiene que convertirlos a FP32 o FP64 antes de hacer las cuentas, y eso las hace m√°s lentas cuando el problema es chico.</p></li><li><p><strong>El tama√±o importa</strong>: En los ejemplos de las tablas, las matrices y redes son chicas. Cuando los datos son peque√±os, la mayor parte del tiempo se va en preparar todo (inicializar, convertir tipos, sincronizar), no en hacer las cuentas en s√≠. Por eso, a veces FP64 parece ‚Äúm√°s r√°pido‚Äù, pero es porque el camino para ese tipo es m√°s directo y est√° mejor optimizado.</p></li><li><p><strong>En GPU es al rev√©s</strong>: En las GPUs, FP16 y BF16 son mucho m√°s r√°pidos porque el hardware tiene partes especiales (como Tensor Cores en NVIDIA) que est√°n hechas para trabajar con estos formatos de baja precisi√≥n y pueden hacer muchas operaciones a la vez, usando menos memoria y ancho de banda.</p></li></ol><p>Para que lo comprueben ustedes mismos, si se ejecutan las pruebas en una GPU usando matrices de dimensiones mucho mayores, se va a ver la diferencia. A continuaci√≥n muestro una gr√°fica que la ilustra claramente medido en TFLOPS (Teraflops), calculado como el n√∫mero de operaciones de punto flotante por segundo.</p><div style=display:flex;justify-content:center><figure><img src=images/nvidia-a100-matmul-tflops.png alt="Comparaci√≥n de precisiones" style=max-width:350px;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 3. Comparaci√≥n de precisiones.<br></figcaption></figure></div><p>Por √∫timo, a modo de conclusi√≥n de esta secci√≥n, les dejo un ejemplo de implementaci√≥n de precisi√≥n mixta, que es lo que suele hacerse en la pr√°ctica para entrenar modelos a gran escala.</p><p>La idea central es simple: las partes del modelo que requieren estabilidad num√©rica se calculan en FP32, mientras que los resultados intermedios, gradientes y par√°metros se almacenan en FP16 o BF16, aprovechando as√≠ el ahorro de memoria y el mayor throughput del hardware.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>mixed_precision_forward_pass</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=c1># Convertir a FP32 para c√≥mputo</span>
</span></span><span class=line><span class=cl>   <span class=n>x_fp32</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>W_fp32</span> <span class=o>=</span> <span class=n>W</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>b_fp32</span> <span class=o>=</span> <span class=n>b</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Pase forward</span>
</span></span><span class=line><span class=cl>   <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_fp32</span><span class=p>,</span> <span class=n>W_fp32</span><span class=p>)</span> <span class=o>+</span> <span class=n>b_fp32</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Convertir de vuelta a FP16 para eficiencia de memoria</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>y</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=cuantizaci√≥n>Cuantizaci√≥n</h1><p>Cuando entrenamos modelos, solemos utilizar FP32 para los pesos y FP16 para los gradientes, pero al llevar el modelo a producci√≥n surge un desaf√≠o: necesitamos reducir el consumo de memoria y la latencia sin sacrificar la calidad. Ac√° entra en juego la cuantizaci√≥n. Este proceso consiste en representar los pesos y activaciones con menos bits, lo que permite ahorrar memoria y acelerar la inferencia, a cambio de introducir un peque√±o error num√©rico controlado.</p><p>Hay 2 enfoques principales: Post Training Quantization (PTQ) y Quantization Aware Training (QAT).</p><div style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><img src=images/Quantization-Aware-TrainingQAT-and-Post-Training-Quantization-PTQ.png alt="Comparaci√≥n entre QAT y PTQ" style="max-width:100%;height:auto;border:1px solid #ccc;border-radius:8px"><p style=text-align:center;margin-top:.5em;color:#555>Figura 4. Comparaci√≥n visual entre QAT (Izquierda) y PTQ (Derecha).</p></div><h2 id=post-training-quantization-ptq>Post Training Quantization (PTQ)</h2><p>La cuantizaci√≥n post entrenamiento, <strong>PTQ</strong>, toma un modelo ya entrenado y convierte pesos y activaciones a formatos de menor precisi√≥n sin volver a entrenar. La mayor√≠a de las veces es suficiente.</p><p>Matem√°ticamente, si partimos de un valor real $x\in\mathbb{R}$ en FP32, la cuantizaci√≥n cl√°sica mapea $x$ a un entero $q$ de $b$ bits mediante una escala $s$ y un zero point $z$:</p><p>$$
q = \operatorname{clip}\Big(\operatorname{round}\big(\tfrac{x}{s}\big) + z,; q_{\min}, q_{\max}\Big),
$$</p><p>y para reconstruir en reales usamos</p><p>$$
\hat{x} = s\cdot (q - z).
$$</p><p>El rango entero $[q_{\min},q_{\max}]$ depende de si usamos representaci√≥n signada o no. Para $b$ bits signados, t√≠picamente $q_{\min}=-2^{b-1}$ y $q_{\max}=2^{b-1}-1$. La elecci√≥n de $s$ y $z$ define cu√°nto error agregamos.</p><p>Si queremos cubrir un intervalo real $[\alpha,\beta]$ la escala pr√°ctica es:</p><p>$$
s=\frac{\beta-\alpha}{q_{\max}-q_{\min}},
\qquad
z=\operatorname{round}\Big(\frac{-\alpha}{s}\Big)+q_{\min}.
$$</p><p>Dependiendo del valor de $z$, la cuantizaci√≥n puede ser sim√©trica o asim√©trica. En la sim√©trica se fija $z=0$ y se toma $s=\tfrac{\max|x|}{q_{\max}}$, de modo que el rango entero queda centrado alrededor de cero. En cambio, si la distribuci√≥n de los valores est√° sesgada, la cuantizaci√≥n asim√©trica con $z\neq 0$ desplaza el rango y aprovecha mejor los niveles representables.</p><p>En python podr√≠amos implementar algo as√≠:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>quantize_tensor</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>num_bits</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>signed</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>signed</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>qmin</span> <span class=o>=</span> <span class=o>-</span> <span class=p>(</span><span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>qmax</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>qmin</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>qmax</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>**</span> <span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x_min</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_max</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=p>(</span><span class=n>x_max</span> <span class=o>-</span> <span class=n>x_min</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>qmax</span> <span class=o>-</span> <span class=n>qmin</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>scale</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=n>scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>zero_point</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>qmin</span> <span class=o>-</span> <span class=n>x_min</span> <span class=o>/</span> <span class=p>(</span><span class=n>scale</span> <span class=o>+</span> <span class=n>eps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>zero_point</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>zero_point</span><span class=p>,</span> <span class=n>qmin</span><span class=p>,</span> <span class=n>qmax</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>x</span> <span class=o>/</span> <span class=p>(</span><span class=n>scale</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span> <span class=o>+</span> <span class=n>zero_point</span><span class=p>),</span> <span class=n>qmin</span><span class=p>,</span> <span class=n>qmax</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_hat</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=p>(</span><span class=n>q</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span> <span class=o>-</span> <span class=n>zero_point</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>q</span><span class=p>,</span> <span class=n>x_hat</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=n>scale</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>zero_point</span><span class=p>)</span>
</span></span></code></pre></div><p>Para cuantizar par√°metros suele bastar con usar los m√≠nimos y m√°ximos por tensor o por canal, porque en general sus distribuciones est√°n centradas. Con activaciones la situaci√≥n es distinta: una ReLU, por ejemplo, devuelve solo valores no negativos, lo que sesga la distribuci√≥n hacia el lado positivo. En ese caso, una cuantizaci√≥n sim√©trica desperdicia la mitad de los niveles en valores negativos vac√≠os. Por eso se usa cuantizaci√≥n asim√©trica (o directamente unsigned) junto con un conjunto de calibraci√≥n que permita estimar rangos representativos y evitar saturaciones por outliers.</p><p>En PTQ el paso cr√≠tico es calibrar correctamente las activaciones. Una estrategia com√∫n es usar percentiles para que unos pocos valores extremos no definan toda la escala. Otra es aplicar clipping, aceptando un ligero sesgo a cambio de reducir errores de saturaci√≥n.</p><div style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><img src=images/quantization_cropped.png alt="Comparaci√≥n entre QAT y PTQ" style="max-width:100%;height:auto;border:1px solid #ccc;border-radius:8px"><p style=text-align:center;margin-top:.5em;color:#555>Figura 5. Comparaci√≥n entre cuantizaci√≥n sim√©trica y asim√©trica.</p></div><p>PTQ es r√°pido y √∫til cuando la ca√≠da de desempe√±o es chiquita; si no alcanza, se recurre a QAT.</p><h2 id=quantization-aware-training-qat>Quantization Aware Training (QAT)</h2><p>El objetivo de QAT es entrenar un modelo sabiendo que, en inferencia, sus pesos y activaciones van a pasar por un cuantizador. Escribo el problema as√≠:</p><div class=math>$$
\min_{w}\ \mathbb{E}_{(x,y)\sim\mathcal{D}}\Big[L\big(f_{q(w)}(x),\,y\big)\Big].
$$</div><p>$w$ es el conjunto completo de par√°metros reales del modelo antes de cuantizar. $x$ es una entrada del dataset y $y$ es su etiqueta o valor objetivo. $\mathcal{D}$ es la distribuci√≥n de datos que genera los pares $(x,y)$. $\mathbb{E}$ es la esperanza sobre esa distribuci√≥n. $f_{q(w)}$ es el mismo modelo que usar√≠as en FP32 pero con sus pesos pasados por un operador de cuantizaci√≥n $q(\cdot)$. $L(\hat y,y)$ es la funci√≥n de costo que compara la predicci√≥n $\hat y=f_{q(w)}(x)$ con el objetivo $y$. Puede ser entrop√≠a cruzada, error cuadr√°tico medio u otra diferenciable respecto de las salidas del modelo.</p><p>Si definiera $q(w)$ de forma literal con redondeo y recorte, el objetivo ser√≠a no diferenciable. Ese cuantizador uniforme af√≠n es</p><div class=math>$$
q=\operatorname{clip}\!\big(\operatorname{round}(u),\,q_{\min},\,q_{\max}\big),\qquad
u=\frac{w}{s}+z,\qquad
\tilde w=s\,(q-z).
$$</div><p>donde $u=\tfrac{w}{s}+z$ es la versi√≥n escalada y desplazada de $w$ para pasar por el cuantizador, $s>0$ es la escala y $z$ es el zero point que alinea el cero real con un entero. $[q_{\min},q_{\max}]$ es el rango entero permitido por el ancho de bits. $\tilde w$ es la versi√≥n de $w$ de-cuantizada que se usa para computar la salida y la p√©rdida. El forward siempre se hace con $\tilde w$, no con $w$ directo.</p><p>Como $\operatorname{round}$ y $\operatorname{clip}$ no son diferenciables en sentido estricto, QAT usa el estimador <a href=https://hassanaskary.medium.com/intuitive-explanation-of-straight-through-estimators-with-pytorch-implementation-71d99d25d9d0 target=_blank>straight through</a> en el backward. La idea es dejar pasar gradiente como si el redondeo fuera la identidad y hacer que el clip tenga derivada uno dentro del rango √∫til y cero en saturaci√≥n. Con $u=\tfrac{w}{s}+z$ queda</p><div class=math>$$
\frac{\partial \tilde w}{\partial w}\ \approx\
\begin{cases}
1 & \text{si } q_{\min} < u < q_{\max} \\
0 & \text{en caso contrario}
\end{cases}
$$</div><p>En otras palabras: mientras el valor $u$ no se pase de los l√≠mites del cuantizador, el gradiente se transmite normalmente, como si no hubiera cuantizaci√≥n. Pero si $u$ se sale del rango (por ejemplo, porque $w$ es muy grande o muy chico), el gradiente se bloquea y no pasa. As√≠, el modelo aprende a mantener los valores dentro del rango √∫til del cuantizador, y solo se &ldquo;corta&rdquo; el gradiente cuando hay saturaci√≥n.</p><p>Si adem√°s se decide aprender la escala, se trata $s$ como par√°metro y se usa otra vez STE. Reescribiendo $q\approx \operatorname{clip}(u,q_{\min},q_{\max})$ para propagar gradiente, una forma clara del t√©rmino es</p><div class=math>$$
\frac{\partial \tilde w}{\partial s}\ \approx\
\begin{cases}
-\,z\;-\;\frac{w}{s} & \text{si } q_{\min} < u < q_{\max} \\
\operatorname{clip}(u,\,q_{\min},\,q_{\max})\;-\;z\;-\;\frac{w}{s} & \text{en caso contrario}
\end{cases}
$$</div><p>En la pr√°ctica este gradiente se normaliza por la cantidad de elementos que comparten la misma escala, lo que estabiliza la actualizaci√≥n. Ac√° aparece el concepto per-tensor y per-channel.</p><p>Definimos una capa lineal con pesos $W\in\mathbb{R}^{C_o\times C_i}$ que recibe activaciones $X\in\mathbb{R}^{T\times C_i}$, donde $T$ es la cantidad de tokens, $C_i$ los canales de entrada y $C_o$ los de salida.</p><ul><li><p>En per-tensor se usa una sola escala para todo $X$ y una sola escala para todo $W$. Esa elecci√≥n simplifica el c√≥mputo y la normalizaci√≥n del gradiente de $s$ se hace sobre todos los elementos del bloque, pero si los rangos internos difieren mucho se desperdicia resoluci√≥n.</p></li><li><p>En per-channel se asigna una escala distinta a cada canal de salida de $W$, lo que en la pr√°ctica implica un vector $\Delta_W\in\mathbb{R}^{1\times C_o}$ y una normalizaci√≥n por elementos de cada canal. Si adem√°s se aplica per-token en activaciones, se usa $\Delta_X\in\mathbb{R}^{T\times 1}$ y cada fila de $X$ tiene su propia escala. Este esquema alinea la cuantizaci√≥n con la heterogeneidad real de los datos y reduce el error en 8 y sobre todo en 4 bits, a costa de almacenar m√°s escalas.</p></li></ul><p align=center><img src=images/per-channel_per-tensor.png alt="Per-tensor vs Per-channel quantization" style=max-width:100%;height:auto></p><p align=center style=color:#555;margin-top:.5em>\(X\in\mathbb{R}^{T\times C_i}\) son activaciones y \(W\in\mathbb{R}^{C_o\times C_i}\) son pesos. Arriba se ilustra per-tensor, con \(\Delta X[1]\) y \(\Delta W[1]\) como escalas √∫nicas para todo \(X\) y todo \(W\). Abajo se muestra per-token m√°s per-channel, con \(\Delta X[T\times 1]\) como una escala por fila de \(X\) y \(\Delta W[1\times C_o]\) como una escala por canal de salida de \(W\). Las zonas punteadas indican el alcance de cada escala.</p><p>Por √∫ltimo, todo esto se entrena como riesgo emp√≠rico con mini batches y con el mismo esquema de cuantizaci√≥n usado en el forward. En cada paso se calcula la salida con $\tilde w=s,(q-z)$ aplicando las escalas per-tensor o per-channel de los pesos y, si corresponde, per-token en las activaciones; se eval√∫a la p√©rdida; y se hace backward con STE. El problema queda</p><div class=math>$$
\min_{w}\ \frac{1}{B}\sum_{i=1}^{B} L\big(f_{\tilde w}(x_i),\,y_i\big),
$$</div><p>donde $\tilde w$ depende de $w,s,z$ y de la granularidad elegida para las escalas. Cuando hay varias escalas se normaliza su gradiente por el n√∫mero de elementos que las comparten para estabilizar. El optimizador actualiza $w$ y, si est√°n habilitadas, tambi√©n las escalas $s$.</p></article><footer class=footer><div class=footer-content><p>&copy; 2025 Juan Lebrero. All rights reserved.</p><div class=footer-links><a href=/es/about/>About</a>
<span class=separator>‚Ä¢</span>
<a href=/es/contact/>Contact</a>
<span class=separator>‚Ä¢</span>
<a href=/es/index.xml>RSS</a></div></div></footer></div></main></body></html>