<!doctype html><html lang=es dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Entrenamiento a Gran Escala: FSDP, QLoRA, y más. | Juan Lebrero</title><meta name=keywords content="LoRA,QLoRA,LLMs,finetuning,cuantización,4-bit,deepspeed,fsdp,zero,precisión,JAX,bfloat16,fp16,train-at-scale"><meta name=description content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.
Precisión numérica
La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección."><meta name=author content="Juan Francisco Lebrero"><link rel=canonical href=https://juanlebrero.com/es/posts/train-at-scale/><link crossorigin=anonymous href=/assets/css/stylesheet.a1fde4156680e69bfffaba99ce74309162dd968c8d923159061a96f66f6a4b51.css integrity="sha256-of3kFWaA5pv/+rqZznQwkWLdloyNkjFZBhqW9m9qS1E=" rel="preload stylesheet" as=style><link rel=icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://juanlebrero.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://juanlebrero.com/favicon-32x32.png><link rel=apple-touch-icon href=https://juanlebrero.com/apple-touch-icon.png><link rel=mask-icon href=https://juanlebrero.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://juanlebrero.com/posts/train-at-scale/><link rel=alternate hreflang=es href=https://juanlebrero.com/es/posts/train-at-scale/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><meta property="og:url" content="https://juanlebrero.com/es/posts/train-at-scale/"><meta property="og:site_name" content="Juan Lebrero"><meta property="og:title" content="Entrenamiento a Gran Escala: FSDP, QLoRA, y más."><meta property="og:description" content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.
Precisión numérica La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección."><meta property="og:locale" content="es"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-13T00:00:00+00:00"><meta property="article:tag" content="LoRA"><meta property="article:tag" content="QLoRA"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Finetuning"><meta property="article:tag" content="Cuantización"><meta property="article:tag" content="4-Bit"><meta name=twitter:card content="summary"><meta name=twitter:title content="Entrenamiento a Gran Escala: FSDP, QLoRA, y más."><meta name=twitter:description content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.
Precisión numérica
La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://juanlebrero.com/es/posts/"},{"@type":"ListItem","position":2,"name":"Entrenamiento a Gran Escala: FSDP, QLoRA, y más.","item":"https://juanlebrero.com/es/posts/train-at-scale/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Entrenamiento a Gran Escala: FSDP, QLoRA, y más.","name":"Entrenamiento a Gran Escala: FSDP, QLoRA, y más.","description":"Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.\nPrecisión numérica La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección.\n","keywords":["LoRA","QLoRA","LLMs","finetuning","cuantización","4-bit","deepspeed","fsdp","zero","precisión","JAX","bfloat16","fp16","train-at-scale"],"articleBody":"Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.\nPrecisión numérica La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección.\n¿Por qué Importa la Precisión? ¿Como representarías el número $\\pi$, un número con decimales INFINITOS, en algo FINITO como lo es una computadora? De está pregunta, surge como respuesta el punto flotante.\nLos números representados con punto flotante representan, de manera aproximada, a los números reales, y lo hacen con dos componentes clave: la mantisa y el exponente.\nUn número en punto flotante representa aproximadamente un valor real mediante la fórmula:\n$$\\text{valor} \\approx \\text{signo} \\times \\text{mantisa} \\times \\text{base}^{\\text{exponente}}$$\ndonde:\nMantisa: Controla la resolución fina (cuántos pasos discretos entra en el intervalo 1.0 - 2.0) Exponente: Determina el rango dinámico (qué tan grandes o pequeños pueden ser los números representables) Base: En IEEE 754 es 2. Más bits para la mantisa $\\rightarrow$ mayor precisión\nMás bits para el exponente $\\rightarrow$ mayor rango\nFigura 1. Esquema de la representación de un número en punto flotante de 32 bits (FP32) según el estándar IEEE 754. Pero, ¿por qué nos interesa a nosotros?\nBueno, hay tres razones principales por las que la precisión numérica es crucial en el entrenamiento de modelos de gran escala:\nEficiencia computacional: Los formatos de menor ancho de bits aceleran el cómputo en Tensor Cores/TPUs y reducen bastante el uso de memoria.\nEstabilidad numérica: Básicamente, si el formato de número no tiene suficiente rango o detalle, los números pueden volverse demasiado grandes, demasiado chiquitos o perder precisión, lo que puede causar errores o resultados raros durante el entrenamiento.\nEscabilidad: Cuando entrenamos LLMs a gran escala, aprovechar la precisión mixta es crucial para que el costo computacional no se nos vaya a la luna.\nFigura 2. Comparación visual de los formatos de punto flotante más utilizados en deep learning: BF16, FP32 y FP16. FP32 (IEEE 754, precisión simple) Este es el formato “normal” que se usa casi siempre. Guarda los números usando 1 bit para el signo, 8 para el exponente y 23 para la parte decimal (mantisa). Puede representar números muy chicos y muy grandes, desde $1.18\\times10^{-38}$ hasta $3.4\\times10^{38}$, y su precisión es muy alta ($\\varepsilon \\approx 1.19\\times10^{-7}$).\nEn machine learning, FP32 es lo que se considera “precisión completa”. Incluso cuando usamos otros formatos para ahorrar memoria, los cálculos importantes (como acumular los gradientes) se hacen en FP32 para que el entrenamiento no se vuelva inestable.\nFP16 (IEEE 754, half) FP16 es un formato de número que usa menos memoria y permite que todo vaya más rápido. Básicamente, guarda los números usando menos bits que el formato normal (FP32), así que ocupa menos espacio y acelera los cálculos.\nLo bueno: hace que entrenar y usar modelos sea más rápido y barato. Lo malo: como tiene menos detalle y menos rango, a veces los números muy chicos pueden desaparecer (por eso se suele usar loss scaling para evitarlo), y si los números son muy grandes, se pueden “saturar” y perder información.\nBF16 (Brain Floating Point) BF16 es el formato que más se usa hoy para entrenar modelos grandes en TPUs y GPUs modernas (como la H100).\nGuarda los números de una forma parecida a FP32 (el formato “normal”), pero con menos detalle en los decimales. Lo importante es que puede representar números igual de grandes o chicos que FP32, así que no se “rompe” con números extremos. Además, casi siempre funciona bien sin tener que hacer trucos raros como el loss scaling. Aunque no tiene tanta precisión en los decimales como FP16, para entrenar modelos grandes (como los LLMs) suele ser suficiente y no da problemas.\nComparación de precisiones Para comparar las precisiones, voy a usar JAX, un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La razón de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitirá más adelante ver en “crudo” la paralelización de las operaciones y la optimización de la memoria.\nPrimero, importamos JAX y vemos la versión y el backend, así como los dispositivos disponibles:\nimport jax # Configuración de JAX print(f\"JAX version: {jax.__version__}\") print(f\"JAX backend: {jax.default_backend()}\") print(f\"Available devices: {jax.devices()}\") Necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecución. Para esto, vamos a usar psutil, tracemalloc y time.\nimport jax.numpy as jnp import psutil import tracemalloc import time def get_memory_usage(): \"\"\"Obtiene el uso actual de memoria en MB\"\"\" process = psutil.Process() return process.memory_info().rss / 1024 / 1024 def measure_memory_and_time(func): def wrapper(*args, **kwargs): tracemalloc.start() start_memory = get_memory_usage() start_time = time.time() result = func(*args, **kwargs) jax.block_until_ready(result) end_time = time.time() end_memory = get_memory_usage() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() return { 'result': result, 'execution_time': end_time - start_time, 'memory_delta': end_memory - start_memory, 'peak_memory': peak / 1024 / 1024, 'current_memory': current / 1024 / 1024 } return wrapper Ahora, vamos a medir el rendimiento y la memoria de la multiplicación de matrices y la “red neuronal”. Para esto, vamos a usar el decorador measure_memory_and_time que definimos anteriormente.\n@measure_memory_and_time def matrix_multiplication_test(dtype, shape): \"\"\"Prueba de multiplicación de matrices con precisión dada\"\"\" key = jax.random.PRNGKey(42) key1, key2 = jax.random.split(key) def matmul_operation(): a = jax.random.normal(key1, shape, dtype=dtype) b = jax.random.normal(key2, shape, dtype=dtype) return jnp.dot(a, b) return matmul_operation() @measure_memory_and_time def neural_network_forward_pass_test(dtype, input_size, hidden_size, output_size): \"\"\"Prueba de pase forward de red neuronal simple\"\"\" key = jax.random.PRNGKey(123) keys = jax.random.split(key, 3) def nn_forward(): # Inicialización de pesos W1 = jax.random.normal(keys[0], (input_size, hidden_size), dtype=dtype) b1 = jax.random.normal(keys[1], (hidden_size,), dtype=dtype) W2 = jax.random.normal(keys[2], (hidden_size, output_size), dtype=dtype) b2 = jax.random.normal(keys[2], (output_size,), dtype=dtype) # Datos de entrada x = jax.random.normal(keys[0], (input_size,), dtype=dtype) # Pase forward h = jnp.tanh(jnp.dot(x, W1) + b1) y = jnp.dot(h, W2) + b2 return y return nn_forward() Para correr las pruebas, simplemente llamamos a las funciones matrix_multiplication_test y neural_network_forward_pass_test con los tipos de precisión y las dimensiones de las matrices y la red neuronal, por ejemplo:\nmatrix_multiplication_test(jnp.float16, (5000, 5000)) neural_network_forward_pass_test(jnp.float16, 784, 256, 10) Para correr las pruebas, voy a incluir también FP64, para mostrar algo que puede ser contraintuitivo.\nDependiendo en qué hardware las estemos corriendo, los resultados pueden variar bastante. Corriendo las pruebas en un M1, obtenemos los siguientes resultados. Tiempo en segundos y memoria en MB:\nMultiplicación de matrices:\nPrecisión Tiempo Memoria Pico FP16 1.024 0.013 BF16 0.978 0.008 FP32 0.943 0.008 FP64 0.928 0.009 Red neuronal:\nPrecisión Tiempo Memoria Pico FP16 0.007 0.020 BF16 0.004 0.015 FP32 0.003 0.015 FP64 0.002 0.016 A simple vista, uno pensaría que este blog no sirve de nada y que todo lo que dije es una farsa porque “FP64 dio mejor”. Pero en realidad no es así. Estos resultados se explican por varios factores:\nCPUs están optimizadas para ciertos tipos: Los procesadores como el M1 funcionan mejor con FP32 y FP64 porque las librerías que usan (como Accelerate en Mac) están hechas para esos formatos. En cambio, FP16 y BF16 no están tan bien soportados en CPU, así que muchas veces el sistema tiene que convertirlos a FP32 o FP64 antes de hacer las cuentas, y eso las hace más lentas cuando el problema es chico.\nEl tamaño importa: En los ejemplos de las tablas, las matrices y redes son chicas. Cuando los datos son pequeños, la mayor parte del tiempo se va en preparar todo (inicializar, convertir tipos, sincronizar), no en hacer las cuentas en sí. Por eso, a veces FP64 parece “más rápido”, pero es porque el camino para ese tipo es más directo y está mejor optimizado.\nEn GPU es al revés: En las GPUs, FP16 y BF16 son mucho más rápidos porque el hardware tiene partes especiales (como Tensor Cores en NVIDIA) que están hechas para trabajar con estos formatos de baja precisión y pueden hacer muchas operaciones a la vez, usando menos memoria y ancho de banda.\nPara que lo comprueben ustedes mismos, si se ejecutan las pruebas en una GPU usando matrices de dimensiones mucho mayores, se va a ver la diferencia. A continuación muestro una gráfica que la ilustra claramente medido en TFLOPS (Teraflops), calculado como el número de operaciones de punto flotante por segundo.\nFigura 3. Comparación de precisiones. Por útimo, a modo de conclusión de esta sección, les dejo un ejemplo de implementación de precisión mixta, que es lo que suele hacerse en la práctica para entrenar modelos a gran escala.\nLa idea central es simple: las partes del modelo que requieren estabilidad numérica se calculan en FP32, mientras que los resultados intermedios, gradientes y parámetros se almacenan en FP16 o BF16, aprovechando así el ahorro de memoria y el mayor throughput del hardware.\ndef mixed_precision_forward_pass(x, W, b): # Convertir a FP32 para cómputo x_fp32 = x.astype(jnp.float32) W_fp32 = W.astype(jnp.float32) b_fp32 = b.astype(jnp.float32) # Pase forward y = jnp.dot(x_fp32, W_fp32) + b_fp32 # Convertir de vuelta a FP16 para eficiencia de memoria return y.astype(jnp.float16) Cuantización Cuando entrenamos modelos, solemos utilizar FP32 para los pesos y FP16 para los gradientes, pero al llevar el modelo a producción surge un desafío: necesitamos reducir el consumo de memoria y la latencia sin sacrificar la calidad. Acá entra en juego la cuantización. Este proceso consiste en representar los pesos y activaciones con menos bits, lo que permite ahorrar memoria y acelerar la inferencia, a cambio de introducir un pequeño error numérico controlado.\nHay 2 enfoques principales: Post Training Quantization (PTQ) y Quantization Aware Training (QAT).\nFigura 4. Comparación visual entre QAT (Izquierda) y PTQ (Derecha). Post Training Quantization (PTQ) La cuantización post entrenamiento, PTQ, toma un modelo ya entrenado y convierte pesos y activaciones a formatos de menor precisión sin volver a entrenar. La mayoría de las veces es suficiente.\nMatemáticamente, si partimos de un valor real $x\\in\\mathbb{R}$ en FP32, la cuantización clásica mapea $x$ a un entero $q$ de $b$ bits mediante una escala $s$ y un zero point $z$:\n$$ q = \\operatorname{clip}\\Big(\\operatorname{round}\\big(\\tfrac{x}{s}\\big) + z,; q_{\\min}, q_{\\max}\\Big), $$\ny para reconstruir en reales usamos\n$$ \\hat{x} = s\\cdot (q - z). $$\nEl rango entero $[q_{\\min},q_{\\max}]$ depende de si usamos representación signada o no. Para $b$ bits signados, típicamente $q_{\\min}=-2^{b-1}$ y $q_{\\max}=2^{b-1}-1$. La elección de $s$ y $z$ define cuánto error agregamos.\nSi queremos cubrir un intervalo real $[\\alpha,\\beta]$ la escala práctica es:\n$$ s=\\frac{\\beta-\\alpha}{q_{\\max}-q_{\\min}}, \\qquad z=\\operatorname{round}\\Big(\\frac{-\\alpha}{s}\\Big)+q_{\\min}. $$\nDependiendo del valor de $z$, la cuantización puede ser simétrica o asimétrica. En la simétrica se fija $z=0$ y se toma $s=\\tfrac{\\max|x|}{q_{\\max}}$, de modo que el rango entero queda centrado alrededor de cero. En cambio, si la distribución de los valores está sesgada, la cuantización asimétrica con $z\\neq 0$ desplaza el rango y aprovecha mejor los niveles representables.\nEn python podríamos implementar algo así:\nimport jax.numpy as jnp def quantize_tensor(x, num_bits=8, signed=True, eps=1e-8): if signed: qmin = - (2 ** (num_bits - 1)) qmax = 2 ** (num_bits - 1) - 1 else: qmin = 0 qmax = 2 ** num_bits - 1 x_min = jnp.min(x) x_max = jnp.max(x) scale = (x_max - x_min) / (qmax - qmin + eps) scale = jnp.where(scale == 0, 1.0, scale) zero_point = jnp.round(qmin - x_min / (scale + eps)) zero_point = jnp.clip(zero_point, qmin, qmax) q = jnp.clip(jnp.round(x / (scale + eps) + zero_point), qmin, qmax).astype(jnp.int32) x_hat = scale * (q.astype(jnp.float32) - zero_point) return q, x_hat, float(scale), int(zero_point) Para cuantizar parámetros suele bastar con usar los mínimos y máximos por tensor o por canal, porque en general sus distribuciones están centradas. Con activaciones la situación es distinta: una ReLU, por ejemplo, devuelve solo valores no negativos, lo que sesga la distribución hacia el lado positivo. En ese caso, una cuantización simétrica desperdicia la mitad de los niveles en valores negativos vacíos. Por eso se usa cuantización asimétrica (o directamente unsigned) junto con un conjunto de calibración que permita estimar rangos representativos y evitar saturaciones por outliers.\nEn PTQ el paso crítico es calibrar correctamente las activaciones. Una estrategia común es usar percentiles para que unos pocos valores extremos no definan toda la escala. Otra es aplicar clipping, aceptando un ligero sesgo a cambio de reducir errores de saturación.\nFigura 5. Comparación entre cuantización simétrica y asimétrica. PTQ es rápido y útil cuando la caída de desempeño es chiquita; si no alcanza, se recurre a QAT.\nQuantization Aware Training (QAT) El objetivo de QAT es entrenar un modelo sabiendo que, en inferencia, sus pesos y activaciones van a pasar por un cuantizador. Escribo el problema así:\n$$ \\min_{w}\\ \\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\Big[L\\big(f_{q(w)}(x),\\,y\\big)\\Big]. $$ $w$ es el conjunto completo de parámetros reales del modelo antes de cuantizar. $x$ es una entrada del dataset y $y$ es su etiqueta o valor objetivo. $\\mathcal{D}$ es la distribución de datos que genera los pares $(x,y)$. $\\mathbb{E}$ es la esperanza sobre esa distribución. $f_{q(w)}$ es el mismo modelo que usarías en FP32 pero con sus pesos pasados por un operador de cuantización $q(\\cdot)$. $L(\\hat y,y)$ es la función de costo que compara la predicción $\\hat y=f_{q(w)}(x)$ con el objetivo $y$. Puede ser entropía cruzada, error cuadrático medio u otra diferenciable respecto de las salidas del modelo.\nSi definiera $q(w)$ de forma literal con redondeo y recorte, el objetivo sería no diferenciable. Ese cuantizador uniforme afín es\n$$ q=\\operatorname{clip}\\!\\big(\\operatorname{round}(u),\\,q_{\\min},\\,q_{\\max}\\big),\\qquad u=\\frac{w}{s}+z,\\qquad \\tilde w=s\\,(q-z). $$ donde $u=\\tfrac{w}{s}+z$ es la versión escalada y desplazada de $w$ para pasar por el cuantizador, $s\u003e0$ es la escala y $z$ es el zero point que alinea el cero real con un entero. $[q_{\\min},q_{\\max}]$ es el rango entero permitido por el ancho de bits. $\\tilde w$ es la versión de $w$ de-cuantizada que se usa para computar la salida y la pérdida. El forward siempre se hace con $\\tilde w$, no con $w$ directo.\nComo $\\operatorname{round}$ y $\\operatorname{clip}$ no son diferenciables en sentido estricto, QAT usa el estimador straight through en el backward. La idea es dejar pasar gradiente como si el redondeo fuera la identidad y hacer que el clip tenga derivada uno dentro del rango útil y cero en saturación. Con $u=\\tfrac{w}{s}+z$ queda\n$$ \\frac{\\partial \\tilde w}{\\partial w}\\ \\approx\\ \\begin{cases} 1 \u0026 \\text{si } q_{\\min} \u003c u \u003c q_{\\max} \\\\ 0 \u0026 \\text{en caso contrario} \\end{cases} $$ En otras palabras: mientras el valor $u$ no se pase de los límites del cuantizador, el gradiente se transmite normalmente, como si no hubiera cuantización. Pero si $u$ se sale del rango (por ejemplo, porque $w$ es muy grande o muy chico), el gradiente se bloquea y no pasa. Así, el modelo aprende a mantener los valores dentro del rango útil del cuantizador, y solo se “corta” el gradiente cuando hay saturación.\nSi además se decide aprender la escala, se trata $s$ como parámetro y se usa otra vez STE. Reescribiendo $q\\approx \\operatorname{clip}(u,q_{\\min},q_{\\max})$ para propagar gradiente, una forma clara del término es\n$$ \\frac{\\partial \\tilde w}{\\partial s}\\ \\approx\\ \\begin{cases} -\\,z\\;-\\;\\frac{w}{s} \u0026 \\text{si } q_{\\min} \u003c u \u003c q_{\\max} \\\\ \\operatorname{clip}(u,\\,q_{\\min},\\,q_{\\max})\\;-\\;z\\;-\\;\\frac{w}{s} \u0026 \\text{en caso contrario} \\end{cases} $$ En la práctica este gradiente se normaliza por la cantidad de elementos que comparten la misma escala, lo que estabiliza la actualización. Acá aparece el concepto per-tensor y per-channel.\nConsiderá una capa lineal con pesos $W\\in\\mathbb{R}^{C_o\\times C_i}$ que recibe activaciones $X\\in\\mathbb{R}^{T\\times C_i}$, donde $T$ es la cantidad de tokens, $C_i$ los canales de entrada y $C_o$ los de salida.\nEn per-tensor se usa una sola escala para todo $X$ y una sola escala para todo $W$. Esa elección simplifica el cómputo y la normalización del gradiente de $s$ se hace sobre todos los elementos del bloque, pero si los rangos internos difieren mucho se desperdicia resolución.\nEn per-channel se asigna una escala distinta a cada canal de salida de $W$, lo que en la práctica implica un vector $\\Delta_W\\in\\mathbb{R}^{1\\times C_o}$ y una normalización por elementos de cada canal. Si además se aplica per-token en activaciones, se usa $\\Delta_X\\in\\mathbb{R}^{T\\times 1}$ y cada fila de $X$ tiene su propia escala. Este esquema alinea la cuantización con la heterogeneidad real de los datos y reduce el error en 8 y sobre todo en 4 bits, a costa de almacenar más escalas.\n\\(X\\in\\mathbb{R}^{T\\times C_i}\\) son activaciones y \\(W\\in\\mathbb{R}^{C_o\\times C_i}\\) son pesos. Arriba se ilustra per-tensor, con \\(\\Delta X[1]\\) y \\(\\Delta W[1]\\) como escalas únicas para todo \\(X\\) y todo \\(W\\). Abajo se muestra per-token más per-channel, con \\(\\Delta X[T\\times 1]\\) como una escala por fila de \\(X\\) y \\(\\Delta W[1\\times C_o]\\) como una escala por canal de salida de \\(W\\). Las zonas punteadas indican el alcance de cada escala. Por último, todo esto se entrena como riesgo empírico con mini batches y con el mismo esquema de cuantización usado en el forward. En cada paso se calcula la salida con $\\tilde w=s,(q-z)$ aplicando las escalas per-tensor o per-channel de los pesos y, si corresponde, per-token en las activaciones; se evalúa la pérdida; y se hace backward con STE. El problema queda\n$$ \\min_{w}\\ \\frac{1}{B}\\sum_{i=1}^{B} L\\big(f_{\\tilde w}(x_i),\\,y_i\\big), $$ donde $\\tilde w$ depende de $w,s,z$ y de la granularidad elegida para las escalas. Cuando hay varias escalas se normaliza su gradiente por el número de elementos que las comparten para estabilizar. El optimizador actualiza $w$ y, si están habilitadas, también las escalas $s$.\n","wordCount":"2885","inLanguage":"es","datePublished":"2025-09-13T00:00:00Z","dateModified":"2025-09-13T00:00:00Z","author":{"@type":"Person","name":"Juan Francisco Lebrero"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://juanlebrero.com/es/posts/train-at-scale/"},"publisher":{"@type":"Organization","name":"Juan Lebrero","logo":{"@type":"ImageObject","url":"https://juanlebrero.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://juanlebrero.com/es/ accesskey=h title="Juan Lebrero (Alt + H)">Juan Lebrero</a><div class=social-icons align=left><a href=https://x.com/lebrious target=_blank rel="noopener noreferrer me" title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a href=https://www.linkedin.com/in/lebrero-juan-francisco/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li class=sep aria-hidden=true>|</li><li><a href=https://juanlebrero.com/posts/train-at-scale/ title=English aria-label=English>EN</a></li></ul></div></div><ul id=menu><li><a href=https://juanlebrero.com/es/ title=Inicio><span>Inicio</span></a></li><li><a href=https://juanlebrero.com/es/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Entrenamiento a Gran Escala: FSDP, QLoRA, y más.</h1><div class=post-meta><span title='2025-09-13 00:00:00 +0000 UTC'>septiembre 13, 2025</span>&nbsp;·&nbsp;Juan Francisco Lebrero&nbsp;|&nbsp;Traducciones:
<a href=https://juanlebrero.com/posts/train-at-scale/>En</a></div></header><div class=post-content><p>Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.</p><h2 id=precisión-numérica>Precisión numérica<a hidden class=anchor aria-hidden=true href=#precisión-numérica>#</a></h2><p>La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección.</p><h3 id=por-qué-importa-la-precisión>¿Por qué Importa la Precisión?<a hidden class=anchor aria-hidden=true href=#por-qué-importa-la-precisión>#</a></h3><p>¿Como representarías el número $\pi$, un número con decimales INFINITOS, en algo FINITO como lo es una computadora? De está pregunta, surge como respuesta el punto flotante.</p><p>Los números representados con punto flotante representan, de manera aproximada, a los números reales, y lo hacen con dos componentes clave: la <em>mantisa</em> y el <em>exponente</em>.</p><p>Un número en punto flotante representa aproximadamente un valor real mediante la fórmula:</p><p>$$\text{valor} \approx \text{signo} \times \text{mantisa} \times \text{base}^{\text{exponente}}$$</p><p>donde:</p><ul><li><strong>Mantisa</strong>: Controla la resolución fina (cuántos pasos discretos entra en el intervalo 1.0 - 2.0)</li><li><strong>Exponente</strong>: Determina el rango dinámico (qué tan grandes o pequeños pueden ser los números representables)</li><li><strong>Base</strong>: En IEEE 754 es 2.</li></ul><p>Más bits para la mantisa $\rightarrow$ mayor precisión</p><p>Más bits para el exponente $\rightarrow$ mayor rango</p><figure><img src=images/layout.png alt="Representación de Punto Flotante de 32 bits (FP32)" style=width:100%;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 1. Esquema de la representación de un número en punto flotante de 32 bits (FP32) según el estándar IEEE 754.<br></figcaption></figure><p>Pero, ¿por qué nos interesa a nosotros?</p><p>Bueno, hay tres razones principales por las que la precisión numérica es crucial en el entrenamiento de modelos de gran escala:</p><ul><li><p><strong>Eficiencia computacional</strong>: Los formatos de menor ancho de bits aceleran el cómputo en Tensor Cores/TPUs y reducen bastante el uso de memoria.</p></li><li><p><strong>Estabilidad numérica</strong>: Básicamente, si el formato de número no tiene suficiente rango o detalle, los números pueden volverse demasiado grandes, demasiado chiquitos o perder precisión, lo que puede causar errores o resultados raros durante el entrenamiento.</p></li><li><p><strong>Escabilidad</strong>: Cuando entrenamos LLMs a gran escala, aprovechar la precisión mixta es crucial para que el costo computacional no se nos vaya a la luna.</p></li></ul><figure><img src=images/floating_point.png alt="Comparativa de formatos de punto flotante: BF16, FP32 y FP16"><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 2. Comparación visual de los formatos de punto flotante más utilizados en deep learning: <b>BF16</b>, <b>FP32</b> y <b>FP16</b>.<br></figcaption></figure><h3 id=fp32-ieee-754-precisión-simple>FP32 (IEEE 754, precisión simple)<a hidden class=anchor aria-hidden=true href=#fp32-ieee-754-precisión-simple>#</a></h3><p>Este es el formato “normal” que se usa casi siempre. Guarda los números usando 1 bit para el signo, 8 para el exponente y 23 para la parte decimal (mantisa). Puede representar números muy chicos y muy grandes, desde $1.18\times10^{-38}$ hasta $3.4\times10^{38}$, y su precisión es muy alta ($\varepsilon \approx 1.19\times10^{-7}$).</p><p>En machine learning, FP32 es lo que se considera “precisión completa”. Incluso cuando usamos otros formatos para ahorrar memoria, los cálculos importantes (como acumular los gradientes) se hacen en FP32 para que el entrenamiento no se vuelva inestable.</p><h3 id=fp16-ieee-754-half>FP16 (IEEE 754, half)<a hidden class=anchor aria-hidden=true href=#fp16-ieee-754-half>#</a></h3><p>FP16 es un formato de número que usa menos memoria y permite que todo vaya más rápido. Básicamente, guarda los números usando menos bits que el formato normal (FP32), así que ocupa menos espacio y acelera los cálculos.</p><p>Lo bueno: hace que entrenar y usar modelos sea más rápido y barato. Lo malo: como tiene menos detalle y menos rango, a veces los números muy chicos pueden desaparecer (por eso se suele usar <a href=https://picdictionary.com/ml-dictionary/loss-scaling-in-ai-and-deep-learning target=_blank rel=noopener>loss scaling</a> para evitarlo), y si los números son muy grandes, se pueden &ldquo;saturar&rdquo; y perder información.</p><h3 id=bf16-brain-floating-point>BF16 (Brain Floating Point)<a hidden class=anchor aria-hidden=true href=#bf16-brain-floating-point>#</a></h3><p>BF16 es el formato que más se usa hoy para entrenar modelos grandes en TPUs y GPUs modernas (como la H100).</p><p>Guarda los números de una forma parecida a FP32 (el formato “normal”), pero con menos detalle en los decimales. Lo importante es que puede representar números igual de grandes o chicos que FP32, así que no se “rompe” con números extremos. Además, casi siempre funciona bien sin tener que hacer trucos raros como el <em>loss scaling</em>. Aunque no tiene tanta precisión en los decimales como FP16, para entrenar modelos grandes (como los LLMs) suele ser suficiente y no da problemas.</p><h2 id=comparación-de-precisiones>Comparación de precisiones<a hidden class=anchor aria-hidden=true href=#comparación-de-precisiones>#</a></h2><p>Para comparar las precisiones, voy a usar JAX, un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La razón de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitirá más adelante ver en &ldquo;crudo&rdquo; la paralelización de las operaciones y la optimización de la memoria.</p><p>Primero, importamos JAX y vemos la versión y el backend, así como los dispositivos disponibles:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax</span>
</span></span><span class=line><span class=cl><span class=c1># Configuración de JAX</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX version: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX backend: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>default_backend</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Available devices: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>devices</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecución. Para esto, vamos a usar <code>psutil</code>, <code>tracemalloc</code> y <code>time</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>psutil</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tracemalloc</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_memory_usage</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Obtiene el uso actual de memoria en MB&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>process</span> <span class=o>=</span> <span class=n>psutil</span><span class=o>.</span><span class=n>Process</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>process</span><span class=o>.</span><span class=n>memory_info</span><span class=p>()</span><span class=o>.</span><span class=n>rss</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>measure_memory_and_time</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>wrapper</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>start_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>result</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>jax</span><span class=o>.</span><span class=n>block_until_ready</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=n>end_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>current</span><span class=p>,</span> <span class=n>peak</span> <span class=o>=</span> <span class=n>tracemalloc</span><span class=o>.</span><span class=n>get_traced_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>result</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;execution_time&#39;</span><span class=p>:</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;memory_delta&#39;</span><span class=p>:</span> <span class=n>end_memory</span> <span class=o>-</span> <span class=n>start_memory</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;peak_memory&#39;</span><span class=p>:</span> <span class=n>peak</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;current_memory&#39;</span><span class=p>:</span> <span class=n>current</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>       <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>wrapper</span>
</span></span></code></pre></div><p>Ahora, vamos a medir el rendimiento y la memoria de la multiplicación de matrices y la &ldquo;red neuronal&rdquo;. Para esto, vamos a usar el decorador <code>measure_memory_and_time</code> que definimos anteriormente.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>matrix_multiplication_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Prueba de multiplicación de matrices con precisión dada&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>key1</span><span class=p>,</span> <span class=n>key2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>matmul_operation</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=n>a</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key1</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key2</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>matmul_operation</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Prueba de pase forward de red neuronal simple&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>keys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>nn_forward</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=c1># Inicialización de pesos</span>
</span></span><span class=line><span class=cl>       <span class=n>W1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>W2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>output_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=c1># Datos de entrada</span>
</span></span><span class=line><span class=cl>       <span class=n>x</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=c1># Pase forward</span>
</span></span><span class=line><span class=cl>       <span class=n>h</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>nn_forward</span><span class=p>()</span>
</span></span></code></pre></div><p>Para correr las pruebas, simplemente llamamos a las funciones <code>matrix_multiplication_test</code> y <code>neural_network_forward_pass_test</code> con los tipos de precisión y las dimensiones de las matrices y la red neuronal, por ejemplo:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>matrix_multiplication_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=p>(</span><span class=mi>5000</span><span class=p>,</span> <span class=mi>5000</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><p>Para correr las pruebas, voy a incluir también FP64, para mostrar algo que puede ser contraintuitivo.</p><p>Dependiendo en qué hardware las estemos corriendo, los resultados pueden variar bastante.
Corriendo las pruebas en un M1, obtenemos los siguientes resultados. Tiempo en segundos y memoria en MB:</p><div style=display:flex;gap:2px;flex-wrap:wrap><div style=flex:1;min-width:300px><p><strong>Multiplicación de matrices:</strong></p><table><thead><tr><th>Precisión</th><th>Tiempo</th><th>Memoria Pico</th></tr></thead><tbody><tr><td>FP16</td><td>1.024</td><td>0.013</td></tr><tr><td>BF16</td><td>0.978</td><td>0.008</td></tr><tr><td>FP32</td><td>0.943</td><td>0.008</td></tr><tr><td>FP64</td><td><strong>0.928</strong></td><td>0.009</td></tr></tbody></table></div><div style=flex:1;min-width:300px><p><strong>Red neuronal:</strong></p><table><thead><tr><th>Precisión</th><th>Tiempo</th><th>Memoria Pico</th></tr></thead><tbody><tr><td>FP16</td><td>0.007</td><td>0.020</td></tr><tr><td>BF16</td><td>0.004</td><td>0.015</td></tr><tr><td>FP32</td><td>0.003</td><td>0.015</td></tr><tr><td>FP64</td><td><strong>0.002</strong></td><td>0.016</td></tr></tbody></table></div></div><p>A simple vista, uno pensaría que este blog no sirve de nada y que todo lo que dije es una farsa porque &ldquo;FP64 dio mejor&rdquo;. Pero en realidad no es así. Estos resultados se explican por varios factores:</p><ol><li><p><strong>CPUs están optimizadas para ciertos tipos</strong>: Los procesadores como el M1 funcionan mejor con FP32 y FP64 porque las librerías que usan (como Accelerate en Mac) están hechas para esos formatos. En cambio, FP16 y BF16 no están tan bien soportados en CPU, así que muchas veces el sistema tiene que convertirlos a FP32 o FP64 antes de hacer las cuentas, y eso las hace más lentas cuando el problema es chico.</p></li><li><p><strong>El tamaño importa</strong>: En los ejemplos de las tablas, las matrices y redes son chicas. Cuando los datos son pequeños, la mayor parte del tiempo se va en preparar todo (inicializar, convertir tipos, sincronizar), no en hacer las cuentas en sí. Por eso, a veces FP64 parece “más rápido”, pero es porque el camino para ese tipo es más directo y está mejor optimizado.</p></li><li><p><strong>En GPU es al revés</strong>: En las GPUs, FP16 y BF16 son mucho más rápidos porque el hardware tiene partes especiales (como Tensor Cores en NVIDIA) que están hechas para trabajar con estos formatos de baja precisión y pueden hacer muchas operaciones a la vez, usando menos memoria y ancho de banda.</p></li></ol><p>Para que lo comprueben ustedes mismos, si se ejecutan las pruebas en una GPU usando matrices de dimensiones mucho mayores, se va a ver la diferencia. A continuación muestro una gráfica que la ilustra claramente medido en TFLOPS (Teraflops), calculado como el número de operaciones de punto flotante por segundo.</p><div style=display:flex;justify-content:center><figure><img src=images/nvidia-a100-matmul-tflops.png alt="Comparación de precisiones" style=max-width:350px;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 3. Comparación de precisiones.<br></figcaption></figure></div><p>Por útimo, a modo de conclusión de esta sección, les dejo un ejemplo de implementación de precisión mixta, que es lo que suele hacerse en la práctica para entrenar modelos a gran escala.</p><p>La idea central es simple: las partes del modelo que requieren estabilidad numérica se calculan en FP32, mientras que los resultados intermedios, gradientes y parámetros se almacenan en FP16 o BF16, aprovechando así el ahorro de memoria y el mayor throughput del hardware.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>mixed_precision_forward_pass</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=c1># Convertir a FP32 para cómputo</span>
</span></span><span class=line><span class=cl>   <span class=n>x_fp32</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>W_fp32</span> <span class=o>=</span> <span class=n>W</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>b_fp32</span> <span class=o>=</span> <span class=n>b</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Pase forward</span>
</span></span><span class=line><span class=cl>   <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_fp32</span><span class=p>,</span> <span class=n>W_fp32</span><span class=p>)</span> <span class=o>+</span> <span class=n>b_fp32</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Convertir de vuelta a FP16 para eficiencia de memoria</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>y</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=cuantización>Cuantización<a hidden class=anchor aria-hidden=true href=#cuantización>#</a></h1><p>Cuando entrenamos modelos, solemos utilizar FP32 para los pesos y FP16 para los gradientes, pero al llevar el modelo a producción surge un desafío: necesitamos reducir el consumo de memoria y la latencia sin sacrificar la calidad. Acá entra en juego la cuantización. Este proceso consiste en representar los pesos y activaciones con menos bits, lo que permite ahorrar memoria y acelerar la inferencia, a cambio de introducir un pequeño error numérico controlado.</p><p>Hay 2 enfoques principales: Post Training Quantization (PTQ) y Quantization Aware Training (QAT).</p><div style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><img src=images/Quantization-Aware-TrainingQAT-and-Post-Training-Quantization-PTQ.png alt="Comparación entre QAT y PTQ" style="max-width:100%;height:auto;border:1px solid #ccc;border-radius:8px"><p style=text-align:center;margin-top:.5em;color:#555>Figura 4. Comparación visual entre QAT (Izquierda) y PTQ (Derecha).</p></div><h2 id=post-training-quantization-ptq>Post Training Quantization (PTQ)<a hidden class=anchor aria-hidden=true href=#post-training-quantization-ptq>#</a></h2><p>La cuantización post entrenamiento, <strong>PTQ</strong>, toma un modelo ya entrenado y convierte pesos y activaciones a formatos de menor precisión sin volver a entrenar. La mayoría de las veces es suficiente.</p><p>Matemáticamente, si partimos de un valor real $x\in\mathbb{R}$ en FP32, la cuantización clásica mapea $x$ a un entero $q$ de $b$ bits mediante una escala $s$ y un zero point $z$:</p><p>$$
q = \operatorname{clip}\Big(\operatorname{round}\big(\tfrac{x}{s}\big) + z,; q_{\min}, q_{\max}\Big),
$$</p><p>y para reconstruir en reales usamos</p><p>$$
\hat{x} = s\cdot (q - z).
$$</p><p>El rango entero $[q_{\min},q_{\max}]$ depende de si usamos representación signada o no. Para $b$ bits signados, típicamente $q_{\min}=-2^{b-1}$ y $q_{\max}=2^{b-1}-1$. La elección de $s$ y $z$ define cuánto error agregamos.</p><p>Si queremos cubrir un intervalo real $[\alpha,\beta]$ la escala práctica es:</p><p>$$
s=\frac{\beta-\alpha}{q_{\max}-q_{\min}},
\qquad
z=\operatorname{round}\Big(\frac{-\alpha}{s}\Big)+q_{\min}.
$$</p><p>Dependiendo del valor de $z$, la cuantización puede ser simétrica o asimétrica. En la simétrica se fija $z=0$ y se toma $s=\tfrac{\max|x|}{q_{\max}}$, de modo que el rango entero queda centrado alrededor de cero. En cambio, si la distribución de los valores está sesgada, la cuantización asimétrica con $z\neq 0$ desplaza el rango y aprovecha mejor los niveles representables.</p><p>En python podríamos implementar algo así:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>quantize_tensor</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>num_bits</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>signed</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-8</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>signed</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>qmin</span> <span class=o>=</span> <span class=o>-</span> <span class=p>(</span><span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>qmax</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>**</span> <span class=p>(</span><span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>qmin</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>qmax</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>**</span> <span class=n>num_bits</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x_min</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_max</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=p>(</span><span class=n>x_max</span> <span class=o>-</span> <span class=n>x_min</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>qmax</span> <span class=o>-</span> <span class=n>qmin</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>scale</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>,</span> <span class=n>scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>zero_point</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>qmin</span> <span class=o>-</span> <span class=n>x_min</span> <span class=o>/</span> <span class=p>(</span><span class=n>scale</span> <span class=o>+</span> <span class=n>eps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>zero_point</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>zero_point</span><span class=p>,</span> <span class=n>qmin</span><span class=p>,</span> <span class=n>qmax</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>x</span> <span class=o>/</span> <span class=p>(</span><span class=n>scale</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span> <span class=o>+</span> <span class=n>zero_point</span><span class=p>),</span> <span class=n>qmin</span><span class=p>,</span> <span class=n>qmax</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_hat</span> <span class=o>=</span> <span class=n>scale</span> <span class=o>*</span> <span class=p>(</span><span class=n>q</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span> <span class=o>-</span> <span class=n>zero_point</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>q</span><span class=p>,</span> <span class=n>x_hat</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=n>scale</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>zero_point</span><span class=p>)</span>
</span></span></code></pre></div><p>Para cuantizar parámetros suele bastar con usar los mínimos y máximos por tensor o por canal, porque en general sus distribuciones están centradas. Con activaciones la situación es distinta: una ReLU, por ejemplo, devuelve solo valores no negativos, lo que sesga la distribución hacia el lado positivo. En ese caso, una cuantización simétrica desperdicia la mitad de los niveles en valores negativos vacíos. Por eso se usa cuantización asimétrica (o directamente unsigned) junto con un conjunto de calibración que permita estimar rangos representativos y evitar saturaciones por outliers.</p><p>En PTQ el paso crítico es calibrar correctamente las activaciones. Una estrategia común es usar percentiles para que unos pocos valores extremos no definan toda la escala. Otra es aplicar clipping, aceptando un ligero sesgo a cambio de reducir errores de saturación.</p><div style="display:flex;flex-direction:column;align-items:center;margin:2em 0"><img src=images/quantization_cropped.png alt="Comparación entre QAT y PTQ" style="max-width:100%;height:auto;border:1px solid #ccc;border-radius:8px"><p style=text-align:center;margin-top:.5em;color:#555>Figura 5. Comparación entre cuantización simétrica y asimétrica.</p></div><p>PTQ es rápido y útil cuando la caída de desempeño es chiquita; si no alcanza, se recurre a QAT.</p><h2 id=quantization-aware-training-qat>Quantization Aware Training (QAT)<a hidden class=anchor aria-hidden=true href=#quantization-aware-training-qat>#</a></h2><p>El objetivo de QAT es entrenar un modelo sabiendo que, en inferencia, sus pesos y activaciones van a pasar por un cuantizador. Escribo el problema así:</p><div class=math>$$
\min_{w}\ \mathbb{E}_{(x,y)\sim\mathcal{D}}\Big[L\big(f_{q(w)}(x),\,y\big)\Big].
$$</div><p>$w$ es el conjunto completo de parámetros reales del modelo antes de cuantizar. $x$ es una entrada del dataset y $y$ es su etiqueta o valor objetivo. $\mathcal{D}$ es la distribución de datos que genera los pares $(x,y)$. $\mathbb{E}$ es la esperanza sobre esa distribución. $f_{q(w)}$ es el mismo modelo que usarías en FP32 pero con sus pesos pasados por un operador de cuantización $q(\cdot)$. $L(\hat y,y)$ es la función de costo que compara la predicción $\hat y=f_{q(w)}(x)$ con el objetivo $y$. Puede ser entropía cruzada, error cuadrático medio u otra diferenciable respecto de las salidas del modelo.</p><p>Si definiera $q(w)$ de forma literal con redondeo y recorte, el objetivo sería no diferenciable. Ese cuantizador uniforme afín es</p><div class=math>$$
q=\operatorname{clip}\!\big(\operatorname{round}(u),\,q_{\min},\,q_{\max}\big),\qquad
u=\frac{w}{s}+z,\qquad
\tilde w=s\,(q-z).
$$</div><p>donde $u=\tfrac{w}{s}+z$ es la versión escalada y desplazada de $w$ para pasar por el cuantizador, $s>0$ es la escala y $z$ es el zero point que alinea el cero real con un entero. $[q_{\min},q_{\max}]$ es el rango entero permitido por el ancho de bits. $\tilde w$ es la versión de $w$ de-cuantizada que se usa para computar la salida y la pérdida. El forward siempre se hace con $\tilde w$, no con $w$ directo.</p><p>Como $\operatorname{round}$ y $\operatorname{clip}$ no son diferenciables en sentido estricto, QAT usa el estimador <a href=https://hassanaskary.medium.com/intuitive-explanation-of-straight-through-estimators-with-pytorch-implementation-71d99d25d9d0 target=_blank>straight through</a> en el backward. La idea es dejar pasar gradiente como si el redondeo fuera la identidad y hacer que el clip tenga derivada uno dentro del rango útil y cero en saturación. Con $u=\tfrac{w}{s}+z$ queda</p><div class=math>$$
\frac{\partial \tilde w}{\partial w}\ \approx\
\begin{cases}
1 & \text{si } q_{\min} < u < q_{\max} \\
0 & \text{en caso contrario}
\end{cases}
$$</div><p>En otras palabras: mientras el valor $u$ no se pase de los límites del cuantizador, el gradiente se transmite normalmente, como si no hubiera cuantización. Pero si $u$ se sale del rango (por ejemplo, porque $w$ es muy grande o muy chico), el gradiente se bloquea y no pasa. Así, el modelo aprende a mantener los valores dentro del rango útil del cuantizador, y solo se &ldquo;corta&rdquo; el gradiente cuando hay saturación.</p><p>Si además se decide aprender la escala, se trata $s$ como parámetro y se usa otra vez STE. Reescribiendo $q\approx \operatorname{clip}(u,q_{\min},q_{\max})$ para propagar gradiente, una forma clara del término es</p><div class=math>$$
\frac{\partial \tilde w}{\partial s}\ \approx\
\begin{cases}
-\,z\;-\;\frac{w}{s} & \text{si } q_{\min} < u < q_{\max} \\
\operatorname{clip}(u,\,q_{\min},\,q_{\max})\;-\;z\;-\;\frac{w}{s} & \text{en caso contrario}
\end{cases}
$$</div><p>En la práctica este gradiente se normaliza por la cantidad de elementos que comparten la misma escala, lo que estabiliza la actualización. Acá aparece el concepto per-tensor y per-channel.</p><p>Considerá una capa lineal con pesos $W\in\mathbb{R}^{C_o\times C_i}$ que recibe activaciones $X\in\mathbb{R}^{T\times C_i}$, donde $T$ es la cantidad de tokens, $C_i$ los canales de entrada y $C_o$ los de salida.</p><ul><li><p>En per-tensor se usa una sola escala para todo $X$ y una sola escala para todo $W$. Esa elección simplifica el cómputo y la normalización del gradiente de $s$ se hace sobre todos los elementos del bloque, pero si los rangos internos difieren mucho se desperdicia resolución.</p></li><li><p>En per-channel se asigna una escala distinta a cada canal de salida de $W$, lo que en la práctica implica un vector $\Delta_W\in\mathbb{R}^{1\times C_o}$ y una normalización por elementos de cada canal. Si además se aplica per-token en activaciones, se usa $\Delta_X\in\mathbb{R}^{T\times 1}$ y cada fila de $X$ tiene su propia escala. Este esquema alinea la cuantización con la heterogeneidad real de los datos y reduce el error en 8 y sobre todo en 4 bits, a costa de almacenar más escalas.</p></li></ul><p align=center><img src=images/per-channel_per-tensor.png alt="Per-tensor vs Per-channel quantization" style=max-width:100%;height:auto></p><p align=center style=color:#555;margin-top:.5em>\(X\in\mathbb{R}^{T\times C_i}\) son activaciones y \(W\in\mathbb{R}^{C_o\times C_i}\) son pesos. Arriba se ilustra per-tensor, con \(\Delta X[1]\) y \(\Delta W[1]\) como escalas únicas para todo \(X\) y todo \(W\). Abajo se muestra per-token más per-channel, con \(\Delta X[T\times 1]\) como una escala por fila de \(X\) y \(\Delta W[1\times C_o]\) como una escala por canal de salida de \(W\). Las zonas punteadas indican el alcance de cada escala.</p><p>Por último, todo esto se entrena como riesgo empírico con mini batches y con el mismo esquema de cuantización usado en el forward. En cada paso se calcula la salida con $\tilde w=s,(q-z)$ aplicando las escalas per-tensor o per-channel de los pesos y, si corresponde, per-token en las activaciones; se evalúa la pérdida; y se hace backward con STE. El problema queda</p><div class=math>$$
\min_{w}\ \frac{1}{B}\sum_{i=1}^{B} L\big(f_{\tilde w}(x_i),\,y_i\big),
$$</div><p>donde $\tilde w$ depende de $w,s,z$ y de la granularidad elegida para las escalas. Cuando hay varias escalas se normaliza su gradiente por el número de elementos que las comparten para estabilizar. El optimizador actualiza $w$ y, si están habilitadas, también las escalas $s$.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://juanlebrero.com/es/tags/lora/>LoRA</a></li><li><a href=https://juanlebrero.com/es/tags/qlora/>QLoRA</a></li><li><a href=https://juanlebrero.com/es/tags/llms/>LLMs</a></li><li><a href=https://juanlebrero.com/es/tags/finetuning/>Finetuning</a></li><li><a href=https://juanlebrero.com/es/tags/cuantizaci%C3%B3n/>Cuantización</a></li><li><a href=https://juanlebrero.com/es/tags/4-bit/>4-Bit</a></li><li><a href=https://juanlebrero.com/es/tags/deepspeed/>Deepspeed</a></li><li><a href=https://juanlebrero.com/es/tags/fsdp/>Fsdp</a></li><li><a href=https://juanlebrero.com/es/tags/zero/>Zero</a></li><li><a href=https://juanlebrero.com/es/tags/precisi%C3%B3n/>Precisión</a></li><li><a href=https://juanlebrero.com/es/tags/jax/>JAX</a></li><li><a href=https://juanlebrero.com/es/tags/bfloat16/>Bfloat16</a></li><li><a href=https://juanlebrero.com/es/tags/fp16/>Fp16</a></li><li><a href=https://juanlebrero.com/es/tags/train-at-scale/>Train-at-Scale</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://juanlebrero.com/es/>Juan Lebrero</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>