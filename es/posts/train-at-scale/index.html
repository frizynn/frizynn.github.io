<!doctype html><html lang=es dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Entrenamiento a Gran Escala: FSDP, QLoRA, y más. | Juan Lebrero</title><meta name=keywords content="LoRA,QLoRA,LLMs,finetuning,cuantización,4-bit,deepspeed,fsdp,zero,precisión,JAX,bfloat16,fp16"><meta name=description content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.
Precisión numérica
La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección."><meta name=author content="Juan Francisco Lebrero"><link rel=canonical href=https://juanlebrero.com/es/posts/train-at-scale/><link crossorigin=anonymous href=/assets/css/stylesheet.baccaaa2085a898c7eec3389fdd189261eed2042be4931a1bc8dffaae7ea290a.css integrity="sha256-usyqoghaiYx+7DOJ/dGJJh7tIEK+STGhvI3/qufqKQo=" rel="preload stylesheet" as=style><link rel=icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://juanlebrero.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://juanlebrero.com/favicon-32x32.png><link rel=apple-touch-icon href=https://juanlebrero.com/apple-touch-icon.png><link rel=mask-icon href=https://juanlebrero.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://juanlebrero.com/posts/train-at-scale/><link rel=alternate hreflang=es href=https://juanlebrero.com/es/posts/train-at-scale/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){typeof renderMathInElement=="function"&&renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,ignoredTags:["script","noscript","style","textarea","pre","code"]})})</script><meta property="og:url" content="https://juanlebrero.com/es/posts/train-at-scale/"><meta property="og:site_name" content="Juan Lebrero"><meta property="og:title" content="Entrenamiento a Gran Escala: FSDP, QLoRA, y más."><meta property="og:description" content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.
Precisión numérica La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección."><meta property="og:locale" content="es"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-13T00:00:00+00:00"><meta property="article:tag" content="LoRA"><meta property="article:tag" content="QLoRA"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Finetuning"><meta property="article:tag" content="Cuantización"><meta property="article:tag" content="4-Bit"><meta name=twitter:card content="summary"><meta name=twitter:title content="Entrenamiento a Gran Escala: FSDP, QLoRA, y más."><meta name=twitter:description content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.
Precisión numérica
La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://juanlebrero.com/es/posts/"},{"@type":"ListItem","position":2,"name":"Entrenamiento a Gran Escala: FSDP, QLoRA, y más.","item":"https://juanlebrero.com/es/posts/train-at-scale/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Entrenamiento a Gran Escala: FSDP, QLoRA, y más.","name":"Entrenamiento a Gran Escala: FSDP, QLoRA, y más.","description":"Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.\nPrecisión numérica La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección.\n","keywords":["LoRA","QLoRA","LLMs","finetuning","cuantización","4-bit","deepspeed","fsdp","zero","precisión","JAX","bfloat16","fp16"],"articleBody":"Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.\nPrecisión numérica La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección.\n¿Por qué Importa la Precisión? ¿Como representarías el número $\\pi$, un número con decimales INFINITOS, en algo FINITO como lo es una computadora? De está pregunta, surge como respuesta el punto flotante.\nLos números representados con punto flotante representan, de manera aproximada, a los números reales, y lo hacen con dos componentes clave: la mantisa y el exponente.\nUn número en punto flotante representa aproximadamente un valor real mediante la fórmula:\n$$\\text{valor} \\approx \\text{signo} \\times \\text{mantisa} \\times \\text{base}^{\\text{exponente}}$$\ndonde:\nMantisa: Controla la resolución fina (cuántos pasos discretos entra en el intervalo 1.0 - 2.0) Exponente: Determina el rango dinámico (qué tan grandes o pequeños pueden ser los números representables) Base: En IEEE 754 es 2. Más bits para la mantisa $\\rightarrow$ mayor precisión\nMás bits para el exponente $\\rightarrow$ mayor rango\nFigura 1. Esquema de la representación de un número en punto flotante de 32 bits (FP32) según el estándar IEEE 754. Pero, ¿por qué nos interesa a nosotros?\nBueno, hay tres razones principales por las que la precisión numérica es crucial en el entrenamiento de modelos de gran escala:\nEficiencia computacional: Los formatos de menor ancho de bits aceleran el cómputo en Tensor Cores/TPUs y reducen bastante el uso de memoria.\nEstabilidad numérica: Básicamente, si el formato de número no tiene suficiente rango o detalle, los números pueden volverse demasiado grandes, demasiado chiquitos o perder precisión, lo que puede causar errores o resultados raros durante el entrenamiento.\nEscabilidad: Cuando entrenamos LLMs a gran escala, aprovechar la precisión mixta es crucial para que el costo computacional no se nos vaya a la luna.\nFigura 2. Comparación visual de los formatos de punto flotante más utilizados en deep learning: BF16, FP32 y FP16. FP32 (IEEE 754, precisión simple) Este es el formato “normal” que se usa casi siempre. Guarda los números usando 1 bit para el signo, 8 para el exponente y 23 para la parte decimal (mantisa). Puede representar números muy chicos y muy grandes, desde $1.18\\times10^{-38}$ hasta $3.4\\times10^{38}$, y su precisión es muy alta ($\\varepsilon \\approx 1.19\\times10^{-7}$).\nEn machine learning, FP32 es lo que se considera “precisión completa”. Incluso cuando usamos otros formatos para ahorrar memoria, los cálculos importantes (como acumular los gradientes) se hacen en FP32 para que el entrenamiento no se vuelva inestable.\nFP16 (IEEE 754, half) FP16 es un formato de número que usa menos memoria y permite que todo vaya más rápido. Básicamente, guarda los números usando menos bits que el formato normal (FP32), así que ocupa menos espacio y acelera los cálculos.\nLo bueno: hace que entrenar y usar modelos sea más rápido y barato. Lo malo: como tiene menos detalle y menos rango, a veces los números muy chicos pueden desaparecer (por eso se suele usar loss scaling para evitarlo), y si los números son muy grandes, se pueden “saturar” y perder información.\nBF16 (Brain Floating Point) BF16 es el formato que más se usa hoy para entrenar modelos grandes en TPUs y GPUs modernas (como la H100).\nGuarda los números de una forma parecida a FP32 (el formato “normal”), pero con menos detalle en los decimales. Lo importante es que puede representar números igual de grandes o chicos que FP32, así que no se “rompe” con números extremos. Además, casi siempre funciona bien sin tener que hacer trucos raros como el loss scaling. Aunque no tiene tanta precisión en los decimales como FP16, para entrenar modelos grandes (como los LLMs) suele ser suficiente y no da problemas.\nComparación de precisiones Para comparar las precisiones, voy a usar JAX, un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La razón de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitirá más adelante ver en “crudo” la paralelización de las operaciones y la optimización de la memoria.\nPrimero, importamos JAX y vemos la versión y el backend, así como los dispositivos disponibles:\nimport jax # Configuración de JAX print(f\"JAX version: {jax.__version__}\") print(f\"JAX backend: {jax.default_backend()}\") print(f\"Available devices: {jax.devices()}\") Para medir el rendimiento y la memoria, necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecución. Para esto, vamos a usar psutil, tracemalloc y time.\nimport jax.numpy as jnp import psutil import tracemalloc import time def get_memory_usage(): \"\"\"Obtiene el uso actual de memoria en MB\"\"\" process = psutil.Process() return process.memory_info().rss / 1024 / 1024 def measure_memory_and_time(func): def wrapper(*args, **kwargs): tracemalloc.start() start_memory = get_memory_usage() start_time = time.time() result = func(*args, **kwargs) jax.block_until_ready(result) end_time = time.time() end_memory = get_memory_usage() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() return { 'result': result, 'execution_time': end_time - start_time, 'memory_delta': end_memory - start_memory, 'peak_memory': peak / 1024 / 1024, 'current_memory': current / 1024 / 1024 } return wrapper Ahora, vamos a medir el rendimiento y la memoria de la multiplicación de matrices y la “red neuronal”. Para esto, vamos a usar el decorador measure_memory_and_time que definimos anteriormente.\n@measure_memory_and_time def matrix_multiplication_test(dtype, shape): \"\"\"Prueba de multiplicación de matrices con precisión dada\"\"\" key = jax.random.PRNGKey(42) key1, key2 = jax.random.split(key) def matmul_operation(): a = jax.random.normal(key1, shape, dtype=dtype) b = jax.random.normal(key2, shape, dtype=dtype) return jnp.dot(a, b) return matmul_operation() @measure_memory_and_time def neural_network_forward_pass_test(dtype, input_size, hidden_size, output_size): \"\"\"Prueba de pase forward de red neuronal simple\"\"\" key = jax.random.PRNGKey(123) keys = jax.random.split(key, 3) def nn_forward(): # Inicialización de pesos W1 = jax.random.normal(keys[0], (input_size, hidden_size), dtype=dtype) b1 = jax.random.normal(keys[1], (hidden_size,), dtype=dtype) W2 = jax.random.normal(keys[2], (hidden_size, output_size), dtype=dtype) b2 = jax.random.normal(keys[2], (output_size,), dtype=dtype) # Datos de entrada x = jax.random.normal(keys[0], (input_size,), dtype=dtype) # Pase forward h = jnp.tanh(jnp.dot(x, W1) + b1) y = jnp.dot(h, W2) + b2 return y return nn_forward() Para correr las pruebas, simplemente llamamos a las funciones matrix_multiplication_test y neural_network_forward_pass_test con los tipos de precisión y las dimensiones de las matrices y la red neuronal, por ejemplo:\nmatrix_multiplication_test(jnp.float16, (5000, 5000)) neural_network_forward_pass_test(jnp.float16, 784, 256, 10) Para correr las pruebas, voy a incluir también FP64, para mostrar algo que puede ser contraintuitivo.\nDependiendo en qué hardware las estemos corriendo, los resultados pueden variar bastante. Corriendo las pruebas en un M1, obtenemos los siguientes resultados para la multiplicación de matrices:\nPrecisión Tiempo (s) Memoria Pico (MB) FP16 1.024 0.013 BF16 0.978 0.008 FP32 0.943 0.008 FP64 0.928 0.009 Y para la red neuronal:\nPrecisión Tiempo (s) Memoria Pico (MB) FP16 0.007 0.020 BF16 0.004 0.015 FP32 0.003 0.015 FP64 0.002 0.016 A simple vista, uno pensaría: “entonces FP64 es lo mejor”. Pero en realidad no es así. Estos resultados se explican por varios factores:\nCPUs están optimizadas para ciertos tipos: Los procesadores como el M1 funcionan mejor con FP32 y FP64 porque las librerías que usan (como Accelerate en Mac) están hechas para esos formatos. En cambio, FP16 y BF16 no están tan bien soportados en CPU, así que muchas veces el sistema tiene que convertirlos a FP32 o FP64 antes de hacer las cuentas, y eso las hace más lentas cuando el problema es chico.\nEl tamaño importa: En los ejemplos de las tablas, las matrices y redes son chicas. Cuando los datos son pequeños, la mayor parte del tiempo se va en preparar todo (inicializar, convertir tipos, sincronizar), no en hacer las cuentas en sí. Por eso, a veces FP64 parece “más rápido”, pero es porque el camino para ese tipo es más directo y está mejor optimizado.\nEn GPU es al revés: En las GPUs, FP16 y BF16 son mucho más rápidos porque el hardware tiene partes especiales (como Tensor Cores en NVIDIA) que están hechas para trabajar con estos formatos de baja precisión y pueden hacer muchas operaciones a la vez, usando menos memoria y ancho de banda.\nPara que lo comprueben ustedes mismos, les propongo que ejecuten las pruebas en una GPU usando matrices de dimensiones mucho mayores; así podrán observar la diferencia por su cuenta. A continuación les muestro una gráfica que ilustra claramente esa gran diferencia.\nFigura 3. Comparación de precisiones. Por útimo, a modo de conclusión de esta sección, les dejo un ejemplo de implementación de precisión mixta, que es lo que suele hacerse en la práctica para entrenar modelos a gran escala.\nLa idea central es simple: las partes del modelo que requieren estabilidad numérica se calculan en FP32, mientras que los resultados intermedios, gradientes y parámetros se almacenan en FP16 o BF16, aprovechando así el ahorro de memoria y el mayor throughput del hardware.\n# Ejemplo de entrenamiento con precisión mixta def mixed_precision_forward_pass(x, W, b): # Convertir a FP32 para cómputo x_fp32 = x.astype(jnp.float32) W_fp32 = W.astype(jnp.float32) b_fp32 = b.astype(jnp.float32) # Pase forward y = jnp.dot(x_fp32, W_fp32) + b_fp32 # Convertir de vuelta a FP16 para eficiencia de memoria return y.astype(jnp.float16) Quantización ","wordCount":"1525","inLanguage":"es","datePublished":"2025-09-13T00:00:00Z","dateModified":"2025-09-13T00:00:00Z","author":{"@type":"Person","name":"Juan Francisco Lebrero"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://juanlebrero.com/es/posts/train-at-scale/"},"publisher":{"@type":"Organization","name":"Juan Lebrero","logo":{"@type":"ImageObject","url":"https://juanlebrero.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://juanlebrero.com/es/ accesskey=h title="Juan Lebrero (Alt + H)">Juan Lebrero</a><div class=social-icons align=left><a href=https://x.com/lebrious target=_blank rel="noopener noreferrer me" title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a href=https://www.linkedin.com/in/lebrero-juan-francisco/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li class=sep aria-hidden=true>|</li><li><a href=https://juanlebrero.com/posts/train-at-scale/ title=English aria-label=English>EN</a></li></ul></div></div><ul id=menu><li><a href=https://juanlebrero.com/es/ title=Inicio><span>Inicio</span></a></li><li><a href=https://juanlebrero.com/es/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Entrenamiento a Gran Escala: FSDP, QLoRA, y más.</h1><div class=post-meta><span title='2025-09-13 00:00:00 +0000 UTC'>septiembre 13, 2025</span>&nbsp;·&nbsp;Juan Francisco Lebrero&nbsp;|&nbsp;Traducciones:<ul class=i18n_list><li><a href=https://juanlebrero.com/posts/train-at-scale/>En</a></li></ul></div></header><div class=post-content><p>Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisión numérica, paralelización de datos, cuantización, LoRA, y más.</p><h2 id=precisión-numérica>Precisión numérica<a hidden class=anchor aria-hidden=true href=#precisión-numérica>#</a></h2><p>La elección del formato numérico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores más determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisión numérica y como afecta al rendimiento de los modelos, lo que se explicará en esta sección.</p><h3 id=por-qué-importa-la-precisión>¿Por qué Importa la Precisión?<a hidden class=anchor aria-hidden=true href=#por-qué-importa-la-precisión>#</a></h3><p>¿Como representarías el número $\pi$, un número con decimales INFINITOS, en algo FINITO como lo es una computadora? De está pregunta, surge como respuesta el punto flotante.</p><p>Los números representados con punto flotante representan, de manera aproximada, a los números reales, y lo hacen con dos componentes clave: la <em>mantisa</em> y el <em>exponente</em>.</p><p>Un número en punto flotante representa aproximadamente un valor real mediante la fórmula:</p><p>$$\text{valor} \approx \text{signo} \times \text{mantisa} \times \text{base}^{\text{exponente}}$$</p><p>donde:</p><ul><li><strong>Mantisa</strong>: Controla la resolución fina (cuántos pasos discretos entra en el intervalo 1.0 - 2.0)</li><li><strong>Exponente</strong>: Determina el rango dinámico (qué tan grandes o pequeños pueden ser los números representables)</li><li><strong>Base</strong>: En IEEE 754 es 2.</li></ul><p>Más bits para la mantisa $\rightarrow$ mayor precisión</p><p>Más bits para el exponente $\rightarrow$ mayor rango</p><figure><img src=images/layout.png alt="Representación de Punto Flotante de 32 bits (FP32)" style=width:100%;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 1. Esquema de la representación de un número en punto flotante de 32 bits (FP32) según el estándar IEEE 754.<br></figcaption></figure><p>Pero, ¿por qué nos interesa a nosotros?</p><p>Bueno, hay tres razones principales por las que la precisión numérica es crucial en el entrenamiento de modelos de gran escala:</p><ul><li><p><strong>Eficiencia computacional</strong>: Los formatos de menor ancho de bits aceleran el cómputo en Tensor Cores/TPUs y reducen bastante el uso de memoria.</p></li><li><p><strong>Estabilidad numérica</strong>: Básicamente, si el formato de número no tiene suficiente rango o detalle, los números pueden volverse demasiado grandes, demasiado chiquitos o perder precisión, lo que puede causar errores o resultados raros durante el entrenamiento.</p></li><li><p><strong>Escabilidad</strong>: Cuando entrenamos LLMs a gran escala, aprovechar la precisión mixta es crucial para que el costo computacional no se nos vaya a la luna.</p></li></ul><figure><img src=images/floating_point.png alt="Comparativa de formatos de punto flotante: BF16, FP32 y FP16"><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 2. Comparación visual de los formatos de punto flotante más utilizados en deep learning: <b>BF16</b>, <b>FP32</b> y <b>FP16</b>.<br></figcaption></figure><h3 id=fp32-ieee-754-precisión-simple>FP32 (IEEE 754, precisión simple)<a hidden class=anchor aria-hidden=true href=#fp32-ieee-754-precisión-simple>#</a></h3><p>Este es el formato “normal” que se usa casi siempre. Guarda los números usando 1 bit para el signo, 8 para el exponente y 23 para la parte decimal (mantisa). Puede representar números muy chicos y muy grandes, desde $1.18\times10^{-38}$ hasta $3.4\times10^{38}$, y su precisión es muy alta ($\varepsilon \approx 1.19\times10^{-7}$).</p><p>En machine learning, FP32 es lo que se considera “precisión completa”. Incluso cuando usamos otros formatos para ahorrar memoria, los cálculos importantes (como acumular los gradientes) se hacen en FP32 para que el entrenamiento no se vuelva inestable.</p><h3 id=fp16-ieee-754-half>FP16 (IEEE 754, half)<a hidden class=anchor aria-hidden=true href=#fp16-ieee-754-half>#</a></h3><p>FP16 es un formato de número que usa menos memoria y permite que todo vaya más rápido. Básicamente, guarda los números usando menos bits que el formato normal (FP32), así que ocupa menos espacio y acelera los cálculos.</p><p>Lo bueno: hace que entrenar y usar modelos sea más rápido y barato. Lo malo: como tiene menos detalle y menos rango, a veces los números muy chicos pueden desaparecer (por eso se suele usar <a href=https://picdictionary.com/ml-dictionary/loss-scaling-in-ai-and-deep-learning target=_blank rel=noopener>loss scaling</a> para evitarlo), y si los números son muy grandes, se pueden &ldquo;saturar&rdquo; y perder información.</p><h3 id=bf16-brain-floating-point>BF16 (Brain Floating Point)<a hidden class=anchor aria-hidden=true href=#bf16-brain-floating-point>#</a></h3><p>BF16 es el formato que más se usa hoy para entrenar modelos grandes en TPUs y GPUs modernas (como la H100).</p><p>Guarda los números de una forma parecida a FP32 (el formato “normal”), pero con menos detalle en los decimales. Lo importante es que puede representar números igual de grandes o chicos que FP32, así que no se “rompe” con números extremos. Además, casi siempre funciona bien sin tener que hacer trucos raros como el <em>loss scaling</em>. Aunque no tiene tanta precisión en los decimales como FP16, para entrenar modelos grandes (como los LLMs) suele ser suficiente y no da problemas.</p><h2 id=comparación-de-precisiones>Comparación de precisiones<a hidden class=anchor aria-hidden=true href=#comparación-de-precisiones>#</a></h2><p>Para comparar las precisiones, voy a usar JAX, un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La razón de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitirá más adelante ver en &ldquo;crudo&rdquo; la paralelización de las operaciones y la optimización de la memoria.</p><p>Primero, importamos JAX y vemos la versión y el backend, así como los dispositivos disponibles:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Configuración de JAX</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX version: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;JAX backend: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>default_backend</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Available devices: </span><span class=si>{</span><span class=n>jax</span><span class=o>.</span><span class=n>devices</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Para medir el rendimiento y la memoria, necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecución. Para esto, vamos a usar <code>psutil</code>, <code>tracemalloc</code> y <code>time</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jax.numpy</span> <span class=k>as</span> <span class=nn>jnp</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>psutil</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tracemalloc</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_memory_usage</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Obtiene el uso actual de memoria en MB&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>process</span> <span class=o>=</span> <span class=n>psutil</span><span class=o>.</span><span class=n>Process</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>process</span><span class=o>.</span><span class=n>memory_info</span><span class=p>()</span><span class=o>.</span><span class=n>rss</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>measure_memory_and_time</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>wrapper</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>start</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>start_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=n>start_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>result</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>jax</span><span class=o>.</span><span class=n>block_until_ready</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>end_time</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=n>end_memory</span> <span class=o>=</span> <span class=n>get_memory_usage</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>current</span><span class=p>,</span> <span class=n>peak</span> <span class=o>=</span> <span class=n>tracemalloc</span><span class=o>.</span><span class=n>get_traced_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>       <span class=n>tracemalloc</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>result</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;execution_time&#39;</span><span class=p>:</span> <span class=n>end_time</span> <span class=o>-</span> <span class=n>start_time</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;memory_delta&#39;</span><span class=p>:</span> <span class=n>end_memory</span> <span class=o>-</span> <span class=n>start_memory</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;peak_memory&#39;</span><span class=p>:</span> <span class=n>peak</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=s1>&#39;current_memory&#39;</span><span class=p>:</span> <span class=n>current</span> <span class=o>/</span> <span class=mi>1024</span> <span class=o>/</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>       <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>wrapper</span>
</span></span></code></pre></div><p>Ahora, vamos a medir el rendimiento y la memoria de la multiplicación de matrices y la &ldquo;red neuronal&rdquo;. Para esto, vamos a usar el decorador <code>measure_memory_and_time</code> que definimos anteriormente.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>matrix_multiplication_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Prueba de multiplicación de matrices con precisión dada&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>key1</span><span class=p>,</span> <span class=n>key2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>matmul_operation</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=n>a</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key1</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>key2</span><span class=p>,</span> <span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>matmul_operation</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@measure_memory_and_time</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>dtype</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;&#34;&#34;Prueba de pase forward de red neuronal simple&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>   <span class=n>key</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>PRNGKey</span><span class=p>(</span><span class=mi>123</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>keys</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>key</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>def</span> <span class=nf>nn_forward</span><span class=p>():</span>
</span></span><span class=line><span class=cl>       <span class=c1># Inicialización de pesos</span>
</span></span><span class=line><span class=cl>       <span class=n>W1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b1</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>W2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>b2</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=p>(</span><span class=n>output_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=c1># Datos de entrada</span>
</span></span><span class=line><span class=cl>       <span class=n>x</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>keys</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>input_size</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=c1># Pase forward</span>
</span></span><span class=line><span class=cl>       <span class=n>h</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>       <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl>       <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>nn_forward</span><span class=p>()</span>
</span></span></code></pre></div><p>Para correr las pruebas, simplemente llamamos a las funciones <code>matrix_multiplication_test</code> y <code>neural_network_forward_pass_test</code> con los tipos de precisión y las dimensiones de las matrices y la red neuronal, por ejemplo:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>matrix_multiplication_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=p>(</span><span class=mi>5000</span><span class=p>,</span> <span class=mi>5000</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>neural_network_forward_pass_test</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><p>Para correr las pruebas, voy a incluir también FP64, para mostrar algo que puede ser contraintuitivo.</p><p>Dependiendo en qué hardware las estemos corriendo, los resultados pueden variar bastante.
Corriendo las pruebas en un M1, obtenemos los siguientes resultados para la multiplicación de matrices:</p><table><thead><tr><th>Precisión</th><th>Tiempo (s)</th><th>Memoria Pico (MB)</th></tr></thead><tbody><tr><td>FP16</td><td>1.024</td><td>0.013</td></tr><tr><td>BF16</td><td>0.978</td><td>0.008</td></tr><tr><td>FP32</td><td>0.943</td><td>0.008</td></tr><tr><td>FP64</td><td>0.928</td><td>0.009</td></tr></tbody></table><p>Y para la red neuronal:</p><table><thead><tr><th>Precisión</th><th>Tiempo (s)</th><th>Memoria Pico (MB)</th></tr></thead><tbody><tr><td>FP16</td><td>0.007</td><td>0.020</td></tr><tr><td>BF16</td><td>0.004</td><td>0.015</td></tr><tr><td>FP32</td><td>0.003</td><td>0.015</td></tr><tr><td>FP64</td><td>0.002</td><td>0.016</td></tr></tbody></table><p>A simple vista, uno pensaría: “entonces FP64 es lo mejor”. Pero en realidad no es así. Estos resultados se explican por varios factores:</p><ol><li><p><strong>CPUs están optimizadas para ciertos tipos</strong>: Los procesadores como el M1 funcionan mejor con FP32 y FP64 porque las librerías que usan (como Accelerate en Mac) están hechas para esos formatos. En cambio, FP16 y BF16 no están tan bien soportados en CPU, así que muchas veces el sistema tiene que convertirlos a FP32 o FP64 antes de hacer las cuentas, y eso las hace más lentas cuando el problema es chico.</p></li><li><p><strong>El tamaño importa</strong>: En los ejemplos de las tablas, las matrices y redes son chicas. Cuando los datos son pequeños, la mayor parte del tiempo se va en preparar todo (inicializar, convertir tipos, sincronizar), no en hacer las cuentas en sí. Por eso, a veces FP64 parece “más rápido”, pero es porque el camino para ese tipo es más directo y está mejor optimizado.</p></li><li><p><strong>En GPU es al revés</strong>: En las GPUs, FP16 y BF16 son mucho más rápidos porque el hardware tiene partes especiales (como Tensor Cores en NVIDIA) que están hechas para trabajar con estos formatos de baja precisión y pueden hacer muchas operaciones a la vez, usando menos memoria y ancho de banda.</p></li></ol><p>Para que lo comprueben ustedes mismos, les propongo que ejecuten las pruebas en una GPU usando matrices de dimensiones mucho mayores; así podrán observar la diferencia por su cuenta. A continuación les muestro una gráfica que ilustra claramente esa gran diferencia.</p><div style=display:flex;justify-content:center><figure><img src=images/nvidia-a100-matmul-tflops.png alt="Comparación de precisiones" style=max-width:350px;height:auto><figcaption style=text-align:center;font-size:.95em;color:#666>Figura 3. Comparación de precisiones.<br></figcaption></figure></div><p>Por útimo, a modo de conclusión de esta sección, les dejo un ejemplo de implementación de precisión mixta, que es lo que suele hacerse en la práctica para entrenar modelos a gran escala.</p><p>La idea central es simple: las partes del modelo que requieren estabilidad numérica se calculan en FP32, mientras que los resultados intermedios, gradientes y parámetros se almacenan en FP16 o BF16, aprovechando así el ahorro de memoria y el mayor throughput del hardware.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Ejemplo de entrenamiento con precisión mixta</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mixed_precision_forward_pass</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=c1># Convertir a FP32 para cómputo</span>
</span></span><span class=line><span class=cl>   <span class=n>x_fp32</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>W_fp32</span> <span class=o>=</span> <span class=n>W</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=n>b_fp32</span> <span class=o>=</span> <span class=n>b</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Pase forward</span>
</span></span><span class=line><span class=cl>   <span class=n>y</span> <span class=o>=</span> <span class=n>jnp</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x_fp32</span><span class=p>,</span> <span class=n>W_fp32</span><span class=p>)</span> <span class=o>+</span> <span class=n>b_fp32</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>   <span class=c1># Convertir de vuelta a FP16 para eficiencia de memoria</span>
</span></span><span class=line><span class=cl>   <span class=k>return</span> <span class=n>y</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=quantización>Quantización<a hidden class=anchor aria-hidden=true href=#quantización>#</a></h1></div><footer class=post-footer><ul class=post-tags><li><a href=https://juanlebrero.com/es/tags/lora/>LoRA</a></li><li><a href=https://juanlebrero.com/es/tags/qlora/>QLoRA</a></li><li><a href=https://juanlebrero.com/es/tags/llms/>LLMs</a></li><li><a href=https://juanlebrero.com/es/tags/finetuning/>Finetuning</a></li><li><a href=https://juanlebrero.com/es/tags/cuantizaci%C3%B3n/>Cuantización</a></li><li><a href=https://juanlebrero.com/es/tags/4-bit/>4-Bit</a></li><li><a href=https://juanlebrero.com/es/tags/deepspeed/>Deepspeed</a></li><li><a href=https://juanlebrero.com/es/tags/fsdp/>Fsdp</a></li><li><a href=https://juanlebrero.com/es/tags/zero/>Zero</a></li><li><a href=https://juanlebrero.com/es/tags/precisi%C3%B3n/>Precisión</a></li><li><a href=https://juanlebrero.com/es/tags/jax/>JAX</a></li><li><a href=https://juanlebrero.com/es/tags/bfloat16/>Bfloat16</a></li><li><a href=https://juanlebrero.com/es/tags/fp16/>Fp16</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://juanlebrero.com/es/>Juan Lebrero</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>