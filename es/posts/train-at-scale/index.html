<!DOCTYPE html>
<html lang="es"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisi√≥n num√©rica, paralelizaci√≥n de datos, cuantizaci√≥n, LoRA, y m√°s.
Precisi√≥n num√©rica
La elecci√≥n del formato num√©rico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores m√°s determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisi√≥n num√©rica y como afecta al rendimiento de los modelos, lo que se explicar√° en esta secci√≥n.">  

  <title>
    
      Entrenamiento a Gran Escala: FSDP, QLoRA, y m√°s.
    
  </title>

  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  
  
  <link rel="stylesheet" href="/css/main.28a7439b9550b83d3a5c7595d7a696931ebc2a2ad3d4ca19111b9e621784331c7904e46b7aff95891dc7f536272d85359b62fe0fe14c1c1841e57d9799b774a6.css" integrity="sha512-KKdDm5VQuD06XHWV16aWkx68KirT1MoZERueYheEMxx5BORrev&#43;ViR3H9TYnLYU1m2L&#43;D&#43;FMHBhB5X2Xmbd0pg==" />
  
  
  <link rel="stylesheet" href="/css/custom.css" />
  
   <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
 
</head>
<body a="auto">
<button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
  <span class="theme-icon">üåô</span>
</button>

<style>
.theme-toggle {
  background: none;
  border: none;
  cursor: pointer;
  font-size: 1.2em;
  padding: 0.5rem;
  border-radius: 50%;
  transition: all 0.3s ease;
  position: fixed;
  top: 1rem;
  right: 1rem;
  z-index: 1000;
  width: 2.5rem;
  height: 2.5rem;
  display: flex;
  align-items: center;
  justify-content: center;
}

.theme-toggle:hover {
  background-color: var(--primary-text-color, #000);
  color: var(--bg-color, #fff);
  transform: scale(1.1);
}

.theme-toggle:focus {
  outline: 2px solid var(--link-color, #3548cf);
  outline-offset: 2px;
}

.theme-icon {
  transition: transform 0.3s ease;
}

.theme-toggle:hover .theme-icon {
  transform: rotate(180deg);
}

 
@media (max-width: 768px) {
  .theme-toggle {
    top: 0.5rem;
    right: 0.5rem;
    width: 2rem;
    height: 2rem;
    font-size: 1em;
  }
}
</style>

<script>
(function() {
  'use strict';
  
  
  const THEME_KEY = 'theme-preference';
  const THEMES = {
    LIGHT: 'light', 
    DARK: 'dark'
  };
  
  
  const ICONS = {
    [THEMES.LIGHT]: '‚òÄÔ∏è',
    [THEMES.DARK]: 'üåô'
  };
  
  
  function getCurrentTheme() {
    return localStorage.getItem(THEME_KEY) || THEMES.LIGHT;
  }
  
  
  function saveTheme(theme) {
    localStorage.setItem(THEME_KEY, theme);
  }
  
  
  function applyTheme(theme) {
    document.body.setAttribute('a', theme);
  }
  
  
  function updateButtonIcon(theme) {
    const button = document.getElementById('theme-toggle');
    const icon = button.querySelector('.theme-icon');
    icon.textContent = ICONS[theme];
  }
  
  
  function cycleTheme() {
    const current = getCurrentTheme();
    let next;
    
    switch(current) {
      case THEMES.LIGHT:
        next = THEMES.DARK;
        break;
      case THEMES.DARK:
        next = THEMES.LIGHT;
        break;
      default:
        next = THEMES.LIGHT;
    }
    
    saveTheme(next);
    applyTheme(next);
    updateButtonIcon(next);
  }
  
  
  function initTheme() {
    const theme = getCurrentTheme();
    applyTheme(theme);
    updateButtonIcon(theme);
  }
  
  
  function addToggleListener() {
    const button = document.getElementById('theme-toggle');
    if (button) {
      button.addEventListener('click', cycleTheme);
    }
  }
  
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
      initTheme();
      addToggleListener();
    });
  } else {
    initTheme();
    addToggleListener();
  }
})();
</script>

<div class="lang-switch-container">
  <ul class="lang-switch">
    <li><a href="/" title="English" aria-label="English">EN</a>
    </li>
  </ul>
</div>
<main class="page-content" aria-label="Content">
            <div class="w"><nav class="breadcrumbs" aria-label="Breadcrumb">
  <ol><li>
      <a href="http://localhost:1313/es/">Juan Lebrero</a>
    </li><li>
      <a href="http://localhost:1313/es/posts/">Posts</a>
    </li><li aria-current="page">Entrenamiento a Gran Escala: FSDP, QLoRA, y m√°s.</li>
  </ol>
</nav>
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2025-09-13 00:00:00 &#43;0000 UTC">
            2025-09-13
        </time>
    </p>

    <h1>Entrenamiento a Gran Escala: FSDP, QLoRA, y m√°s.</h1>

    

    <p>Para poder entrenar modelos a gran escala, necesitamos entender diversos conceptos que nos van a ayudar a optimizar el rendimiento y la estabilidad del entrenamiento. Por eso, vamos a ver conceptos como precisi√≥n num√©rica, paralelizaci√≥n de datos, cuantizaci√≥n, LoRA, y m√°s.</p>
<h2 id="precisi√≥n-num√©rica">Precisi√≥n num√©rica</h2>
<p>La elecci√≥n del formato num√©rico (FP32, FP16, BF16, FP8, INT8, etc.) constituye uno de los factores m√°s determinantes para el rendimiento, el uso de memoria y la estabilidad del entrenamiento de modelos de gran escala. Por eso, es importante entender como funciona la precisi√≥n num√©rica y como afecta al rendimiento de los modelos, lo que se explicar√° en esta secci√≥n.</p>
<h3 id="por-qu√©-importa-la-precisi√≥n">¬øPor qu√© Importa la Precisi√≥n?</h3>
<p>¬øComo representar√≠as el n√∫mero $\pi$, un n√∫mero con decimales INFINITOS, en algo FINITO como lo es una computadora? De est√° pregunta, surge como respuesta el punto flotante.</p>
<p>Los n√∫meros representados con punto flotante representan, de manera aproximada, a los n√∫meros reales, y lo hacen con dos componentes clave: la <em>mantisa</em> y el <em>exponente</em>.</p>
<p>Un n√∫mero en punto flotante representa aproximadamente un valor real mediante la f√≥rmula:</p>
<p>$$\text{valor} \approx \text{signo} \times \text{mantisa} \times \text{base}^{\text{exponente}}$$</p>
<p>donde:</p>
<ul>
<li><strong>Mantisa</strong>: Controla la resoluci√≥n fina (cu√°ntos pasos discretos entra en el intervalo 1.0 - 2.0)</li>
<li><strong>Exponente</strong>: Determina el rango din√°mico (qu√© tan grandes o peque√±os pueden ser los n√∫meros representables)</li>
<li><strong>Base</strong>: En IEEE 754 es 2.</li>
</ul>
<p>M√°s bits para la mantisa $\rightarrow$ mayor precisi√≥n</p>
<p>M√°s bits para el exponente $\rightarrow$ mayor rango</p>
<figure>
 <img src="images/layout.png" alt="Representaci√≥n de Punto Flotante de 32 bits (FP32)" style="width: 100%; height: auto;">
 <figcaption style="text-align: center; font-size: 0.95em; color: #666;">
   Figura 1. Esquema de la representaci√≥n de un n√∫mero en punto flotante de 32 bits (FP32) seg√∫n el est√°ndar IEEE 754. <br>
 </figcaption>
</figure>
<p>Pero, ¬øpor qu√© nos interesa a nosotros?</p>
<p>Bueno, hay tres razones principales por las que la precisi√≥n num√©rica es crucial en el entrenamiento de modelos de gran escala:</p>
<ul>
<li>
<p><strong>Eficiencia computacional</strong>: Los formatos de menor ancho de bits aceleran el c√≥mputo en Tensor Cores/TPUs y reducen bastante el uso de memoria.</p>
</li>
<li>
<p><strong>Estabilidad num√©rica</strong>: B√°sicamente, si el formato de n√∫mero no tiene suficiente rango o detalle, los n√∫meros pueden volverse demasiado grandes, demasiado chiquitos o perder precisi√≥n, lo que puede causar errores o resultados raros durante el entrenamiento.</p>
</li>
<li>
<p><strong>Escabilidad</strong>: Cuando entrenamos LLMs a gran escala, aprovechar la precisi√≥n mixta es crucial para que el costo computacional no se nos vaya a la luna.</p>
</li>
</ul>
<figure>
 <img src="images/floating_point.png" alt="Comparativa de formatos de punto flotante: BF16, FP32 y FP16">
 <figcaption style="text-align: center; font-size: 0.95em; color: #666;">
   Figura 2. Comparaci√≥n visual de los formatos de punto flotante m√°s utilizados en deep learning: <b>BF16</b>, <b>FP32</b> y <b>FP16</b>. <br>
 </figcaption>
</figure>
<h3 id="fp32-ieee-754-precisi√≥n-simple">FP32 (IEEE 754, precisi√≥n simple)</h3>
<p>Este es el formato ‚Äúnormal‚Äù que se usa casi siempre. Guarda los n√∫meros usando 1 bit para el signo, 8 para el exponente y 23 para la parte decimal (mantisa). Puede representar n√∫meros muy chicos y muy grandes, desde $1.18\times10^{-38}$ hasta $3.4\times10^{38}$, y su precisi√≥n es muy alta ($\varepsilon \approx 1.19\times10^{-7}$).</p>
<p>En machine learning, FP32 es lo que se considera ‚Äúprecisi√≥n completa‚Äù. Incluso cuando usamos otros formatos para ahorrar memoria, los c√°lculos importantes (como acumular los gradientes) se hacen en FP32 para que el entrenamiento no se vuelva inestable.</p>
<h3 id="fp16-ieee-754-half">FP16 (IEEE 754, half)</h3>
<p>FP16 es un formato de n√∫mero que usa menos memoria y permite que todo vaya m√°s r√°pido. B√°sicamente, guarda los n√∫meros usando menos bits que el formato normal (FP32), as√≠ que ocupa menos espacio y acelera los c√°lculos.</p>
<p>Lo bueno: hace que entrenar y usar modelos sea m√°s r√°pido y barato. Lo malo: como tiene menos detalle y menos rango, a veces los n√∫meros muy chicos pueden desaparecer (por eso se suele usar <a href="https://picdictionary.com/ml-dictionary/loss-scaling-in-ai-and-deep-learning" target="_blank" rel="noopener">loss scaling</a> para evitarlo), y si los n√∫meros son muy grandes, se pueden &ldquo;saturar&rdquo; y perder informaci√≥n.</p>
<h3 id="bf16-brain-floating-point">BF16 (Brain Floating Point)</h3>
<p>BF16 es el formato que m√°s se usa hoy para entrenar modelos grandes en TPUs y GPUs modernas (como la H100).</p>
<p>Guarda los n√∫meros de una forma parecida a FP32 (el formato ‚Äúnormal‚Äù), pero con menos detalle en los decimales. Lo importante es que puede representar n√∫meros igual de grandes o chicos que FP32, as√≠ que no se ‚Äúrompe‚Äù con n√∫meros extremos. Adem√°s, casi siempre funciona bien sin tener que hacer trucos raros como el <em>loss scaling</em>. Aunque no tiene tanta precisi√≥n en los decimales como FP16, para entrenar modelos grandes (como los LLMs) suele ser suficiente y no da problemas.</p>
<h2 id="comparaci√≥n-de-precisiones">Comparaci√≥n de precisiones</h2>
<p>Para comparar las precisiones, voy a usar JAX, un framework de ML hecho por Google, que permite realizar operaciones de manera eficiente en GPUs y TPUs. La raz√≥n de utilizar JAX y no PyTorch, por ejemplo, es que JAX nos permitir√° m√°s adelante ver en &ldquo;crudo&rdquo; la paralelizaci√≥n de las operaciones y la optimizaci√≥n de la memoria.</p>
<p>Primero, importamos JAX y vemos la versi√≥n y el backend, as√≠ como los dispositivos disponibles:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Configuraci√≥n de JAX</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;JAX version: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;JAX backend: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">default_backend</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Available devices: </span><span class="si">{</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>Necesitamos funciones para obtener el uso de memoria y medir el tiempo de ejecuci√≥n. Para esto, vamos a usar <code>psutil</code>, <code>tracemalloc</code> y <code>time</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">psutil</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tracemalloc</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_memory_usage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">   <span class="s2">&#34;&#34;&#34;Obtiene el uso actual de memoria en MB&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">measure_memory_and_time</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">       <span class="n">tracemalloc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">start_memory</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">       <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">jax</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">       <span class="n">end_memory</span> <span class="o">=</span> <span class="n">get_memory_usage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">current</span><span class="p">,</span> <span class="n">peak</span> <span class="o">=</span> <span class="n">tracemalloc</span><span class="o">.</span><span class="n">get_traced_memory</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">       <span class="n">tracemalloc</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;result&#39;</span><span class="p">:</span> <span class="n">result</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;execution_time&#39;</span><span class="p">:</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;memory_delta&#39;</span><span class="p">:</span> <span class="n">end_memory</span> <span class="o">-</span> <span class="n">start_memory</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;peak_memory&#39;</span><span class="p">:</span> <span class="n">peak</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">           <span class="s1">&#39;current_memory&#39;</span><span class="p">:</span> <span class="n">current</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">       <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">wrapper</span>
</span></span></code></pre></div><p>Ahora, vamos a medir el rendimiento y la memoria de la multiplicaci√≥n de matrices y la &ldquo;red neuronal&rdquo;. Para esto, vamos a usar el decorador <code>measure_memory_and_time</code> que definimos anteriormente.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@measure_memory_and_time</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">matrix_multiplication_test</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="s2">&#34;&#34;&#34;Prueba de multiplicaci√≥n de matrices con precisi√≥n dada&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="k">def</span> <span class="nf">matmul_operation</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">       <span class="n">a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key1</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">matmul_operation</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@measure_memory_and_time</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">neural_network_forward_pass_test</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="s2">&#34;&#34;&#34;Prueba de pase forward de red neuronal simple&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="k">def</span> <span class="nf">nn_forward</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">       <span class="c1"># Inicializaci√≥n de pesos</span>
</span></span><span class="line"><span class="cl">       <span class="n">W1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">b1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">hidden_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">W2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">b2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">       <span class="c1"># Datos de entrada</span>
</span></span><span class="line"><span class="cl">       <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">       <span class="c1"># Pase forward</span>
</span></span><span class="line"><span class="cl">       <span class="n">h</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">       <span class="k">return</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">nn_forward</span><span class="p">()</span>
</span></span></code></pre></div><p>Para correr las pruebas, simplemente llamamos a las funciones <code>matrix_multiplication_test</code> y <code>neural_network_forward_pass_test</code> con los tipos de precisi√≥n y las dimensiones de las matrices y la red neuronal, por ejemplo:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">matrix_multiplication_test</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="mi">5000</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">neural_network_forward_pass_test</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span></code></pre></div><p>Para correr las pruebas, voy a incluir tambi√©n FP64, para mostrar algo que puede ser contraintuitivo.</p>
<p>Dependiendo en qu√© hardware las estemos corriendo, los resultados pueden variar bastante.
Corriendo las pruebas en un M1, obtenemos los siguientes resultados. Tiempo en segundos y memoria en MB:</p>
<div style="display: flex; gap: 2px; flex-wrap: wrap;">
<div style="flex: 1; min-width: 300px;">
<p><strong>Multiplicaci√≥n de matrices:</strong></p>
<table>
  <thead>
      <tr>
          <th>Precisi√≥n</th>
          <th>Tiempo</th>
          <th>Memoria Pico</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FP16</td>
          <td>1.024</td>
          <td>0.013</td>
      </tr>
      <tr>
          <td>BF16</td>
          <td>0.978</td>
          <td>0.008</td>
      </tr>
      <tr>
          <td>FP32</td>
          <td>0.943</td>
          <td>0.008</td>
      </tr>
      <tr>
          <td>FP64</td>
          <td><strong>0.928</strong></td>
          <td>0.009</td>
      </tr>
  </tbody>
</table>
</div>
<div style="flex: 1; min-width: 300px;">
<p><strong>Red neuronal:</strong></p>
<table>
  <thead>
      <tr>
          <th>Precisi√≥n</th>
          <th>Tiempo</th>
          <th>Memoria Pico</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FP16</td>
          <td>0.007</td>
          <td>0.020</td>
      </tr>
      <tr>
          <td>BF16</td>
          <td>0.004</td>
          <td>0.015</td>
      </tr>
      <tr>
          <td>FP32</td>
          <td>0.003</td>
          <td>0.015</td>
      </tr>
      <tr>
          <td>FP64</td>
          <td><strong>0.002</strong></td>
          <td>0.016</td>
      </tr>
  </tbody>
</table>
</div>
</div>
<p>A simple vista, uno pensar√≠a que este blog no sirve de nada y que todo lo que dije es una farsa porque &ldquo;FP64 dio mejor&rdquo;. Pero en realidad no es as√≠. Estos resultados se explican por varios factores:</p>
<ol>
<li>
<p><strong>CPUs est√°n optimizadas para ciertos tipos</strong>: Los procesadores como el M1 funcionan mejor con FP32 y FP64 porque las librer√≠as que usan (como Accelerate en Mac) est√°n hechas para esos formatos. En cambio, FP16 y BF16 no est√°n tan bien soportados en CPU, as√≠ que muchas veces el sistema tiene que convertirlos a FP32 o FP64 antes de hacer las cuentas, y eso las hace m√°s lentas cuando el problema es chico.</p>
</li>
<li>
<p><strong>El tama√±o importa</strong>: En los ejemplos de las tablas, las matrices y redes son chicas. Cuando los datos son peque√±os, la mayor parte del tiempo se va en preparar todo (inicializar, convertir tipos, sincronizar), no en hacer las cuentas en s√≠. Por eso, a veces FP64 parece ‚Äúm√°s r√°pido‚Äù, pero es porque el camino para ese tipo es m√°s directo y est√° mejor optimizado.</p>
</li>
<li>
<p><strong>En GPU es al rev√©s</strong>: En las GPUs, FP16 y BF16 son mucho m√°s r√°pidos porque el hardware tiene partes especiales (como Tensor Cores en NVIDIA) que est√°n hechas para trabajar con estos formatos de baja precisi√≥n y pueden hacer muchas operaciones a la vez, usando menos memoria y ancho de banda.</p>
</li>
</ol>
<p>Para que lo comprueben ustedes mismos, si se ejecutan las pruebas en una GPU usando matrices de dimensiones mucho mayores, se va a ver la diferencia. A continuaci√≥n muestro una gr√°fica que la ilustra claramente medido en TFLOPS (Teraflops), calculado como el n√∫mero de operaciones de punto flotante por segundo.</p>
<div style="display: flex; justify-content: center;">
  <figure>
    <img src="images/nvidia-a100-matmul-tflops.png" alt="Comparaci√≥n de precisiones" style="max-width: 350px; height: auto;">
    <figcaption style="text-align: center; font-size: 0.95em; color: #666;">
      Figura 3. Comparaci√≥n de precisiones. <br>
    </figcaption>
  </figure>
</div>
<p>Por √∫timo, a modo de conclusi√≥n de esta secci√≥n, les dejo un ejemplo de implementaci√≥n de precisi√≥n mixta, que es lo que suele hacerse en la pr√°ctica para entrenar modelos a gran escala.</p>
<p>La idea central es simple: las partes del modelo que requieren estabilidad num√©rica se calculan en FP32, mientras que los resultados intermedios, gradientes y par√°metros se almacenan en FP16 o BF16, aprovechando as√≠ el ahorro de memoria y el mayor throughput del hardware.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">mixed_precision_forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">   <span class="c1"># Convertir a FP32 para c√≥mputo</span>
</span></span><span class="line"><span class="cl">   <span class="n">x_fp32</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">W_fp32</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   <span class="n">b_fp32</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="c1"># Pase forward</span>
</span></span><span class="line"><span class="cl">   <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_fp32</span><span class="p">,</span> <span class="n">W_fp32</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fp32</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="c1"># Convertir de vuelta a FP16 para eficiencia de memoria</span>
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</span></span></code></pre></div><h1 id="cuantizaci√≥n">Cuantizaci√≥n</h1>
<p>Cuando entrenamos modelos, solemos utilizar FP32 para los pesos y FP16 para los gradientes, pero al llevar el modelo a producci√≥n surge un desaf√≠o: necesitamos reducir el consumo de memoria y la latencia sin sacrificar la calidad. Ac√° entra en juego la cuantizaci√≥n. Este proceso consiste en representar los pesos y activaciones con menos bits, lo que permite ahorrar memoria y acelerar la inferencia, a cambio de introducir un peque√±o error num√©rico controlado.</p>
<p>Hay 2 enfoques principales: Post Training Quantization (PTQ) y Quantization Aware Training (QAT).</p>
<div style="display: flex; flex-direction: column; align-items: center; margin: 2em 0;">
  <img src="images/Quantization-Aware-TrainingQAT-and-Post-Training-Quantization-PTQ.png" alt="Comparaci√≥n entre QAT y PTQ" style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
  <p style="text-align: center; margin-top: 0.5em; color: #555;">
    Figura 4. Comparaci√≥n visual entre QAT (Izquierda) y PTQ (Derecha).
  </p>
</div>
<h2 id="post-training-quantization-ptq">Post Training Quantization (PTQ)</h2>
<p>La cuantizaci√≥n post entrenamiento, <strong>PTQ</strong>, toma un modelo ya entrenado y convierte pesos y activaciones a formatos de menor precisi√≥n sin volver a entrenar. La mayor√≠a de las veces es suficiente.</p>
<p>Matem√°ticamente, si partimos de un valor real $x\in\mathbb{R}$ en FP32, la cuantizaci√≥n cl√°sica mapea $x$ a un entero $q$ de $b$ bits mediante una escala $s$ y un zero point $z$:</p>
<p>$$
q = \operatorname{clip}\Big(\operatorname{round}\big(\tfrac{x}{s}\big) + z,; q_{\min}, q_{\max}\Big),
$$</p>
<p>y para reconstruir en reales usamos</p>
<p>$$
\hat{x} = s\cdot (q - z).
$$</p>
<p>El rango entero $[q_{\min},q_{\max}]$ depende de si usamos representaci√≥n signada o no. Para $b$ bits signados, t√≠picamente $q_{\min}=-2^{b-1}$ y $q_{\max}=2^{b-1}-1$. La elecci√≥n de $s$ y $z$ define cu√°nto error agregamos.</p>
<p>Si queremos cubrir un intervalo real $[\alpha,\beta]$ la escala pr√°ctica es:</p>
<p>$$
s=\frac{\beta-\alpha}{q_{\max}-q_{\min}},
\qquad
z=\operatorname{round}\Big(\frac{-\alpha}{s}\Big)+q_{\min}.
$$</p>
<p>Dependiendo del valor de $z$, la cuantizaci√≥n puede ser sim√©trica o asim√©trica. En la sim√©trica se fija $z=0$ y se toma $s=\tfrac{\max|x|}{q_{\max}}$, de modo que el rango entero queda centrado alrededor de cero. En cambio, si la distribuci√≥n de los valores est√° sesgada, la cuantizaci√≥n asim√©trica con $z\neq 0$ desplaza el rango y aprovecha mejor los niveles representables.</p>
<p>En python podr√≠amos implementar algo as√≠:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">quantize_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">signed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">signed</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmin</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmax</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmin</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">qmax</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x_min</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_max</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">qmax</span> <span class="o">-</span> <span class="n">qmin</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">scale</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">qmin</span> <span class="o">-</span> <span class="n">x_min</span> <span class="o">/</span> <span class="p">(</span><span class="n">scale</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">zero_point</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">zero_point</span><span class="p">,</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="n">scale</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">zero_point</span><span class="p">),</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_hat</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="n">zero_point</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">zero_point</span><span class="p">)</span>
</span></span></code></pre></div><p>Para cuantizar par√°metros suele bastar con usar los m√≠nimos y m√°ximos por tensor o por canal, porque en general sus distribuciones est√°n centradas. Con activaciones la situaci√≥n es distinta: una ReLU, por ejemplo, devuelve solo valores no negativos, lo que sesga la distribuci√≥n hacia el lado positivo. En ese caso, una cuantizaci√≥n sim√©trica desperdicia la mitad de los niveles en valores negativos vac√≠os. Por eso se usa cuantizaci√≥n asim√©trica (o directamente unsigned) junto con un conjunto de calibraci√≥n que permita estimar rangos representativos y evitar saturaciones por outliers.</p>
<p>En PTQ el paso cr√≠tico es calibrar correctamente las activaciones. Una estrategia com√∫n es usar percentiles para que unos pocos valores extremos no definan toda la escala. Otra es aplicar clipping, aceptando un ligero sesgo a cambio de reducir errores de saturaci√≥n.</p>
<div style="display: flex; flex-direction: column; align-items: center; margin: 2em 0;">
  <img src="images/quantization_cropped.png" alt="Comparaci√≥n entre QAT y PTQ" style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
  <p style="text-align: center; margin-top: 0.5em; color: #555;">
    Figura 5. Comparaci√≥n entre cuantizaci√≥n sim√©trica y asim√©trica.
  </p>
</div>
<p>PTQ es r√°pido y √∫til cuando la ca√≠da de desempe√±o es chiquita; si no alcanza, se recurre a QAT.</p>
<h2 id="quantization-aware-training-qat">Quantization Aware Training (QAT)</h2>
<p>El objetivo de QAT es entrenar un modelo sabiendo que, en inferencia, sus pesos y activaciones van a pasar por un cuantizador. Escribo el problema as√≠:</p>
<div class="math">$$
\min_{w}\ \mathbb{E}_{(x,y)\sim\mathcal{D}}\Big[L\big(f_{q(w)}(x),\,y\big)\Big].
$$</div>
<p>$w$ es el conjunto completo de par√°metros reales del modelo antes de cuantizar. $x$ es una entrada del dataset y $y$ es su etiqueta o valor objetivo. $\mathcal{D}$ es la distribuci√≥n de datos que genera los pares $(x,y)$. $\mathbb{E}$ es la esperanza sobre esa distribuci√≥n. $f_{q(w)}$ es el mismo modelo que usar√≠as en FP32 pero con sus pesos pasados por un operador de cuantizaci√≥n $q(\cdot)$. $L(\hat y,y)$ es la funci√≥n de costo que compara la predicci√≥n $\hat y=f_{q(w)}(x)$ con el objetivo $y$. Puede ser entrop√≠a cruzada, error cuadr√°tico medio u otra diferenciable respecto de las salidas del modelo.</p>
<p>Si definiera $q(w)$ de forma literal con redondeo y recorte, el objetivo ser√≠a no diferenciable. Ese cuantizador uniforme af√≠n es</p>
<div class="math">$$
q=\operatorname{clip}\!\big(\operatorname{round}(u),\,q_{\min},\,q_{\max}\big),\qquad
u=\frac{w}{s}+z,\qquad
\tilde w=s\,(q-z).
$$</div>
<p>donde $u=\tfrac{w}{s}+z$ es la versi√≥n escalada y desplazada de $w$ para pasar por el cuantizador, $s&gt;0$ es la escala y $z$ es el zero point que alinea el cero real con un entero. $[q_{\min},q_{\max}]$ es el rango entero permitido por el ancho de bits. $\tilde w$ es la versi√≥n de $w$ de-cuantizada que se usa para computar la salida y la p√©rdida. El forward siempre se hace con $\tilde w$, no con $w$ directo.</p>
<p>Como $\operatorname{round}$ y $\operatorname{clip}$ no son diferenciables en sentido estricto, QAT usa el estimador <a href="https://hassanaskary.medium.com/intuitive-explanation-of-straight-through-estimators-with-pytorch-implementation-71d99d25d9d0" target="_blank">straight through</a> en el backward. La idea es dejar pasar gradiente como si el redondeo fuera la identidad y hacer que el clip tenga derivada uno dentro del rango √∫til y cero en saturaci√≥n. Con $u=\tfrac{w}{s}+z$ queda</p>
<div class="math">$$
\frac{\partial \tilde w}{\partial w}\ \approx\ 
\begin{cases}
1 & \text{si } q_{\min} < u < q_{\max} \\
0 & \text{en caso contrario}
\end{cases}
$$</div>
<p>En otras palabras: mientras el valor $u$ no se pase de los l√≠mites del cuantizador, el gradiente se transmite normalmente, como si no hubiera cuantizaci√≥n. Pero si $u$ se sale del rango (por ejemplo, porque $w$ es muy grande o muy chico), el gradiente se bloquea y no pasa. As√≠, el modelo aprende a mantener los valores dentro del rango √∫til del cuantizador, y solo se &ldquo;corta&rdquo; el gradiente cuando hay saturaci√≥n.</p>
<p>Si adem√°s se decide aprender la escala, se trata $s$ como par√°metro y se usa otra vez STE. Reescribiendo $q\approx \operatorname{clip}(u,q_{\min},q_{\max})$ para propagar gradiente, una forma clara del t√©rmino es</p>
<div class="math">$$
\frac{\partial \tilde w}{\partial s}\ \approx\ 
\begin{cases}
-\,z\;-\;\frac{w}{s} & \text{si } q_{\min} < u < q_{\max} \\
\operatorname{clip}(u,\,q_{\min},\,q_{\max})\;-\;z\;-\;\frac{w}{s} & \text{en caso contrario}
\end{cases}
$$</div>
<p>En la pr√°ctica este gradiente se normaliza por la cantidad de elementos que comparten la misma escala, lo que estabiliza la actualizaci√≥n. Ac√° aparece el concepto per-tensor y per-channel.</p>
<p>Definimos una capa lineal con pesos $W\in\mathbb{R}^{C_o\times C_i}$ que recibe activaciones $X\in\mathbb{R}^{T\times C_i}$, donde $T$ es la cantidad de tokens, $C_i$ los canales de entrada y $C_o$ los de salida.</p>
<ul>
<li>
<p>En per-tensor se usa una sola escala para todo $X$ y una sola escala para todo $W$. Esa elecci√≥n simplifica el c√≥mputo y la normalizaci√≥n del gradiente de $s$ se hace sobre todos los elementos del bloque, pero si los rangos internos difieren mucho se desperdicia resoluci√≥n.</p>
</li>
<li>
<p>En per-channel se asigna una escala distinta a cada canal de salida de $W$, lo que en la pr√°ctica implica un vector $\Delta_W\in\mathbb{R}^{1\times C_o}$ y una normalizaci√≥n por elementos de cada canal. Si adem√°s se aplica per-token en activaciones, se usa $\Delta_X\in\mathbb{R}^{T\times 1}$ y cada fila de $X$ tiene su propia escala. Este esquema alinea la cuantizaci√≥n con la heterogeneidad real de los datos y reduce el error en 8 y sobre todo en 4 bits, a costa de almacenar m√°s escalas.</p>
</li>
</ul>
<p align="center">
  <img src="images/per-channel_per-tensor.png" alt="Per-tensor vs Per-channel quantization" style="max-width: 100%; height: auto;">
</p>
<p align="center" style="color:#555; margin-top:0.5em;">
  \(X\in\mathbb{R}^{T\times C_i}\) son activaciones y \(W\in\mathbb{R}^{C_o\times C_i}\) son pesos. Arriba se ilustra per-tensor, con \(\Delta X[1]\) y \(\Delta W[1]\) como escalas √∫nicas para todo \(X\) y todo \(W\). Abajo se muestra per-token m√°s per-channel, con \(\Delta X[T\times 1]\) como una escala por fila de \(X\) y \(\Delta W[1\times C_o]\) como una escala por canal de salida de \(W\). Las zonas punteadas indican el alcance de cada escala.
</p>
<p>Por √∫ltimo, todo esto se entrena como riesgo emp√≠rico con mini batches y con el mismo esquema de cuantizaci√≥n usado en el forward. En cada paso se calcula la salida con $\tilde w=s,(q-z)$ aplicando las escalas per-tensor o per-channel de los pesos y, si corresponde, per-token en las activaciones; se eval√∫a la p√©rdida; y se hace backward con STE. El problema queda</p>
<div class="math">$$
\min_{w}\ \frac{1}{B}\sum_{i=1}^{B} L\big(f_{\tilde w}(x_i),\,y_i\big),
$$</div>
<p>donde $\tilde w$ depende de $w,s,z$ y de la granularidad elegida para las escalas. Cuando hay varias escalas se normaliza su gradiente por el n√∫mero de elementos que las comparten para estabilizar. El optimizador actualiza $w$ y, si est√°n habilitadas, tambi√©n las escalas $s$.</p>

</article>

                
<footer class="footer">
  <div class="footer-content">
    <p>&copy; 2025 Juan Lebrero. All rights reserved.</p>
    <div class="footer-links">
      <a href="/es/about/">About</a>
      <span class="separator">‚Ä¢</span>
      <a href="/es/contact/">Contact</a>
      <span class="separator">‚Ä¢</span>
      <a href="/es/index.xml">RSS</a>
    </div>
  </div>
</footer>

            </div>
        </main>
    </body>
</html>
