<!doctype html><html lang=es><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Imagin√° intentar aprender un idioma viendo solo el 25% de cada oraci√≥n. Suena como una idea terrible, ¬øno?
Bueno, resulta que esto es exactamente lo que hacen los Vision Transformer Masked Autoencoders (ViT-MAE) con las im√°genes. El paper &ldquo;Masked Autoencoders Are Scalable Vision Learners&rdquo; de He et al. mostr√≥ algo bastante loco: si ocult√°s el 75% de una imagen (dividi√©ndola en parches y enmascarando aleatoriamente el 75% de ellos) y le ped√≠s a un modelo que la reconstruya, obten√©s representaciones visuales sorprendentemente poderosas."><title>Haciendo sufrir a los autoencoders: El caso de los Masked Autoencoders</title><link rel=icon type=image/x-icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png href=https://juanlebrero.com/favicon.png><link rel=stylesheet href=/css/main.5efeca130dc78ba84d14ac924c14a4b5dadfbcac0163ede7b1bc3c21759bc2704ef41737909fe927cf3b93b0624eec5209599f63520c0a0f093a82051fb67592.css integrity="sha512-Xv7KEw3Hi6hNFKySTBSktdrfvKwBY+3nsbw8IXWbwnBO9Bc3kJ/pJ887k7BiTuxSCVmfY1IMCg8JOoIFH7Z1kg=="><link rel=stylesheet href=/css/custom.css><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body a=auto><header class=header><nav class=nav><div class=logo><a href=/es/ accesskey=h title=Home>Juan Francisco Lebrero</a></div><ul id=menu><li><a href=/es/ title=Inicio><span>Inicio</span></a></li><li><a href=/es/about/ title="Sobre m√≠"><span>Sobre m√≠</span></a></li><li><a href=/es/posts/ title=Posts><span>Posts</span></a></li><li><a href=/es/contact/ title=Contacto><span>Contacto</span></a></li></ul></nav></header><div class=header-controls><ul class=lang-switch><li><a href=/posts/vit-mae/ aria-label=English>EN</a></li></ul><button id=theme-toggle class=theme-toggle aria-label="Toggle theme">
<span class=theme-icon>üåô</span>
</button>
<script>(function(){"use strict";const n="theme-preference",e={LIGHT:"light",DARK:"dark"},a={[e.LIGHT]:"‚òÄÔ∏è",[e.DARK]:"üåô"};function s(){return localStorage.getItem(n)||e.LIGHT}function r(e){localStorage.setItem(n,e)}function o(e){document.body.setAttribute("a",e)}function t(e){const n=document.getElementById("theme-toggle"),t=n.querySelector(".theme-icon");t&&(t.textContent=a[e])}function c(){const i=s();let n=i===e.LIGHT?e.DARK:e.LIGHT;r(n),o(n),t(n)}function l(){const e=s();o(e),document.readyState==="loading"?document.addEventListener("DOMContentLoaded",()=>t(e)):t(e)}function i(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",c)}l(),document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script></div><main class=page-content aria-label=Content><div class=w><nav class=breadcrumbs aria-label=Breadcrumb><ol><li><a href=https://juanlebrero.com/es/>Juan Lebrero</a></li><li><a href=https://juanlebrero.com/es/posts/>Posts</a></li><li aria-current=page>Haciendo sufrir a los autoencoders: El caso de los Masked Autoencoders</li></ol></nav><a href=/>..</a><article><p class=post-meta><time datetime="2025-11-06 00:00:00 +0000 UTC">2025-11-06</time></p><h1>Haciendo sufrir a los autoencoders: El caso de los Masked Autoencoders</h1><p>Imagin√° intentar aprender un idioma viendo solo el 25% de cada oraci√≥n. Suena como una idea terrible, ¬øno?</p><p>Bueno, resulta que esto es exactamente lo que hacen los Vision Transformer Masked Autoencoders (ViT-MAE) con las im√°genes. El paper &ldquo;Masked Autoencoders Are Scalable Vision Learners&rdquo; de He et al. mostr√≥ algo bastante loco: si ocult√°s el 75% de una imagen (dividi√©ndola en parches y enmascarando aleatoriamente el 75% de ellos) y le ped√≠s a un modelo que la reconstruya, obten√©s representaciones visuales sorprendentemente poderosas.</p><h2 id=arquitectura-general>Arquitectura General</h2><p>ViT-MAE tiene tres componentes principales: patchificaci√≥n, una arquitectura asim√©trica encoder-decoder y una p√©rdida de reconstrucci√≥n. Vamos a ver cada uno en detalle.</p><p>El siguiente diagrama ilustra la arquitectura completa y el flujo de datos:</p><p><img src=/es/posts/vit-mae/example-arch-vitmae.png alt="Figura 1: Arquitectura ViT-MAE"><br><small><b>Figura 1:</b> El flujo de la arquitectura ViT-MAE: (1) Imagen de entrada con el 75% de los parches enmascarados, (2) Los parches visibles se extraen y se pasan al encoder, (3) El encoder procesa solo los parches visibles para producir embeddings ricos, (4) Se agregan tokens de m√°scara para los parches faltantes, (5) El decoder reconstruye todos los parches incluyendo los enmascarados, (6) Imagen final reconstruida con todos los parches rellenados.</small></p><h3 id=patchificaci√≥n-convirtiendo-im√°genes-en-secuencias>Patchificaci√≥n: Convirtiendo Im√°genes en Secuencias</h3><p>Los Vision Transformers tratan las im√°genes como secuencias de parches, similar a c√≥mo los modelos de lenguaje tratan el texto como secuencias de tokens. Cada parche se convierte en un token al que el transformer puede atender. Ac√° te muestro c√≥mo pod√©s <em>patchificar</em> una imagen en <code>pytorch</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>patchify</span><span class=p>(</span><span class=n>x</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>p</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>16</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>batch</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=n>height</span><span class=p>,</span> <span class=n>width</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>height</span> <span class=o>%</span> <span class=n>p</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>width</span> <span class=o>%</span> <span class=n>p</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>unfold</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Unfold</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=n>p</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>patches</span> <span class=o>=</span> <span class=n>unfold</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>patches</span>
</span></span></code></pre></div><p>Para una imagen RGB de 224x224 con parches de 16x16, obtenemos 196 parches (grid de 14x14). Cada parche se aplana en un vector de tama√±o $3 \times 16 \times 16 = 768$ dimensiones. La operaci√≥n <code>Unfold</code> extrae estos parches eficientemente, y transponemos para obtener el formato de secuencia <code>[batch, num_patches, patch_dim]</code>.</p><p>La operaci√≥n inversa, <code>unpatchify</code>, reconstruye la imagen a partir de los parches:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>unpatchify</span><span class=p>(</span><span class=n>patches</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>p</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>16</span><span class=p>,</span> <span class=n>height</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>224</span><span class=p>,</span> <span class=n>width</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>224</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>batch</span><span class=p>,</span> <span class=n>num_patches</span><span class=p>,</span> <span class=n>patch_size</span> <span class=o>=</span> <span class=n>patches</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>fold</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Fold</span><span class=p>(</span><span class=n>output_size</span><span class=o>=</span><span class=p>(</span><span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>),</span> <span class=n>kernel_size</span><span class=o>=</span><span class=n>p</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>fold</span><span class=p>(</span><span class=n>patches</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=enmascaramiento-aleatorio-el-arte-de-ocultar>Enmascaramiento Aleatorio: El Arte de Ocultar</h3><p>La idea del enmascaramiento es literalmente tapar parte de la imagen al azar, como si agarraras post-its y los pegaras encima. B√°sicamente, tomamos todos los parches, mezclamos el orden y elegimos unos pocos para dejar visibles; el resto los tapamos:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>random_masking</span><span class=p>(</span><span class=n>num_patches</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>keep_ratio</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.25</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>perm</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randperm</span><span class=p>(</span><span class=n>num_patches</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>keep</span> <span class=o>=</span> <span class=n>perm</span><span class=p>[:</span><span class=nb>int</span><span class=p>(</span><span class=n>num_patches</span> <span class=o>*</span> <span class=n>keep_ratio</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>perm</span><span class=p>[</span><span class=nb>int</span><span class=p>(</span><span class=n>num_patches</span> <span class=o>*</span> <span class=n>keep_ratio</span><span class=p>):]</span>
</span></span><span class=line><span class=cl>    <span class=n>restore</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>perm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>keep</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=n>restore</span>
</span></span></code></pre></div><p>El truco est√° en <code>restore</code>: despu√©s de todo el l√≠o de desordenar parches y ocultar unos cuantos, necesitamos devolver todo a su lugar original. El encoder solamente procesa los parches que dejamos destapados (<code>keep</code>), y el decoder tiene que &ldquo;adivinar&rdquo; c√≥mo eran los tapados (<code>mask</code>) y volver a armar la secuencia como si nada. Por eso hace falta reordenar, as√≠ todo queda bien cuadradito al final.</p><h3 id=dando-sentido-a-lo-visible-codificadores-y-mecanismos-de-atenci√≥n>Dando Sentido a lo Visible: Codificadores y Mecanismos de Atenci√≥n</h3><p>El codificador, en pocas palabras, es un Vision Transformer t√≠pico que solo ve los parches que quedaron sin tapar:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Encoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>patch_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>d_e</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span> <span class=n>depth</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=n>heads</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span> <span class=n>npos</span><span class=o>=</span><span class=mi>196</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>patch_dim</span><span class=p>,</span> <span class=n>d_e</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_e</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>npos</span><span class=p>,</span> <span class=n>d_e</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>TransformerBlock</span><span class=p>(</span><span class=n>d_e</span><span class=p>,</span> <span class=n>heads</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>depth</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>patches</span><span class=p>,</span> <span class=n>visible_indices</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_embed</span><span class=p>(</span><span class=n>patches</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_e</span><span class=p>[:,</span> <span class=n>visible_indices</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=n>visible_tokens</span> <span class=o>=</span> <span class=n>tokens</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>block</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>visible_tokens</span> <span class=o>=</span> <span class=n>block</span><span class=p>(</span><span class=n>visible_tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>visible_tokens</span>
</span></span></code></pre></div><p>Fijate que los embeddings posicionales solo los sumamos para los parches que est√°n visibles, o sea, los que el codificador s√≠ puede ver. Justamente esa es la gracia: con solo el 25% de los parches, el encoder tiene que arregl√°rselas y tratar de entender la textura, la estructura y en general el contexto visual de la imagen. Si lo pens√°s, el modelo est√° obligado a volverse bastante &ldquo;vivo&rdquo; para exprimir al m√°ximo lo poco que ve.</p><p>Los bloques transformer en s√≠ usan la t√≠pica atenci√≥n multi-cabeza y un par de capas feed-forward, nada raro:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>heads</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>mlp_mult</span><span class=o>=</span><span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pre_attention_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attention</span> <span class=o>=</span> <span class=n>MHSA</span><span class=p>(</span><span class=n>d</span><span class=p>,</span> <span class=n>heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pre_mlp_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>d</span><span class=p>,</span> <span class=n>mlp_mult</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attention</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>pre_attention_norm</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>pre_mlp_norm</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>Algo que siempre quise probar (pero todav√≠a no me puse) es una variante llamada <em>Deformable Attention</em>. B√°sicamente la idea es que, en vez de que cada token mire a todos los dem√°s (como pasa en atenci√≥n est√°ndar, que termina siendo car√≠simo cuando hay muchos tokens), le dec√≠s que solo preste atenci√≥n a los $k$ tokens m√°s relevantes. Es como pasar de un &ldquo;todos contra todos&rdquo; a un &ldquo;solo miro a los importantes&rdquo;.</p><p>Normalmente, la atenci√≥n completa implica que cada consulta calcula scores con todos los tokens (si ten√©s $n$ tokens, es una matriz de $n \times n$). Eso es potente pero pesado, sobre todo si la secuencia es larga.</p><p>En cambio, con Deformable Attention, cada token de consulta se conecta solo con $k$ tokens (y $k$ suele ser mucho menor que $n$), bajando la cuenta a $O(n \times k)$. La gracia es que el modelo aprende solito a elegir a cu√°les mirar, y as√≠ queda un sistema mucho m√°s eficiente, sin resignar tanta capacidad.</p><p>Fijate este diagrama:</p><p><img src=/es/posts/vit-mae/full-vs-def-att.png alt="Figura 2: Atenci√≥n Completa vs Atenci√≥n Deformable"><br><small><b>Figura 2:</b> Izquierda: atenci√≥n completa (cada token mira a todos, la cl√°sica matriz densa). Derecha: atenci√≥n deformable con <i>k=4</i> (cada token solo mira a sus cuatro vecinos m√°s √∫tiles, y el patr√≥n se adapta seg√∫n la consulta). As√≠, podemos procesar im√°genes grandes con menos recursos sin perder tanto en calidad.</small></p><p>Del lado izquierdo, la atenci√≥n completa es una marea de valores: cada consulta contra cada entrada (ac√° lo ves con una matriz $6 \times 6$). A la derecha, con deformable y $k=4$, cada consulta solo atiende a 4, formando una matriz mucho m√°s liviana. Adem√°s, la ventana de atenci√≥n puede correrse seg√∫n cada token, porque el modelo aprende d√≥nde conviene enfocarse.</p><p>Este m√©todo se vuelve especialmente √∫til cuando trabaj√°s con im√°genes grandes y la cantidad de parches se dispara. En alg√∫n momento lo voy a implementar para comparar frente al transformer cl√°sico, seguro da para un buen experimento.</p><h3 id=reconstruyendo-lo-faltante-mask-tokens-y-recuperaci√≥n-de-p√≠xeles>Reconstruyendo lo Faltante: Mask Tokens y Recuperaci√≥n de P√≠xeles</h3><p>Ac√° es donde ocurre la magia de la reconstrucci√≥n: el decodificador. A diferencia del codificador, este m√≥dulo es m√°s chiquito, con menos capas (por ejemplo, 8 en vez de 12) y un ancho menor (512 dimensiones vs 1024). ¬øPor qu√©? Porque su laburo es m√°s sencillo: agarrar las representaciones que salen del codificador y tratar de recuperar los valores de los p√≠xeles originales.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Decoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_embedd</span><span class=o>=</span><span class=mi>1024</span><span class=p>,</span> <span class=n>d_decoder</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>depth</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>heads</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                 <span class=n>npos</span><span class=o>=</span><span class=mi>196</span><span class=p>,</span> <span class=n>patch_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_embedd</span><span class=p>,</span> <span class=n>d_decoder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mask_token</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>d_decoder</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_d</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>npos</span><span class=p>,</span> <span class=n>d_decoder</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>TransformerBlock</span><span class=p>(</span><span class=n>d_decoder</span><span class=p>,</span> <span class=n>heads</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>depth</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_decoder</span><span class=p>,</span> <span class=n>patch_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>visible_embeddings</span><span class=p>,</span> <span class=n>visible_indices</span><span class=p>,</span> <span class=n>masked_indices</span><span class=p>,</span> <span class=n>restore_indices</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>visible_embeddings</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>projected_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>visible_embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>num_patches</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>visible_indices</span><span class=p>)</span> <span class=o>+</span> <span class=nb>len</span><span class=p>(</span><span class=n>masked_indices</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>full_sequence</span> <span class=o>=</span> <span class=n>projected_embeddings</span><span class=o>.</span><span class=n>new_zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>num_patches</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                                        <span class=n>projected_embeddings</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Ponemos los embeddings visibles en sus posiciones correspondientes</span>
</span></span><span class=line><span class=cl>        <span class=n>full_sequence</span><span class=p>[:,</span> <span class=n>visible_indices</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=n>projected_embeddings</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Ponemos el mask token (un vector aprendible) donde faltan parches</span>
</span></span><span class=line><span class=cl>        <span class=n>full_sequence</span><span class=p>[:,</span> <span class=n>masked_indices</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mask_token</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>batch_size</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>masked_indices</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Restauramos el orden espacial y sumamos los embeddings posicionales</span>
</span></span><span class=line><span class=cl>        <span class=n>full_sequence</span> <span class=o>=</span> <span class=n>full_sequence</span><span class=p>[:,</span> <span class=n>restore_indices</span><span class=p>,</span> <span class=p>:]</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_d</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>decoder_input</span> <span class=o>=</span> <span class=n>full_sequence</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>block</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>decoder_input</span> <span class=o>=</span> <span class=n>block</span><span class=p>(</span><span class=n>decoder_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>head</span><span class=p>(</span><span class=n>decoder_input</span><span class=p>)</span>
</span></span></code></pre></div><p>En resumen, el decodificador se encarga de tomar:</p><ul><li>Los embeddings que s√≠ vio el codificador</li><li>Los mask tokens, que son como &ldquo;placeholders&rdquo; aprendibles para los parches tapados</li><li>Los embeddings posicionales, para recordar d√≥nde va cada cosa</li></ul><p>El mask token es simplemente un vector aprendible ‚Äî uno solo ‚Äî que se copia todas las veces necesarias para tapar los huecos. Gracias a la atenci√≥n del decodificador, estos mask tokens pueden mirar a los patches visibles y a los otros mask tokens, aprendiendo as√≠ a completar lo que falta en la imagen.</p><h3 id=la-p√©rdida-aprendiendo-lo-que-importa>La P√©rdida: Aprendiendo lo que Importa</h3><p>La p√©rdida del modelo solo se fija en los parches de la imagen que fueron tapados, no en los que ya ve√≠a. Esto es re importante: si tambi√©n le sum√°ramos la p√©rdida de los visibles, el modelo podr√≠a simplemente copiarlos tal cual y no aprender√≠a nada √∫til.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>mae_loss</span><span class=p>(</span><span class=n>pred</span><span class=p>,</span> <span class=n>target_patches</span><span class=p>,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=n>norm_pix_loss</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>norm_pix_loss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Normalizamos cada parche usando su propia media y varianza</span>
</span></span><span class=line><span class=cl>        <span class=n>mean_target</span> <span class=o>=</span> <span class=n>target_patches</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var_target</span> <span class=o>=</span> <span class=n>target_patches</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_target</span> <span class=o>=</span> <span class=p>(</span><span class=n>target_patches</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span> <span class=o>-</span> <span class=n>mean_target</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>var_target</span> <span class=o>+</span> <span class=mf>1e-6</span><span class=p>)</span><span class=o>.</span><span class=n>sqrt</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>mean_pred</span> <span class=o>=</span> <span class=n>pred</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var_pred</span> <span class=o>=</span> <span class=n>pred</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_pred</span> <span class=o>=</span> <span class=p>(</span><span class=n>pred</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span> <span class=o>-</span> <span class=n>mean_pred</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>var_pred</span> <span class=o>+</span> <span class=mf>1e-6</span><span class=p>)</span><span class=o>.</span><span class=n>sqrt</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>((</span><span class=n>norm_pred</span> <span class=o>-</span> <span class=n>norm_target</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>((</span><span class=n>pred</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:]</span> <span class=o>-</span> <span class=n>target_patches</span><span class=p>[:,</span> <span class=n>mask_idx</span><span class=p>,</span> <span class=p>:])</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></div><p>Ese <code>norm_pix_loss</code> lo que hace es normalizar cada parche antes de calcular el error, as√≠ el modelo no se distrae con diferencias de brillo o color y se concentra en aprender la estructura de la imagen. B√°sicamente, en vez de pedirle que acierte el color exacto de una casa, le estamos pidiendo que entienda la forma y el patr√≥n general.</p><h2 id=por-qu√©-esto-funciona>Por Qu√© Esto Funciona</h2><p>¬øPor qu√© funciona tan bien ViT-MAE? Pensalo as√≠: es como si te dieran un rompecabezas de 1000 piezas, pero solo te dejaran ver 250. No pod√©s simplemente copiar lo que ves, ten√©s que adivinar y entender c√≥mo encajan las partes que faltan. El modelo est√° obligado a captar el sentido general de la imagen, no pueden hacer trampa porque no hay de d√≥nde copiar.</p><p>La clave est√° en esta divisi√≥n de roles: el codificador (encoder) es el que de verdad piensa y trata de entender la info limitada que recibe. El decodificador es mucho m√°s simple y solo ayuda a reconstruir la imagen al final. Durante el entrenamiento, usamos ambos. Despu√©s, solo nos quedamos con el codificador, que es el que aprendi√≥ realmente a ‚Äúver‚Äù.</p><p>A diferencia de otros m√©todos m√°s complejos, como el aprendizaje contrastivo, ac√° la consigna es directa: ‚ÄúReconstru√≠ lo que falta‚Äù. Los transformers son perfectos para esto porque se la bancan entendiendo relaciones entre partes de la imagen, incluso si la mayor√≠a fue ocultada. Por eso, el modelo aprende representaciones visuales poderosas, a pesar de que se le oculta casi todo.</p><h2 id=poni√©ndolo-todo-junto>Poni√©ndolo Todo Junto</h2><p>As√≠ ser√≠a el &ldquo;forward&rdquo; entero del modelo ViT-MAE:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ViTMAE</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. Convertimos la imagen en parches</span>
</span></span><span class=line><span class=cl>        <span class=n>patches</span> <span class=o>=</span> <span class=n>patchify</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 2. Elegimos aleatoriamente qu√© parches dejar visibles y cu√°les tapar</span>
</span></span><span class=line><span class=cl>        <span class=n>keep</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=n>restore</span> <span class=o>=</span> <span class=n>random_masking</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_patches</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>keep_ratio</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>keep</span> <span class=o>=</span> <span class=n>keep</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>restore</span> <span class=o>=</span> <span class=n>restore</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. Solo los parches visibles pasan por el encoder</span>
</span></span><span class=line><span class=cl>        <span class=n>visible_patches</span> <span class=o>=</span> <span class=n>patches</span><span class=p>[:,</span> <span class=n>keep</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=n>visible_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>visible_patches</span><span class=p>,</span> <span class=n>keep</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># 4. El decoder trata de reconstruir todos los parches (los visibles y los tapados)</span>
</span></span><span class=line><span class=cl>        <span class=n>reconstructed_patches</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>visible_embeddings</span><span class=p>,</span> <span class=n>keep</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=n>restore</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>reconstructed_patches</span><span class=p>,</span> <span class=n>patches</span><span class=p>,</span> <span class=n>mask</span>
</span></span></code></pre></div><p>En el entrenamiento, la gracia est√° en comparar la reconstrucci√≥n con los parches tapados (no con todos), sacar la p√©rdida, y hacer el backpropagation como siempre. Al final, el encoder aprende a sacar el jugo clave de im√°genes medio ocultas, y eso despu√©s sirve un mont√≥n para tareas como clasificaci√≥n, detecci√≥n o segmentaci√≥n, aunque no haya visto la mayor√≠a de la imagen durante el pre-entrenamiento.</p><h2 id=el-panorama-general>El Panorama General</h2><p>En resumen, ViT-MAE es una idea tan simple que hasta parece absurda: tap√° gran parte de la imagen y pedile al modelo que adivine el resto. Y resulta que eso lo obliga a aprender en serio, entendiendo la estructura y el significado de las im√°genes, sin que nadie le diga qu√© es qu√©.</p><p>Este m√©todo, por m√°s raro que suene, funcion√≥ tan bien que inspir√≥ un mont√≥n de variantes: desde MAE para video hasta mezclas con texto y otros dominios. Los investigadores siguen probando c√≥mo ocultar cosas para que los modelos aprendan mejor. Pero si lo pens√°s, el truco no cambia: escond√© lo suficiente como para que el modelo no pueda hacer trampa, y dej√° que resuelva el rompecabezas con lo poco que tiene. Si logra reconstruir una imagen a partir de solo un cuarto de los datos, est√° claro que entendi√≥ mucho m√°s de lo que parece.</p><p>As√≠ que, qui√©n te dice, la pr√≥xima vez que te quieras desafiar de verdad, prob√° tapar parte del problema. Si funciona para los transformers, capaz te sorprende a vos tambi√©n.</p></article></div></main></body></html>