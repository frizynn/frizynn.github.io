<!doctype html><html lang=es><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='El sistema RAG (Retrieval-Augmented Generation) se consolid√≥ como el est√°ndar de la industria para mitigar las alucinaciones de los modelos de lenguaje grandes (LLMs), inyectando datos confiables durante la generaci√≥n de respuestas. El mecanismo es conocido: ante una consulta, el sistema busca fragmentos de texto (&ldquo;chunks&rdquo;) relevantes en una base de datos vectorial y los pasa como contexto al modelo para que formule una respuesta fundada. Este enfoque demostr√≥ ser efectivo para preguntas puntuales sobre datos espec√≠ficos. Sin embargo, su rendimiento decae significativamente cuando la tarea requiere una comprensi√≥n transversal de un corpus completo, como responder "¬øCu√°les son los patrones de evoluci√≥n tecnol√≥gica en estos 10.000 informes?". La recuperaci√≥n por similitud vectorial, al entregar piezas aisladas, carece de la arquitectura necesaria para sintetizar un panorama global.'><title>De lo Local a lo Global: Un An√°lisis Profundo de GraphRAG</title><link rel=icon type=image/x-icon href=https://juanlebrero.com/favicon.ico><link rel=icon type=image/png href=https://juanlebrero.com/favicon.png><link rel=stylesheet href=/css/main.0765131bd814d9b69e7cadfb0b3c55a3a332b2257e25eeaea0d3ce4a0c6170d991392395f875b164f19265c8b80bb8cb4ca5ea9c34b3d0bdb1f660828e56b6c7.css integrity="sha512-B2UTG9gU2baefK37CzxVo6MysiV+Je6uoNPOSgxhcNmROSOV+HWxZPGSZci4C7jLTKXqnDSz0L2x9mCCjla2xw=="><link rel=stylesheet href=/css/custom.css></head><body a=auto><header class=header><nav class=nav><div class=logo><a href=/es/ accesskey=h title=Home>Juan Francisco Lebrero</a></div><ul id=menu><li><a href=/es/ title=Inicio><span>Inicio</span></a></li><li><a href=/es/about/ title="Sobre m√≠"><span>Sobre m√≠</span></a></li><li><a href=/es/posts/ title=Posts><span>Posts</span></a></li><li><a href=/es/contact/ title=Contacto><span>Contacto</span></a></li></ul></nav></header><div class=header-controls><ul class=lang-switch><li><a href=/ aria-label=English>EN</a></li></ul><button id=theme-toggle class=theme-toggle aria-label="Toggle theme">
<span class=theme-icon>üåô</span>
</button>
<script>(function(){"use strict";const n="theme-preference",e={LIGHT:"light",DARK:"dark"},a={[e.LIGHT]:"‚òÄÔ∏è",[e.DARK]:"üåô"};function s(){return localStorage.getItem(n)||e.LIGHT}function r(e){localStorage.setItem(n,e)}function o(e){document.body.setAttribute("a",e)}function t(e){const n=document.getElementById("theme-toggle"),t=n.querySelector(".theme-icon");t&&(t.textContent=a[e])}function c(){const i=s();let n=i===e.LIGHT?e.DARK:e.LIGHT;r(n),o(n),t(n)}function l(){const e=s();o(e),document.readyState==="loading"?document.addEventListener("DOMContentLoaded",()=>t(e)):t(e)}function i(){const e=document.getElementById("theme-toggle");e&&e.addEventListener("click",c)}l(),document.readyState==="loading"?document.addEventListener("DOMContentLoaded",i):i()})()</script></div><main class=page-content aria-label=Content><div class=w><nav class=breadcrumbs aria-label=Breadcrumb><ol><li><a href=https://juanlebrero.com/es/>Juan Lebrero</a></li><li><a href=https://juanlebrero.com/es/posts/>Posts</a></li><li aria-current=page>De lo Local a lo Global: Un An√°lisis Profundo de GraphRAG</li></ol></nav><a href=/>..</a><article><p class=post-meta><time datetime="2025-12-30 00:00:00 +0000 UTC">2025-12-30</time></p><h1>De lo Local a lo Global: Un An√°lisis Profundo de GraphRAG</h1><p>El sistema RAG (<em>Retrieval-Augmented Generation</em>) se consolid√≥ como el est√°ndar de la industria para mitigar las alucinaciones de los modelos de lenguaje grandes (LLMs), inyectando datos confiables durante la generaci√≥n de respuestas. El mecanismo es conocido: ante una consulta, el sistema busca fragmentos de texto (&ldquo;chunks&rdquo;) relevantes en una base de datos vectorial y los pasa como contexto al modelo para que formule una respuesta fundada. Este enfoque demostr√≥ ser efectivo para preguntas puntuales sobre datos espec√≠ficos. Sin embargo, su rendimiento decae significativamente cuando la tarea requiere una comprensi√≥n transversal de un corpus completo, como responder <em>"¬øCu√°les son los patrones de evoluci√≥n tecnol√≥gica en estos 10.000 informes?"</em>. La recuperaci√≥n por similitud vectorial, al entregar piezas aisladas, carece de la arquitectura necesaria para sintetizar un panorama global.</p><p><strong>GraphRAG</strong>, presentado por Microsoft Research en el paper <em><a href=https://arxiv.org/pdf/2404.16130>&ldquo;From Local to Global: A GraphRAG Approach to Query-Focused Summarization&rdquo;</a></em>, aborda esta limitaci√≥n proponiendo una estrategia diferente: estructurar la informaci√≥n en un grafo de conocimiento jer√°rquico antes de recibir cualquier consulta. Este proceso de indexaci√≥n exhaustiva transforma los documentos fuente en una red de entidades y relaciones, agrupadas en comunidades tem√°ticas, cada una con su propio resumen generado. As√≠, cuando llega una pregunta global, el sistema ya cuenta con un mapa sem√°ntico del corpus listo para ser navegado.</p><p>A continuaci√≥n, analizamos las seis etapas de este pipeline, explicando c√≥mo cada una alimenta a la siguiente y por qu√© el conjunto resulta m√°s potente que la suma de sus partes.</p><hr><h2 id=el-pipeline-de-indexaci√≥n>El Pipeline de Indexaci√≥n</h2><p>El coraz√≥n de GraphRAG est√° en su fase de indexaci√≥n. A diferencia de un sistema RAG tradicional, donde la inversi√≥n computacional se concentra en el momento de la consulta, GraphRAG realiza un trabajo pesado previo para facilitar las respuestas posteriores. Este trabajo consiste en construir, a partir de los documentos crudos, una estructura de conocimiento jer√°rquica que permita razonar sobre el dataset de forma transversal.</p><h3 id=etapa-1-segmentaci√≥n-de-texto>Etapa 1: Segmentaci√≥n de Texto</h3><p>El proceso inicia convirtiendo los documentos fuente en unidades de texto procesables, denominadas &ldquo;TextUnits&rdquo; o chunks. La decisi√≥n sobre el tama√±o de estos fragmentos es una variable de ingenier√≠a cr√≠tica, dado que afecta tanto el costo (cantidad de llamadas al LLM) como la calidad de la extracci√≥n posterior. GraphRAG recomienda fragmentos de aproximadamente 600 tokens, un tama√±o que equilibra la retenci√≥n de contexto local con la eficiencia del procesamiento.</p><p><img src=/es/posts/graph-rag/stage1.png alt="Etapa 1: Segmentaci√≥n de Texto"></p><p>El diagrama representa la transformaci√≥n del documento original en m√∫ltiples fragmentos. La divisi√≥n respeta, en la medida de lo posible, los l√≠mites sem√°nticos del texto, evitando cortar oraciones o p√°rrafos a la mitad.</p><p>Un aspecto fundamental de esta segmentaci√≥n es el solapamiento entre fragmentos. Al repetir aproximadamente 100 tokens entre el final de un chunk y el comienzo del siguiente, el sistema genera una continuidad sem√°ntica. Esta superposici√≥n garantiza que las relaciones que cruzan los l√≠mites de un fragmento (por ejemplo, una menci√≥n de &ldquo;la empresa mencionada anteriormente&rdquo;) no queden hu√©rfanas de contexto. As√≠, cuando en la etapa siguiente el LLM analice cada chunk de forma aislada, contar√° con la informaci√≥n necesaria para interpretar correctamente las referencias internas del texto.</p><h3 id=etapa-2-extracci√≥n-de-elementos>Etapa 2: Extracci√≥n de Elementos</h3><p>Una vez segmentado el corpus, cada chunk es procesado por un LLM con el objetivo de extraer tres tipos de elementos estructurados: entidades (que ser√°n los nodos del grafo), relaciones (que ser√°n las aristas) y claims (afirmaciones f√°cticas que anclan el conocimiento a la fuente).</p><p><img src=/es/posts/graph-rag/stage2.png alt="Etapa 2: Extracci√≥n de Entidades y Claims"></p><p>La figura ilustra este proceso de destilaci√≥n. Los fragmentos de texto ingresan al modelo y emergen como componentes estructurados: √≠conos que representan entidades clasificadas (Persona, Organizaci√≥n, Evento, Tecnolog√≠a) y flechas que describen relaciones entre ellas (&ldquo;trabaja_en&rdquo;, &ldquo;desarrollado_por&rdquo;, &ldquo;colabora_con&rdquo;).</p><p>El prompt utilizado para esta extracci√≥n es multipartite: instruye al modelo para que identifique entidades, las clasifique en categor√≠as predefinidas, y describa expl√≠citamente la naturaleza de sus v√≠nculos. Para maximizar la cobertura, GraphRAG implementa una t√©cnica de &ldquo;gleaning&rdquo; (rebusque): tras la primera pasada de extracci√≥n, el sistema ejecuta un <em>continuation prompt</em> preguntando al modelo si qued√≥ alg√∫n elemento sin capturar. Esta iteraci√≥n adicional reduce significativamente la p√©rdida de informaci√≥n.</p><p>La calidad de este proceso mejora sustancialmente cuando se utilizan prompts <em>few-shot</em> adaptados al dominio espec√≠fico. Inyectar ejemplos de extracci√≥n correcta (por ejemplo, de reportes financieros o papers biom√©dicos) sesga al modelo hacia el tipo de entidades y relaciones relevantes para el corpus. Adem√°s, la extracci√≥n de &ldquo;claims&rdquo; permite conservar el contexto factual original: cada relaci√≥n queda anclada a una cita del texto fuente, con fecha si est√° disponible, lo que facilita la verificaci√≥n posterior y reduce el riesgo de alucinaci√≥n.</p><h3 id=etapa-3-construcci√≥n-del-grafo>Etapa 3: Construcci√≥n del Grafo</h3><p>Con miles de entidades y relaciones extra√≠das, el sistema enfrenta un desaf√≠o de consolidaci√≥n: la misma entidad puede aparecer mencionada con distintas variantes (por ejemplo, &ldquo;Juan P√©rez&rdquo;, &ldquo;J. P√©rez&rdquo; y &ldquo;el ingeniero P√©rez&rdquo; podr√≠an referirse a la misma persona). GraphRAG aplica algoritmos de resoluci√≥n de entidades para fusionar estas menciones duplicadas en un √∫nico nodo can√≥nico, creando as√≠ un multigrafo unificado y coherente.</p><p><img src=/es/posts/graph-rag/stage3.png alt="Etapa 3: Construcci√≥n del Grafo"></p><p>En el diagrama se observa la red consolidada. Los nodos m√°s grandes (en naranja) representan entidades con alta centralidad, es decir, aquellas que participan en m√∫ltiples relaciones y act√∫an como hubs de informaci√≥n. Las l√≠neas de mayor grosor indican relaciones que fueron detectadas en m√∫ltiples instancias a lo largo del corpus.</p><p>Este grosor de las aristas corresponde al concepto de peso ponderado. Cada vez que el sistema detecta una relaci√≥n entre dos entidades en diferentes partes del corpus, incrementa el peso de la arista correspondiente. Una relaci√≥n mencionada cincuenta veces tendr√° un peso sustancialmente mayor que una mencionada una sola vez. Esta ponderaci√≥n transforma al grafo en un mapa de calor de relevancia: permite distinguir v√≠nculos estructurales profundos (aquellos que atraviesan m√∫ltiples documentos y contextos) de conexiones anecd√≥ticas que aparecen de forma aislada. Este peso ser√° el insumo principal para el algoritmo de detecci√≥n de comunidades que sigue, ya que le indica cu√°les nodos deber√≠an agruparse juntos.</p><h3 id=etapa-4-detecci√≥n-de-comunidades>Etapa 4: Detecci√≥n de Comunidades</h3><p>El grafo resultante de la etapa anterior puede contener miles o millones de nodos, una escala que excede la capacidad de cualquier ventana de contexto de LLM. Para organizar esta informaci√≥n de forma navegable, GraphRAG aplica el algoritmo Leiden, un m√©todo de detecci√≥n de comunidades que agrupa los nodos en clusters bas√°ndose en la densidad de sus conexiones.</p><p><img src=/es/posts/graph-rag/stage4.png alt="Etapa 4: Detecci√≥n de Comunidades"></p><p>El diagrama muestra la jerarqu√≠a resultante: clusters de diferentes colores agrupan nodos densamente conectados entre s√≠, con l√≠neas punteadas indicando los niveles superiores de la jerarqu√≠a que engloban a clusters m√°s peque√±os.</p><p>El algoritmo Leiden opera optimizando una m√©trica llamada &ldquo;modularidad&rdquo;, que mide qu√© tan densamente conectados est√°n los nodos dentro de una comunidad en comparaci√≥n con conexiones hacia afuera. Los pesos de las aristas calculados en la etapa anterior son cruciales para este proceso: Leiden utiliza esa informaci√≥n de intensidad para decidir qu√© nodos deben agruparse. Si dos nodos comparten una relaci√≥n pesada (alta frecuencia de co-ocurrencia), el algoritmo los mantiene en la misma comunidad; si la relaci√≥n es d√©bil, permite que se separen.</p><p>Leiden se ejecuta de forma recursiva, generando una jerarqu√≠a de comunidades. El nivel superior (Nivel 0) contiene macro-temas que abarcan grandes porciones del corpus; cada uno de estos macro-temas se subdivide en subtemas m√°s espec√≠ficos (Nivel 1), y as√≠ sucesivamente hasta llegar a comunidades muy focalizadas. El algoritmo incorpora un par√°metro de resoluci√≥n (gamma) que permite ajustar esta granularidad: valores altos producen comunidades m√°s peque√±as y espec√≠ficas, mientras que valores bajos generan clusters m√°s amplios. Esta flexibilidad permite elegir el nivel de detalle apropiado seg√∫n la naturaleza de la consulta posterior.</p><p>Una ventaja t√©cnica de Leiden sobre su predecesor Louvain es la incorporaci√≥n de una fase de refinamiento. Tras una primera agrupaci√≥n, Leiden verifica que cada comunidad est√© internamente bien conectada, evitando que queden nodos aislados dentro de un cluster. Esta garant√≠a de conectividad interna es fundamental para la etapa siguiente, porque asegura que cada comunidad representa un tema cohesivo, susceptible de ser resumido de forma coherente.</p><h3 id=etapa-5-generaci√≥n-de-res√∫menes-de-comunidad>Etapa 5: Generaci√≥n de Res√∫menes de Comunidad</h3><p>Con el grafo particionado en comunidades jer√°rquicas, GraphRAG genera un resumen textual para cada una de ellas. El proceso consiste en alimentar al LLM con todos los nodos y aristas de una comunidad y pedirle que redacte un reporte ejecutivo describiendo de qu√© trata ese cluster.</p><p><img src=/es/posts/graph-rag/stage5.png alt="Etapa 5: Generaci√≥n de Res√∫menes"></p><p>La figura muestra esta transformaci√≥n: los clusters circulares (datos crudos del grafo) se comprimen en documentos de reporte (informaci√≥n procesada lista para consumir).</p><p>Estos res√∫menes funcionan como un √≠ndice invertido sem√°ntico. Cuando posteriormente llegue una consulta, el sistema consultar√° estos reportes curados que ya condensan la informaci√≥n importante sobre cada tema. Este pre-c√°lculo offline tiene un beneficio fundamental: ancla al modelo en informaci√≥n verificada y destilada, reduciendo dr√°sticamente el riesgo de alucinaci√≥n. El LLM, al momento de responder, trabaja con res√∫menes que ya pasaron por un proceso de extracci√≥n y validaci√≥n, y que incluyen referencias a las fuentes originales.</p><p>Los niveles superiores de la jerarqu√≠a contienen res√∫menes de res√∫menes. El sistema procesa primero las comunidades de nivel m√°s bajo (las m√°s espec√≠ficas), genera sus reportes, y luego utiliza esos reportes como insumo para generar los res√∫menes de los niveles superiores. As√≠ se construye una pir√°mide de abstracci√≥n donde la informaci√≥n fluye desde los datos granulares en la base hasta insights de alto nivel en la cima. Esta estructura permite navegar el corpus a diferentes niveles de detalle seg√∫n las necesidades de la consulta.</p><h3 id=etapa-6-generaci√≥n-de-respuesta-global>Etapa 6: Generaci√≥n de Respuesta Global</h3><p>Cuando finalmente llega una consulta global que requiere sintetizar informaci√≥n de todo el corpus, GraphRAG debe decidir qu√© res√∫menes de comunidad utilizar como contexto. Esta decisi√≥n puede realizarse de dos maneras.</p><p>La primera opci√≥n es utilizar un <strong>nivel jer√°rquico preestablecido</strong>. El operador del sistema define de antemano en qu√© nivel de la pir√°mide buscar: si se elige el Nivel 0 (macro-temas), el LLM recibir√° pocos res√∫menes muy amplios; si se elige un nivel inferior, recibir√° m√°s res√∫menes pero m√°s espec√≠ficos. Esta configuraci√≥n es √∫til cuando se conoce de antemano el tipo de consultas esperadas.</p><p>La segunda opci√≥n, m√°s sofisticada, es la <strong>Selecci√≥n Din√°mica de Comunidades</strong> (<em>Dynamic Community Selection</em> o DCS). En este esquema, el sistema comienza desde la ra√≠z del grafo jer√°rquico y utiliza el propio LLM para evaluar la relevancia de cada reporte de comunidad respecto a la consulta entrante. Si un reporte de alto nivel resulta irrelevante, el sistema poda ese sub√°rbol completo sin descender a sus hijos; si resulta relevante, desciende recursivamente hacia niveles m√°s espec√≠ficos. Este mecanismo permite recolectar informaci√≥n del nivel de detalle apropiado para cada consulta, evitando tanto la generalizaci√≥n excesiva como el ruido de informaci√≥n irrelevante.</p><p>Una vez seleccionados los res√∫menes relevantes, el sistema ejecuta un pipeline de procesamiento en dos fases: Map y Reduce.</p><p><img src=/es/posts/graph-rag/stage6.png alt="Etapa 6: Generaci√≥n de Respuesta Global"></p><p>El diagrama ilustra este flujo paralelo: m√∫ltiples fuentes (los res√∫menes de comunidad) alimentan procesos independientes de generaci√≥n de respuestas parciales, que luego convergen en una s√≠ntesis final.</p><p>En la fase Map, el sistema divide la pregunta para que sea respondida por cada comunidad relevante de forma aislada. Cada resumen de comunidad recibe la consulta y genera una respuesta parcial basada √∫nicamente en la informaci√≥n contenida en ese cluster. Este enfoque garantiza que la evidencia se busque localmente, obligando al modelo a fundamentar cada afirmaci√≥n en datos concretos de la comunidad.</p><p>En la fase Reduce, el sistema toma todas las respuestas parciales generadas en la fase anterior, las filtra por relevancia (asignando un puntaje de 0 a 100) y sintetiza las m√°s pertinentes en una narrativa final cohesiva. Este proceso de filtrado es crucial: no toda comunidad tendr√° informaci√≥n relevante para toda consulta, y el scoring permite descartar respuestas parciales que no aportan valor.</p><p>Este enfoque permite escalar la comprensi√≥n a datasets de cualquier tama√±o. Al procesar las comunidades en paralelo, el sistema puede manejar corpus masivos simplemente agregando m√°s nodos de c√≥mputo. La estructura jer√°rquica tambi√©n asegura equidad en la representaci√≥n: un dato crucial contenido en un documento aislado tiene la misma oportunidad de influir en la respuesta final que un dato repetido en cientos de documentos, siempre que la comunidad correspondiente lo considere relevante y lo incluya en su resumen.</p><hr><h2 id=complementariedad-la-b√∫squeda-local-local-search>Complementariedad: La B√∫squeda Local (Local Search)</h2><p>El flujo que acabamos de describir, conocido como &ldquo;Global Search&rdquo;, es extraordinario para responder preguntas de alto nivel sobre la totalidad del dataset. Sin embargo, surge una pregunta natural: ¬øqu√© sucede si necesitamos recuperar un dato extremadamente puntual, como la fecha exacta de un evento menor o el apellido de un personaje secundario? En estos casos, el enfoque global puede resultar ineficiente, ya que los res√∫menes de comunidad tienden a generalizar y podr√≠an omitir detalles microsc√≥picos.</p><p>Para cubrir esta necesidad, el framework de GraphRAG incluye un mecanismo complementario denominado <strong>&ldquo;Local Search&rdquo;</strong>. A diferencia del barrido global, la b√∫squeda local utiliza las entidades extra√≠das como puntos de entrada precisos. Cuando el sistema recibe una consulta sobre una entidad espec√≠fica, no recorre los res√∫menes de comunidad, sino que navega directamente por el grafo, explorando los vecinos inmediatos de esa entidad y recuperando los fragmentos de texto originales (&ldquo;TextUnits&rdquo;) asociados.</p><p>Esta dualidad es la verdadera fortaleza de la arquitectura. Mientras que el modo Global ofrece <em>sensemaking</em> y s√≠ntesis de patrones, el modo Local garantiza precisi√≥n quir√∫rgica y recuperaci√≥n de detalles. Ambos modos conviven sobre la misma estructura indexada, permitiendo al usuario alternar entre una visi√≥n panor√°mica y una lupa de aumento seg√∫n la naturaleza de su pregunta, resolviendo as√≠ el compromiso hist√≥rico entre amplitud y profundidad en la recuperaci√≥n de informaci√≥n.</p><hr><h2 id=conclusi√≥n-y-futuras-perspectivas>Conclusi√≥n y Futuras Perspectivas</h2><p>GraphRAG representa un cambio de paradigma en la forma de abordar la recuperaci√≥n de informaci√≥n para LLMs, pero no est√° exento de desaf√≠os importantes. El costo computacional de construir y mantener el grafo es sustancialmente mayor que el de una simple base de datos vectorial; cada nuevo documento requiere un proceso de extracci√≥n y re-indexaci√≥n que, en la versi√≥n original, puede ser costoso y r√≠gido. Adem√°s, la separaci√≥n estricta entre modos &ldquo;Global&rdquo; y &ldquo;Local&rdquo; obliga muchas veces a tomar decisiones de dise√±o manuales sobre cu√°l pipeline ejecutar, en lugar de tener un sistema unificado que decida por s√≠ mismo.</p><p>La industria, sin embargo, se mueve r√°pido. Nuevos frameworks como <strong>LightRAG</strong> y <strong>RAGAnything</strong> ya est√°n iterando sobre estos conceptos para solucionar estas limitaciones, proponiendo esquemas de actualizaci√≥n incremental y modos h√≠bridos nativos que fusionan lo mejor de ambos mundos en una sola llamada. Pero el an√°lisis profundo de estas nuevas herramientas, y c√≥mo logran bajar los costos manteniendo la calidad del razonamiento, ser√° tema de nuestro pr√≥ximo art√≠culo.</p><p><strong>Referencia:</strong>
Edge, D., et al. (2024). <em>From Local to Global: A GraphRAG Approach to Query-Focused Summarization</em>. arXiv preprint arXiv:2404.16130. Disponible en: <a href=https://arxiv.org/pdf/2404.16130>https://arxiv.org/pdf/2404.16130</a></p></article></div></main></body></html>